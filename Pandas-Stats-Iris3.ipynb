{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8de690ce-49d6-4387-b242-fe36280ca9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: bold;\n",
       "}\n",
       "body {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: 200;\n",
       "}\n",
       "h1 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 30px ;\n",
       "  color: white;\n",
       "  background: #b11d01;\n",
       "  text-align: center;\n",
       "}\n",
       "h2 {\n",
       "  border: 3px solid #333;\n",
       "  padding: 18px ;\n",
       "  color: #b11d01;\n",
       "  background: #ffffff;\n",
       "  text-align: center;\n",
       "}\n",
       "h3 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 12px ;\n",
       "  color: #000000;\n",
       "  background: #c1c1c1;\n",
       "  text-align: left;\n",
       "}\n",
       "h4 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #d9fffc;\n",
       "  text-align: left;\n",
       "}\n",
       "h5 {\n",
       "  border: 1px solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #ffffff;\n",
       "  text-align: left;\n",
       "}\n",
       ".warn {    \n",
       "    background-color: #fcf2f2;\n",
       "    border-color: #dFb5b4;\n",
       "    border-left: 5px solid #dfb5b4;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       ".rq {    \n",
       "    background-color: #e2e2e2;\n",
       "    border-color: #969696;\n",
       "    border-left: 5px solid #969696;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Début à:** Wednesday 22 June 2022, 10:45:45  \n",
       "**Hostname:** 2a02-8440-3341-5fe1-41f9-3b1d-c51e-aab3.rev.sfr.net (Linux)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"./svg/logoBegin.svg\" style=\"margin-left:auto; margin-right:auto\"/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Démarrage du thème 3\n",
    "import visualID as vID\n",
    "from visualID import color\n",
    "vID.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b71cea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Traitement statistique de données<br>(data science pour débutants)\n",
    "## 3. Apprentissage supervisé (*supervised Machine Learning*) appliqué à la classification<br>(régression logistique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43eab3-6d78-4c15-96c5-797e75ee940a",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "    <b style='color:red'>Ce thème n'est pas si complexe, mais l'analyse du code est réservée aux plus curieu(ses)(x) et motivé(e)s.</b>\n",
    "    <br>Il a pour objectif de montrer qu'il est possible de prédire une valeur sur la base d'une corrélation multifactorielle entre une <b>classe d'objets</b> (ici des espèces d'iris) et des <b>propriétés</b> (ou descripteurs, ici les largeurs et longueurs des pétales et des sépales) \n",
    "    <br><b style='color:red'>Les moins curieu(ses)(x) doivent <i>a minima</i> lire les commentaires et exécuter ce code pour en comprendre le principe.</b>\n",
    "   </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2f60c2-8759-4019-8dea-b692e73eaee6",
   "metadata": {},
   "source": [
    "### 3.1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a5449-7e81-4541-a826-da0b12074809",
   "metadata": {},
   "source": [
    "#### 3.1.a. Objectif et principe général\n",
    "On a vu dans la partie précédente (statistiques et régression) qu'il est possible de prédire une valeur ($\\hat{y}$) sur la base d'une régression, c'est-à-dire du *fit* d'une équation mathématique qui relie deux variables entre elles ($y$ et $x$). On a pour cela utilisé les outils de régression offerts par la librairie `SciPy`. On a pu ainsi prédire la longueur d'un pétale d'iris, connaissant sa largeur, compte tenu de la relation linéaire entre longueur et largeur.\n",
    "\n",
    "On a également constaté dans la première partie que la distribution jointe des caractéristiques longueurs et largeurs des pétales (`jointplot`) est presque suffisante pour classifier les 3 espèce d'iris (figure de gauche). La zone de recouvrement entre caractéristiques de pétales des espèces <i>versicolor</i> et <i>virginica</i> ne permet malheureusement pas de trancher entre grands iris <i>versicolor</i> et petits iris <i>virginica</i>. On voit sur la figure de droite que les sépales des iris <i>setosa</i> ont des dimensions différentes des deux autres, alors que <i>versicolor</i> et <i>virginica</i> ont pour la plupart des sépales de dimensions similaires.\n",
    "<p style=\"text-align: center\"><img width=\"900px\" src=\"./svg/jointplot_petals-sepals_classification.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_jpc\"/></img></p>\n",
    "\n",
    "<div class=\"warn\">\n",
    "Le premier objectif de ce TP est d'entraîner un modèle d'Intelligence Artificielle (IA) à <span style=\"color:red\"><b>classifier</b></span> les espèces d'iris sur la base des <span style=\"color:red\">longueurs (L<sub>P</sub>) et largeurs (&ell;<sub>P</sub>) de leurs pétales</span>. L'architecture est la suivante :\n",
    "\n",
    "<p><img width=\"350px\" src=\"./svg/IA-petales.png\" style=\"margin-left:auto; margin-right:auto; display: block;\" id=\"img_IA_jpc\"/></p>  \n",
    "    \n",
    "C'est-à-dire qu'on va spécifier 2 paramètres en entrée, et qu'on veut obtenir en sortie les probabilités <i>P</i> que l'espèce d'iris caractérisée par ces deux propriétés soit de la famille <i>setosa</i>, <i>versicolor</i> ou <i>virginica</i>.\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"rq\">\n",
    "Il faut d'abord entraîner un modèle à faire un lien entre les 2 caractéristiques de pétales d'une part et espèce d'iris d'autre part. On parle d'<b>apprentissage supervisé</b> (<b><i>supervised machine learning</b></i>). Il existe plusieurs modèles statistiques qui peuvent faire ce type d'apprentissage visant à classifier des individus : arbres de décision, séparateurs à vaste marge (<i>support-vector machine</i>, SVM), réseaux de neurones artificiels (<i>artificial neural network</i>, ANN)... Même si ce n'est pas le plus approprié pour un problème aussi simple que celui-ci, on va utiliser une méthode d'apprentissage profond (<i>deep learning</i>), qui est un réseau de neurones particulier. On va découvrir que mettre au point un modèle de ce type n'est en réalité pas si compliqué que ça.\n",
    "\n",
    "Voici à quoi ressemble dans ce cas précis une boîte noire exploitant le deep learning : \n",
    "\n",
    "<p style=\"text-align: center\"><img width=\"250px\" src=\"./svg/ANN-petales.png\" style=\"margin-left:auto; margin-right:auto; display: block;\" id=\"img_ANN_jpc\"/></p>\n",
    "\n",
    "Elle est constituée de neurones artificiels tous connectés entre eux : (a) deux neurones d'entrée qui vont recevoir pour chaque espèce les largeurs et longueurs des pétales; (b) trois neurones de sortie qui vont contenir la probabilité pour que l'iris de cette taille soit <i>setosa</i> (P<sub>1</sub>), <i>versicolor</i> (P<sub>1</sub>) ou <i>virginica</i> (P<sub>3</sub>); (c) deux couches intermédiaires de neurones. Ces couches intermédiaires sont appelées couches cachées. On parle de <b>deep learning</b> pour tout ANN qui contient un mlinimum de deux couches cachées.\n",
    "\n",
    "Un <b>neurone artificiel</b> est en général caractérisé par\n",
    "<li> les <b>poids</b> des connexions qui le relie aux autres neurones, c'est-à-dire l'équivalent des poids synaptiques\n",
    "<li> son \"mode de fonctionnement\", on parle de <b>fonction d'activation</b>\n",
    "<li> une constante (le <b>biais</b>) qui aide le modèle (l'IA) à s’adapter au mieux aux données du problème\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02aa070-ddf4-4bb4-bb26-5043ee7ca53f",
   "metadata": {},
   "source": [
    "#### 3.1.b. Importation des librairies utiles\n",
    "\n",
    "Outre pandas, les deux librairies majeures sont `TensorFlow` et `Keras`. On utilisera également un peu plus tard des utilitaires fournis par la librairie `scikit learn`.\n",
    "\n",
    "<p style=\"text-align: center\"><img width=\"300px\" src=\"./svg/logos-TFKSKL.png\" style=\"margin-left:auto; margin-right:auto; display: block;\" id=\"img_logos\"/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6568ba3-8b12-4725-a890-132a9de260b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a625d707-1cc5-45eb-bdc0-8be3cfe2e9b2",
   "metadata": {},
   "source": [
    "### 3.2. Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7334ed83-f6aa-4f7e-b6a1-7e49b8b9da66",
   "metadata": {},
   "source": [
    "#### 3.2.a. Lecture de la base de données qui ont été adaptées au problème\n",
    "Les données brutes sont en général mal adaptées aux algorithmes d'apprentissage automatique. La/le *data scientist* doit faire en amont un travail de transformation de ces données. On a ici appliqué au préalable un \"encodage 1 parmi n\" (ou *one-hot-encoding*) des espèces d'iris (les plus curieu(x)(ses) peuvent se référer à l'annexe). On va lire la nouvelle base de données (nommée iris_ohe.csv) afin de plus facilement comprendre en quoi consiste cet encodage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef2fe4da-b86e-4c95-8f97-84f8c0f56215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dfi. Structure (shape) :(150, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species  setosa  \\\n",
       "0             5.1          3.5           1.4          0.2     setosa     1.0   \n",
       "1             4.9          3.0           1.4          0.2     setosa     1.0   \n",
       "2             4.7          3.2           1.3          0.2     setosa     1.0   \n",
       "3             4.6          3.1           1.5          0.2     setosa     1.0   \n",
       "4             5.0          3.6           1.4          0.2     setosa     1.0   \n",
       "..            ...          ...           ...          ...        ...     ...   \n",
       "145           6.7          3.0           5.2          2.3  virginica     0.0   \n",
       "146           6.3          2.5           5.0          1.9  virginica     0.0   \n",
       "147           6.5          3.0           5.2          2.0  virginica     0.0   \n",
       "148           6.2          3.4           5.4          2.3  virginica     0.0   \n",
       "149           5.9          3.0           5.1          1.8  virginica     0.0   \n",
       "\n",
       "     versicolor  virginica  \n",
       "0           0.0        0.0  \n",
       "1           0.0        0.0  \n",
       "2           0.0        0.0  \n",
       "3           0.0        0.0  \n",
       "4           0.0        0.0  \n",
       "..          ...        ...  \n",
       "145         0.0        1.0  \n",
       "146         0.0        1.0  \n",
       "147         0.0        1.0  \n",
       "148         0.0        1.0  \n",
       "149         0.0        1.0  \n",
       "\n",
       "[150 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfi=pd.read_csv('./iris-data/iris_ohe.csv', sep=\"\\t\") #les colonnes sont séparées par des tabulations\n",
    "print(f\"Dfi. Structure (shape) :{dfi.shape}\")\n",
    "display(dfi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaf0845-966f-444f-b0a3-2f9da914906f",
   "metadata": {},
   "source": [
    "Le nouveau ficher de données contient 3 nouvelles colonnes (*setosa*, *versicolor*, *virginica*), qui ne contient que des 0 ou des 1, qui ne sont rien d'autre que les probabilités que chacune des fleurs d'iris soit setosa, versicolor ou virginica. Comme c'est un botaniste qui a établi de façon sûre la classification de cette base de données, les probabilités ne peuvent valoir que 0 ou 1 (et bien entendu il ne peut y avoir qu'une seule valeur '1.0' par ligne). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139cdc56-49f8-4775-abb6-1e9b2ff1b292",
   "metadata": {},
   "source": [
    "#### 3.2.b. Séparation des données en deux sous-ensembles d'apprentissage et de test\n",
    "<div class=\"rq\">  \n",
    "On est au c&oelig;ur de l'apprentissage automatique :\n",
    "    <li> les algorithmes doivent être entraînés sur la base d'exemples connus et complètement caractérisés\n",
    "    <li> pour entraîner un algorithme, on va lui donner une multitude d'exemples et, en fonction de la différence entre le résultat obtenu et le résultat attendu, le système va mettre à jour ses \"coefficients\" : c'est la phase d'apprentissage. Cette optimisation des coefficients se fait en minimisant l'écart entre propriété prédite et propriété réelle (la propriété dans ce cas étant l'espèce d'iris)\n",
    "    <li> dans le cas d'un réseau de neurones, les coefficients sont les <b>poids des connexions neuronales</b>, ainsi qu'un paramètre caractéristique de chaque neurone (appelé le <b>biais</b>)\n",
    "    <li> comme pour tout apprentissage, il faut vérifier que les acquis sont solides, c'est la phase de test. Une fois l'algorithme entraîné, on va lui soumettre de nouveaux exemples connus et évaluer sa capacité à donner la bonne réponse.\n",
    " <br><br>   \n",
    "Tout jeu de données est séparé en deux sous-ensembles :\n",
    "    <li> un jeu de données d'<b>apprentissage</b>\n",
    "    <li> un jeu de données de <b>test</b>, indépendantes du jeu de données d'apprentissage, mais qui suit la même distribution de probabilité\n",
    "\n",
    "(C'est ici une approche simplifiée. Dans une approche plus rigoureuse, on sépare la base de données en trois jeux : apprentissage, validation, test).\n",
    "              \n",
    "Le principe est résumé dans la figure ci-dessous :\n",
    "        \n",
    "<p style=\"text-align: center\"><img width=\"750px\" src=\"./svg/holdout.png\" style=\"margin-left:auto; margin-right:auto; display: block;\" id=\"img_IA_holdout\"/> </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0470f71-6f34-4ccf-9e56-836928e32004",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train :  (120, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>5.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>5.6</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     petal_length  petal_width\n",
       "108           5.8          1.8\n",
       "143           5.9          2.3\n",
       "88            4.1          1.3\n",
       "35            1.2          0.2\n",
       "100           6.0          2.5\n",
       "..            ...          ...\n",
       "92            4.0          1.2\n",
       "14            1.2          0.2\n",
       "103           5.6          1.8\n",
       "10            1.5          0.2\n",
       "75            4.4          1.4\n",
       "\n",
       "[120 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train :  (120, 3) y_train_species :  (120, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     setosa  versicolor  virginica\n",
       "108     0.0         0.0        1.0\n",
       "143     0.0         0.0        1.0\n",
       "88      0.0         1.0        0.0\n",
       "35      1.0         0.0        0.0\n",
       "100     0.0         0.0        1.0\n",
       "..      ...         ...        ...\n",
       "92      0.0         1.0        0.0\n",
       "14      1.0         0.0        0.0\n",
       "103     0.0         0.0        1.0\n",
       "10      1.0         0.0        0.0\n",
       "75      0.0         1.0        0.0\n",
       "\n",
       "[120 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        species\n",
       "108   virginica\n",
       "143   virginica\n",
       "88   versicolor\n",
       "35       setosa\n",
       "100   virginica\n",
       "..          ...\n",
       "92   versicolor\n",
       "14       setosa\n",
       "103   virginica\n",
       "10       setosa\n",
       "75   versicolor\n",
       "\n",
       "[120 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# création d'un jeu de données sous forme de dataFrame (data_train) qui contient 80% des échantillons, sélectionnés de façon aléatoire\n",
    "# ce sont les données qui vont servir à l'entraînement de l'algorithme\n",
    "data_train = dfi.sample(frac=0.8, axis='index') \n",
    "# création d'un nouveau dataFrame (data_test) qui contient les 20% restants\n",
    "# ce sont les données qui vont servir à tester l'algorithme sur des données qui lui sont inconnues\n",
    "data_test  = dfi.drop(data_train.index)\n",
    "\n",
    "# sélection des données d'apprentissage (d'entraînement)\n",
    "# x_train contient l'input, c'est-à-dire la largeur et la longueur de chacun des pétales du jeu de données data_train \n",
    "x_train = data_train[['petal_length','petal_width']]\n",
    "# y_train contient ce qu'on veut faire apprendre à l'algorithme, c'est-à-dire le type de chacun  des iris du jeu de données data_train. \n",
    "# comme on veut que l'algorithme prédise des probabilités, on va lui faire apprendre ce type d'information\n",
    "y_train = data_train[['setosa','versicolor','virginica']]\n",
    "y_train_species = data_train[['species']] #sera utile à la fin pour comparer la prédiction et l'espèce réelle\n",
    "\n",
    "# sélection des données de test\n",
    "# on fait pareil que précédemment, mais pour tester l'algorithme (l'IA) une fois qu'il sera optimisé\n",
    "x_test  = data_test[['petal_length','petal_width']]\n",
    "y_test  = data_test[['setosa','versicolor','virginica']]\n",
    "y_test_species = data_test[['species']]\n",
    "\n",
    "\n",
    "# Affichage pour mieux comprendre ce qu'on vient de créer\n",
    "print('x_train : ',x_train.shape)\n",
    "display(x_train)\n",
    "print('y_train : ',y_train.shape,'y_train_species : ',y_train_species.shape)\n",
    "display(y_train, y_train_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b4f9b-46d6-4988-a3f3-ab75734b6ffa",
   "metadata": {},
   "source": [
    "#### 3.2.c. Adaptation des données à la régression logistique par le réseau de neurones\n",
    "Regardons d'abord quelle sont la longueur et largeur moyennes des pétales. On va pour cela utiliser la fonction  `describe()` de `pandas` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b587e381-118c-4cd7-b891-6bcc1bb9f4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_aacfb\">\n",
       "  <caption>Training set avant normalisation</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_aacfb_level0_col0\" class=\"col_heading level0 col0\" >petal_length</th>\n",
       "      <th id=\"T_aacfb_level0_col1\" class=\"col_heading level0 col1\" >petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_aacfb_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_aacfb_row0_col0\" class=\"data row0 col0\" >120.00</td>\n",
       "      <td id=\"T_aacfb_row0_col1\" class=\"data row0 col1\" >120.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aacfb_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_aacfb_row1_col0\" class=\"data row1 col0\" >3.78</td>\n",
       "      <td id=\"T_aacfb_row1_col1\" class=\"data row1 col1\" >1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aacfb_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_aacfb_row2_col0\" class=\"data row2 col0\" >1.75</td>\n",
       "      <td id=\"T_aacfb_row2_col1\" class=\"data row2 col1\" >0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aacfb_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_aacfb_row3_col0\" class=\"data row3 col0\" >1.00</td>\n",
       "      <td id=\"T_aacfb_row3_col1\" class=\"data row3 col1\" >0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aacfb_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_aacfb_row4_col0\" class=\"data row4 col0\" >1.60</td>\n",
       "      <td id=\"T_aacfb_row4_col1\" class=\"data row4 col1\" >0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aacfb_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_aacfb_row5_col0\" class=\"data row5 col0\" >4.30</td>\n",
       "      <td id=\"T_aacfb_row5_col1\" class=\"data row5 col1\" >1.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aacfb_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_aacfb_row6_col0\" class=\"data row6 col0\" >5.10</td>\n",
       "      <td id=\"T_aacfb_row6_col1\" class=\"data row6 col1\" >1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aacfb_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_aacfb_row7_col0\" class=\"data row7 col0\" >6.90</td>\n",
       "      <td id=\"T_aacfb_row7_col1\" class=\"data row7 col1\" >2.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc502048340>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(x_train.describe().style.format(\"{0:.2f}\").set_caption(\"Training set avant normalisation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d095f3-50d2-42de-be9f-ec23941dcec9",
   "metadata": {},
   "source": [
    "En moyenne, les pétales sont trois plus longs que larges. Le risque encouru lors de l'apprentissage est que l'algorithme considère que la longueur est une caractéristique plus importante que la largeur. Il est d'usage d'appliquer une procédure de standardisation afin d'éviter cet écueil. C'est l'objet des quelques lignes ci-dessous, qui exploitent les outils de normalisation des caractéristiques fournis par la libraririe `scikit learn`. \n",
    "<div class=\"warn\"><b>Attention ! </b>Toute donnée soumise à l'algorithme doit au préalable être standardisée à l'aide de la commande <code>scaler.transform()</code> de la librairie <code>scikit learn</code></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70779f6b-4820-427f-a2ce-96ac775809a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(x_train.values)\n",
    "x_trainS = scaler.transform(x_train.values) #returns a numpy array\n",
    "x_testS = scaler.transform(x_test.values) #returns a numpy array\n",
    "#conversion into a dataframe just for display and plotting purpose\n",
    "x_trainDF = pd.DataFrame(x_trainS, columns=x_train.columns, index=x_train.index)\n",
    "x_testDF = pd.DataFrame(x_testS, columns=x_test.columns, index=x_test.index)\n",
    "#\n",
    "x_train = x_trainS\n",
    "x_test = x_testS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88d8f976-af50-4e8c-b1e6-9670c4ccd55d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_eb376\">\n",
       "  <caption>Training set après standardisation</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_eb376_level0_col0\" class=\"col_heading level0 col0\" >petal_length</th>\n",
       "      <th id=\"T_eb376_level0_col1\" class=\"col_heading level0 col1\" >petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_eb376_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_eb376_row0_col0\" class=\"data row0 col0\" >120.00</td>\n",
       "      <td id=\"T_eb376_row0_col1\" class=\"data row0 col1\" >120.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb376_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_eb376_row1_col0\" class=\"data row1 col0\" >-0.00</td>\n",
       "      <td id=\"T_eb376_row1_col1\" class=\"data row1 col1\" >-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb376_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_eb376_row2_col0\" class=\"data row2 col0\" >1.00</td>\n",
       "      <td id=\"T_eb376_row2_col1\" class=\"data row2 col1\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb376_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_eb376_row3_col0\" class=\"data row3 col0\" >-1.59</td>\n",
       "      <td id=\"T_eb376_row3_col1\" class=\"data row3 col1\" >-1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb376_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_eb376_row4_col0\" class=\"data row4 col0\" >-1.25</td>\n",
       "      <td id=\"T_eb376_row4_col1\" class=\"data row4 col1\" >-1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb376_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_eb376_row5_col0\" class=\"data row5 col0\" >0.30</td>\n",
       "      <td id=\"T_eb376_row5_col1\" class=\"data row5 col1\" >0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb376_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_eb376_row6_col0\" class=\"data row6 col0\" >0.76</td>\n",
       "      <td id=\"T_eb376_row6_col1\" class=\"data row6 col1\" >0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb376_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_eb376_row7_col0\" class=\"data row7 col0\" >1.79</td>\n",
       "      <td id=\"T_eb376_row7_col1\" class=\"data row7 col1\" >1.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc502b5e700>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(x_trainDF.describe().style.format(\"{0:.2f}\").set_caption(\"Training set après standardisation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072458b6-6509-43f9-9358-bb08fcc7134e",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "La distribution renormalisée des longueurs et largeurs du training set vient d'être affichée. Analysez ces résultats et comparez-les à ceux calculés avant la standardisation.\n",
    "<br><b>NB.</b> Le test set a lui aussi été standardisé.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d9cd3-ebdc-4bb3-bb75-640600b7d2de",
   "metadata": {},
   "source": [
    "### 3.3. Modèle de réseau de neurones (ANN = artificial neural network)\n",
    "On va maintenant\n",
    "- définir un modèle de réseau de neurones\n",
    "- lancer son apprentissage (dit supervisé, car l'algorithme doit à apprendre à prédire à partir d'exemples annotés)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7111b5f-5db1-457a-a4ed-a93902334ebe",
   "metadata": {},
   "source": [
    "#### 3.3.a. Définition du modèle\n",
    "<div class=\"rq\">\n",
    "    \n",
    "On va définir le réseau de neurones. Rappelons d'abord l'architecture du réseau quon va évaluer ici :\n",
    "    <p style=\"text-align: center\"><img width=\"250px\" src=\"./svg/ANN-petales.png\" style=\"margin-left:auto; margin-right:auto; display: block;\" id=\"img_ANN_jpc\"/></p>\n",
    "\n",
    "Grâce à la simplicité de `Keras`, chaque couche de neurones est définie par une *unique* ligne. \n",
    "- la première définit la couche d'entrée. Le nombre de neurones (*NE*) est passé en paramètres du modèle\n",
    "- la 2ème et la 3ème sont les couches cachées, définies par le nombre de neurones qui les constituent et par leur mode d'activation\n",
    "- la 4ème ligne est la couche de sortie, définie par 3 neurones et par lafonction d'activation. Cette fonction `softmax` est adaptée à la classification\n",
    "\n",
    "Puis on indique à `Keras` de configurer le modèle (`compile`) :\n",
    "- L'optimiseur est la méthode de minimisation entre ce qu'on veut faire apprendre et ce que l'algorithme restitue pendant son apprentissage\n",
    "- On va minimiser l'écart entre probabilités connues (les 3 $y_k$) et probabilités prédites ($\\hat{y}_k$) . C'est la fonction `loss`. Pour une classification, il est recommandé d'utiliser la fonction `categorical cross entropy`, qui est un peu plus complexe en réalité que $y_k-\\hat{y}_k$, puisque son expression est(\\*) :\n",
    "$$-\\frac{1}{n}\\sum_{k}\\left(y_{k}\\log\\hat{y}_{k}(\\boldsymbol{\\theta})+(1-y_{k})\\log(1-\\hat{y}_{k}(\\boldsymbol{\\theta}))\\right)^{2}$$\n",
    "- on demande également au modèle de renvoyer la précision (`accuracy`) grâce à la variable `metrics`. Ça va permettre d'évaluer la capacité prédictive du modèle, puisque c'est basé sur le nombre d'identifications correctes et erronées (les vrais ou faux positifs ou négatifs)\n",
    "\n",
    "(\\*) le paramètre $\\boldsymbol{\\theta}$ représente l'ensemble des poids des connexions neuronales ainsi que l'ensemble des biais associé à chaque neurone \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "776ea4a4-a5f4-4898-886d-cec103569b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(NE): #NE = nombre de neurones d'entrée\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(NE, name='iLayer'))\n",
    "    model.add(keras.layers.Dense(7, activation='relu', name='hLayer1'))\n",
    "    model.add(keras.layers.Dense(5, activation='relu', name='hLayer2'))\n",
    "    model.add(keras.layers.Dense(3, activation='softmax', name='oLayer'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'categorical_crossentropy',\n",
    "                  metrics   = ['accuracy'] )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32938fc-ef05-46d6-9f26-80a727d0bc8f",
   "metadata": {},
   "source": [
    "#### 3.3.b. Apprentissage supervisé du réseau de neurones\n",
    "C'est maitenant qu'on lance l'apprentissage.\n",
    "- on définit d'abord le modèle `ANNmodel=define_model( (2,)) `, où 2 = 2 neurones d'entrée (largeur et longueur)\n",
    "- on lance son optimisation `ANNmodel.fit`. L'algorithme (l'IA) optimal ainsi que les algorithmes intermédiaires vont être sauvegardés en tant que `ANNhistory`\n",
    "\n",
    "<div class=\"warn\"><b>Qu'est-ce qui est sauvé au juste ?</b>\n",
    "Les poids des connexions et les biais, dont l'optimisation a conduit à améliorer la reconnaissance des iris\n",
    "</div>\n",
    "\n",
    "Les principaux paramètres de `ANNmodelfit` sont\n",
    "- les caractéristiques d'entrée : `x_train`\n",
    "- les classes qu'on veut prédire en fonction des caractéristiques d'entrée : `y_train`\n",
    "- le nombre maximal de cycle d'apprentissage (`epochs`)\n",
    "- les jeux de données qui vont permettre à chaque pas d'apprentissage (d'optimisation) d'évaluer les performances de l'algorithme sur des données qui ne servent pas à l'apprentisssage (sorte de \"contrôle continu intégral et permanent\") : `x_test` et le résultat attendu, `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "877b00e0-83b3-4540-ad09-ffa631fde346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hLayer1 (Dense)              (None, 7)                 21        \n",
      "_________________________________________________________________\n",
      "hLayer2 (Dense)              (None, 5)                 40        \n",
      "_________________________________________________________________\n",
      "oLayer (Dense)               (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 79\n",
      "Trainable params: 79\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/700\n",
      "24/24 [==============================] - 1s 17ms/step - loss: 1.1578 - accuracy: 0.0337 - val_loss: 1.1094 - val_accuracy: 0.0333\n",
      "Epoch 2/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1.1168 - accuracy: 0.0181 - val_loss: 1.0622 - val_accuracy: 0.4000\n",
      "Epoch 3/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 1.0674 - accuracy: 0.3169 - val_loss: 1.0200 - val_accuracy: 0.4000\n",
      "Epoch 4/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1.0303 - accuracy: 0.3715 - val_loss: 0.9754 - val_accuracy: 0.4000\n",
      "Epoch 5/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.9892 - accuracy: 0.4187 - val_loss: 0.9298 - val_accuracy: 0.4000\n",
      "Epoch 6/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.9109 - accuracy: 0.5495 - val_loss: 0.8843 - val_accuracy: 0.4667\n",
      "Epoch 7/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.8986 - accuracy: 0.4466 - val_loss: 0.8438 - val_accuracy: 0.5667\n",
      "Epoch 8/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.8553 - accuracy: 0.5984 - val_loss: 0.8046 - val_accuracy: 0.6667\n",
      "Epoch 9/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.8108 - accuracy: 0.6931 - val_loss: 0.7699 - val_accuracy: 0.7667\n",
      "Epoch 10/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.7932 - accuracy: 0.7618 - val_loss: 0.7412 - val_accuracy: 0.8333\n",
      "Epoch 11/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.7530 - accuracy: 0.8220 - val_loss: 0.7146 - val_accuracy: 0.8333\n",
      "Epoch 12/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.7832 - accuracy: 0.7779 - val_loss: 0.6917 - val_accuracy: 0.8333\n",
      "Epoch 13/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.7593 - accuracy: 0.7987 - val_loss: 0.6702 - val_accuracy: 0.8333\n",
      "Epoch 14/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.7122 - accuracy: 0.8825 - val_loss: 0.6473 - val_accuracy: 0.8333\n",
      "Epoch 15/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.6515 - accuracy: 0.8407 - val_loss: 0.6260 - val_accuracy: 0.8333\n",
      "Epoch 16/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.6495 - accuracy: 0.8605 - val_loss: 0.6054 - val_accuracy: 0.8333\n",
      "Epoch 17/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5957 - accuracy: 0.8764 - val_loss: 0.5846 - val_accuracy: 0.8333\n",
      "Epoch 18/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5916 - accuracy: 0.8488 - val_loss: 0.5638 - val_accuracy: 0.8333\n",
      "Epoch 19/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5728 - accuracy: 0.8808 - val_loss: 0.5417 - val_accuracy: 0.8333\n",
      "Epoch 20/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.5947 - accuracy: 0.9047 - val_loss: 0.5227 - val_accuracy: 0.8333\n",
      "Epoch 21/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.5899 - accuracy: 0.8233 - val_loss: 0.5027 - val_accuracy: 0.8333\n",
      "Epoch 22/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.4921 - accuracy: 0.8689 - val_loss: 0.4818 - val_accuracy: 0.8333\n",
      "Epoch 23/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.6162 - accuracy: 0.8018 - val_loss: 0.4627 - val_accuracy: 0.8333\n",
      "Epoch 24/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.5035 - accuracy: 0.8585 - val_loss: 0.4446 - val_accuracy: 0.8333\n",
      "Epoch 25/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5468 - accuracy: 0.8539 - val_loss: 0.4257 - val_accuracy: 0.8333\n",
      "Epoch 26/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5002 - accuracy: 0.8693 - val_loss: 0.4074 - val_accuracy: 0.8333\n",
      "Epoch 27/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3946 - accuracy: 0.9382 - val_loss: 0.3916 - val_accuracy: 0.8667\n",
      "Epoch 28/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4047 - accuracy: 0.8972 - val_loss: 0.3775 - val_accuracy: 0.9667\n",
      "Epoch 29/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4762 - accuracy: 0.8408 - val_loss: 0.3622 - val_accuracy: 0.9667\n",
      "Epoch 30/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.4259 - accuracy: 0.8947 - val_loss: 0.3478 - val_accuracy: 0.9667\n",
      "Epoch 31/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.3625 - accuracy: 0.9148 - val_loss: 0.3332 - val_accuracy: 0.9667\n",
      "Epoch 32/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.3997 - accuracy: 0.9026 - val_loss: 0.3209 - val_accuracy: 0.9667\n",
      "Epoch 33/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.3780 - accuracy: 0.9688 - val_loss: 0.3064 - val_accuracy: 1.0000\n",
      "Epoch 34/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3887 - accuracy: 0.9350 - val_loss: 0.2954 - val_accuracy: 1.0000\n",
      "Epoch 35/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3661 - accuracy: 0.9425 - val_loss: 0.2836 - val_accuracy: 1.0000\n",
      "Epoch 36/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3477 - accuracy: 0.9482 - val_loss: 0.2706 - val_accuracy: 1.0000\n",
      "Epoch 37/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.2992 - accuracy: 0.9693 - val_loss: 0.2599 - val_accuracy: 1.0000\n",
      "Epoch 38/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3054 - accuracy: 0.9516 - val_loss: 0.2491 - val_accuracy: 1.0000\n",
      "Epoch 39/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2891 - accuracy: 0.9548 - val_loss: 0.2384 - val_accuracy: 1.0000\n",
      "Epoch 40/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2592 - accuracy: 0.9626 - val_loss: 0.2270 - val_accuracy: 1.0000\n",
      "Epoch 41/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2817 - accuracy: 0.9568 - val_loss: 0.2154 - val_accuracy: 1.0000\n",
      "Epoch 42/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2361 - accuracy: 0.9386 - val_loss: 0.2033 - val_accuracy: 1.0000\n",
      "Epoch 43/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2960 - accuracy: 0.9273 - val_loss: 0.1933 - val_accuracy: 1.0000\n",
      "Epoch 44/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2751 - accuracy: 0.9589 - val_loss: 0.1832 - val_accuracy: 1.0000\n",
      "Epoch 45/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2496 - accuracy: 0.9288 - val_loss: 0.1732 - val_accuracy: 1.0000\n",
      "Epoch 46/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2194 - accuracy: 0.9557 - val_loss: 0.1640 - val_accuracy: 1.0000\n",
      "Epoch 47/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2070 - accuracy: 0.9467 - val_loss: 0.1545 - val_accuracy: 1.0000\n",
      "Epoch 48/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1833 - accuracy: 0.9729 - val_loss: 0.1460 - val_accuracy: 1.0000\n",
      "Epoch 49/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2384 - accuracy: 0.9387 - val_loss: 0.1378 - val_accuracy: 1.0000\n",
      "Epoch 50/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1919 - accuracy: 0.9487 - val_loss: 0.1311 - val_accuracy: 1.0000\n",
      "Epoch 51/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1787 - accuracy: 0.9664 - val_loss: 0.1239 - val_accuracy: 1.0000\n",
      "Epoch 52/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1867 - accuracy: 0.9528 - val_loss: 0.1175 - val_accuracy: 1.0000\n",
      "Epoch 53/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1493 - accuracy: 0.9668 - val_loss: 0.1121 - val_accuracy: 1.0000\n",
      "Epoch 54/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1570 - accuracy: 0.9718 - val_loss: 0.1058 - val_accuracy: 1.0000\n",
      "Epoch 55/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2541 - accuracy: 0.9069 - val_loss: 0.1011 - val_accuracy: 1.0000\n",
      "Epoch 56/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1937 - accuracy: 0.9298 - val_loss: 0.0967 - val_accuracy: 1.0000\n",
      "Epoch 57/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1642 - accuracy: 0.9503 - val_loss: 0.0919 - val_accuracy: 1.0000\n",
      "Epoch 58/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1827 - accuracy: 0.9338 - val_loss: 0.0884 - val_accuracy: 1.0000\n",
      "Epoch 59/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1458 - accuracy: 0.9631 - val_loss: 0.0849 - val_accuracy: 1.0000\n",
      "Epoch 60/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1740 - accuracy: 0.9592 - val_loss: 0.0823 - val_accuracy: 1.0000\n",
      "Epoch 61/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1071 - accuracy: 0.9805 - val_loss: 0.0791 - val_accuracy: 1.0000\n",
      "Epoch 62/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1729 - accuracy: 0.9350 - val_loss: 0.0764 - val_accuracy: 1.0000\n",
      "Epoch 63/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1278 - accuracy: 0.9625 - val_loss: 0.0740 - val_accuracy: 1.0000\n",
      "Epoch 64/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1813 - accuracy: 0.9134 - val_loss: 0.0712 - val_accuracy: 1.0000\n",
      "Epoch 65/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1108 - accuracy: 0.9689 - val_loss: 0.0690 - val_accuracy: 1.0000\n",
      "Epoch 66/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1563 - accuracy: 0.9298 - val_loss: 0.0665 - val_accuracy: 1.0000\n",
      "Epoch 67/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1488 - accuracy: 0.9429 - val_loss: 0.0648 - val_accuracy: 1.0000\n",
      "Epoch 68/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1352 - accuracy: 0.9658 - val_loss: 0.0629 - val_accuracy: 1.0000\n",
      "Epoch 69/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1032 - accuracy: 0.9630 - val_loss: 0.0612 - val_accuracy: 1.0000\n",
      "Epoch 70/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1094 - accuracy: 0.9628 - val_loss: 0.0590 - val_accuracy: 1.0000\n",
      "Epoch 71/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1072 - accuracy: 0.9480 - val_loss: 0.0578 - val_accuracy: 1.0000\n",
      "Epoch 72/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0846 - accuracy: 0.9728 - val_loss: 0.0559 - val_accuracy: 1.0000\n",
      "Epoch 73/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0775 - accuracy: 0.9758 - val_loss: 0.0557 - val_accuracy: 1.0000\n",
      "Epoch 74/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1570 - accuracy: 0.9474 - val_loss: 0.0535 - val_accuracy: 1.0000\n",
      "Epoch 75/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0938 - accuracy: 0.9599 - val_loss: 0.0521 - val_accuracy: 1.0000\n",
      "Epoch 76/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1464 - accuracy: 0.9276 - val_loss: 0.0507 - val_accuracy: 1.0000\n",
      "Epoch 77/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0978 - accuracy: 0.9523 - val_loss: 0.0498 - val_accuracy: 1.0000\n",
      "Epoch 78/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1250 - accuracy: 0.9317 - val_loss: 0.0488 - val_accuracy: 1.0000\n",
      "Epoch 79/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1277 - accuracy: 0.9287 - val_loss: 0.0476 - val_accuracy: 1.0000\n",
      "Epoch 80/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1091 - accuracy: 0.9484 - val_loss: 0.0465 - val_accuracy: 1.0000\n",
      "Epoch 81/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.9555 - val_loss: 0.0465 - val_accuracy: 1.0000\n",
      "Epoch 82/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1133 - accuracy: 0.9557 - val_loss: 0.0447 - val_accuracy: 1.0000\n",
      "Epoch 83/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.1296 - accuracy: 0.9434 - val_loss: 0.0444 - val_accuracy: 1.0000\n",
      "Epoch 84/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1003 - accuracy: 0.9623 - val_loss: 0.0429 - val_accuracy: 1.0000\n",
      "Epoch 85/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1565 - accuracy: 0.9120 - val_loss: 0.0430 - val_accuracy: 1.0000\n",
      "Epoch 86/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1187 - accuracy: 0.9388 - val_loss: 0.0419 - val_accuracy: 1.0000\n",
      "Epoch 87/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1671 - accuracy: 0.9055 - val_loss: 0.0407 - val_accuracy: 1.0000\n",
      "Epoch 88/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0897 - accuracy: 0.9569 - val_loss: 0.0400 - val_accuracy: 1.0000\n",
      "Epoch 89/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.1245 - accuracy: 0.9436 - val_loss: 0.0400 - val_accuracy: 1.0000\n",
      "Epoch 90/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0887 - accuracy: 0.9536 - val_loss: 0.0390 - val_accuracy: 1.0000\n",
      "Epoch 91/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1053 - accuracy: 0.9542 - val_loss: 0.0389 - val_accuracy: 1.0000\n",
      "Epoch 92/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0974 - accuracy: 0.9513 - val_loss: 0.0381 - val_accuracy: 1.0000\n",
      "Epoch 93/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1047 - accuracy: 0.9409 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
      "Epoch 94/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1469 - accuracy: 0.9130 - val_loss: 0.0369 - val_accuracy: 1.0000\n",
      "Epoch 95/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1134 - accuracy: 0.9416 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
      "Epoch 96/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1241 - accuracy: 0.9327 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
      "Epoch 97/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1008 - accuracy: 0.9413 - val_loss: 0.0356 - val_accuracy: 1.0000\n",
      "Epoch 98/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0977 - accuracy: 0.9404 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
      "Epoch 99/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0942 - accuracy: 0.9453 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
      "Epoch 100/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0774 - accuracy: 0.9595 - val_loss: 0.0342 - val_accuracy: 1.0000\n",
      "Epoch 101/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1081 - accuracy: 0.9366 - val_loss: 0.0336 - val_accuracy: 1.0000\n",
      "Epoch 102/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0895 - accuracy: 0.9561 - val_loss: 0.0341 - val_accuracy: 1.0000\n",
      "Epoch 103/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0785 - accuracy: 0.9563 - val_loss: 0.0329 - val_accuracy: 1.0000\n",
      "Epoch 104/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0991 - accuracy: 0.9415 - val_loss: 0.0337 - val_accuracy: 1.0000\n",
      "Epoch 105/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1000 - accuracy: 0.9363 - val_loss: 0.0324 - val_accuracy: 1.0000\n",
      "Epoch 106/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0873 - accuracy: 0.9476 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
      "Epoch 107/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1265 - accuracy: 0.9317 - val_loss: 0.0324 - val_accuracy: 1.0000\n",
      "Epoch 108/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0881 - accuracy: 0.9570 - val_loss: 0.0321 - val_accuracy: 1.0000\n",
      "Epoch 109/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0688 - accuracy: 0.9709 - val_loss: 0.0320 - val_accuracy: 1.0000\n",
      "Epoch 110/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1301 - accuracy: 0.9127 - val_loss: 0.0307 - val_accuracy: 1.0000\n",
      "Epoch 111/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1023 - accuracy: 0.9304 - val_loss: 0.0310 - val_accuracy: 1.0000\n",
      "Epoch 112/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0954 - accuracy: 0.9421 - val_loss: 0.0305 - val_accuracy: 1.0000\n",
      "Epoch 113/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0740 - accuracy: 0.9551 - val_loss: 0.0304 - val_accuracy: 1.0000\n",
      "Epoch 114/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1385 - accuracy: 0.9227 - val_loss: 0.0300 - val_accuracy: 1.0000\n",
      "Epoch 115/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0791 - accuracy: 0.9571 - val_loss: 0.0296 - val_accuracy: 1.0000\n",
      "Epoch 116/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0809 - accuracy: 0.9559 - val_loss: 0.0293 - val_accuracy: 1.0000\n",
      "Epoch 117/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0554 - accuracy: 0.9730 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
      "Epoch 118/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0830 - accuracy: 0.9559 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
      "Epoch 119/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1007 - accuracy: 0.9364 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
      "Epoch 120/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0826 - accuracy: 0.9539 - val_loss: 0.0289 - val_accuracy: 1.0000\n",
      "Epoch 121/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0678 - accuracy: 0.9626 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
      "Epoch 122/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0830 - accuracy: 0.9552 - val_loss: 0.0282 - val_accuracy: 1.0000\n",
      "Epoch 123/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0752 - accuracy: 0.9502 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
      "Epoch 124/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0897 - accuracy: 0.9452 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
      "Epoch 125/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0969 - accuracy: 0.9403 - val_loss: 0.0280 - val_accuracy: 1.0000\n",
      "Epoch 126/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0908 - accuracy: 0.9449 - val_loss: 0.0280 - val_accuracy: 1.0000\n",
      "Epoch 127/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0963 - accuracy: 0.9309 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
      "Epoch 128/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0787 - accuracy: 0.9546 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
      "Epoch 129/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0796 - accuracy: 0.9492 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
      "Epoch 130/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0941 - accuracy: 0.9447 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
      "Epoch 131/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1161 - accuracy: 0.9301 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
      "Epoch 132/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0958 - accuracy: 0.9330 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
      "Epoch 133/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0726 - accuracy: 0.9484 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "Epoch 134/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1000 - accuracy: 0.9436 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
      "Epoch 135/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0805 - accuracy: 0.9470 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 136/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1361 - accuracy: 0.9160 - val_loss: 0.0267 - val_accuracy: 1.0000\n",
      "Epoch 137/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1248 - accuracy: 0.9163 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
      "Epoch 138/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1159 - accuracy: 0.9225 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
      "Epoch 139/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0993 - accuracy: 0.9330 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
      "Epoch 140/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1020 - accuracy: 0.9295 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 141/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0511 - accuracy: 0.9735 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 142/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1175 - accuracy: 0.9180 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
      "Epoch 143/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0661 - accuracy: 0.9677 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
      "Epoch 144/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1296 - accuracy: 0.9123 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 145/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0755 - accuracy: 0.9555 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 146/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1053 - accuracy: 0.9311 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "Epoch 147/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0899 - accuracy: 0.9357 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
      "Epoch 148/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0908 - accuracy: 0.9476 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 149/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0764 - accuracy: 0.9533 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
      "Epoch 150/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0868 - accuracy: 0.9499 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 151/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0925 - accuracy: 0.9421 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 152/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0932 - accuracy: 0.9394 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 153/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1140 - accuracy: 0.9201 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
      "Epoch 154/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1184 - accuracy: 0.9125 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 155/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1100 - accuracy: 0.9233 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 156/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0989 - accuracy: 0.9344 - val_loss: 0.0257 - val_accuracy: 1.0000\n",
      "Epoch 157/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0546 - accuracy: 0.9686 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 158/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0376 - accuracy: 0.9869 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 159/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1061 - accuracy: 0.9412 - val_loss: 0.0246 - val_accuracy: 1.0000\n",
      "Epoch 160/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0754 - accuracy: 0.9684 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 161/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.1053 - accuracy: 0.9212 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
      "Epoch 162/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0897 - accuracy: 0.9615 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
      "Epoch 163/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0644 - accuracy: 0.9790 - val_loss: 0.0238 - val_accuracy: 1.0000\n",
      "Epoch 164/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0698 - accuracy: 0.9589 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 165/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0880 - accuracy: 0.9486 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 166/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0891 - accuracy: 0.9476 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "Epoch 167/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1088 - accuracy: 0.9344 - val_loss: 0.0237 - val_accuracy: 1.0000\n",
      "Epoch 168/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1343 - accuracy: 0.9310 - val_loss: 0.0241 - val_accuracy: 1.0000\n",
      "Epoch 169/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0722 - accuracy: 0.9670 - val_loss: 0.0241 - val_accuracy: 1.0000\n",
      "Epoch 170/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0729 - accuracy: 0.9618 - val_loss: 0.0238 - val_accuracy: 1.0000\n",
      "Epoch 171/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0468 - accuracy: 0.9789 - val_loss: 0.0231 - val_accuracy: 1.0000\n",
      "Epoch 172/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.1060 - accuracy: 0.9449 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
      "Epoch 173/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0928 - accuracy: 0.9468 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
      "Epoch 174/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0767 - accuracy: 0.9613 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
      "Epoch 175/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1191 - accuracy: 0.9230 - val_loss: 0.0236 - val_accuracy: 1.0000\n",
      "Epoch 176/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0533 - accuracy: 0.9743 - val_loss: 0.0229 - val_accuracy: 1.0000\n",
      "Epoch 177/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0982 - accuracy: 0.9347 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
      "Epoch 178/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0993 - accuracy: 0.9466 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
      "Epoch 179/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0922 - accuracy: 0.9559 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
      "Epoch 180/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0972 - accuracy: 0.9437 - val_loss: 0.0241 - val_accuracy: 1.0000\n",
      "Epoch 181/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0685 - accuracy: 0.9687 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
      "Epoch 182/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1032 - accuracy: 0.9464 - val_loss: 0.0235 - val_accuracy: 1.0000\n",
      "Epoch 183/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0985 - accuracy: 0.9355 - val_loss: 0.0231 - val_accuracy: 1.0000\n",
      "Epoch 184/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0736 - accuracy: 0.9506 - val_loss: 0.0239 - val_accuracy: 1.0000\n",
      "Epoch 185/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0670 - accuracy: 0.9662 - val_loss: 0.0230 - val_accuracy: 1.0000\n",
      "Epoch 186/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0780 - accuracy: 0.9613 - val_loss: 0.0237 - val_accuracy: 1.0000\n",
      "Epoch 187/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0910 - accuracy: 0.9384 - val_loss: 0.0237 - val_accuracy: 1.0000\n",
      "Epoch 188/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0620 - accuracy: 0.9664 - val_loss: 0.0238 - val_accuracy: 1.0000\n",
      "Epoch 189/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0565 - accuracy: 0.9714 - val_loss: 0.0236 - val_accuracy: 1.0000\n",
      "Epoch 190/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0708 - accuracy: 0.9613 - val_loss: 0.0228 - val_accuracy: 1.0000\n",
      "Epoch 191/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0983 - accuracy: 0.9319 - val_loss: 0.0224 - val_accuracy: 1.0000\n",
      "Epoch 192/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0678 - accuracy: 0.9629 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
      "Epoch 193/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.1203 - accuracy: 0.9162 - val_loss: 0.0235 - val_accuracy: 1.0000\n",
      "Epoch 194/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0527 - accuracy: 0.9735 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
      "Epoch 195/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1087 - accuracy: 0.9396 - val_loss: 0.0221 - val_accuracy: 1.0000\n",
      "Epoch 196/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0648 - accuracy: 0.9648 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
      "Epoch 197/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1296 - accuracy: 0.9126 - val_loss: 0.0235 - val_accuracy: 1.0000\n",
      "Epoch 198/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0940 - accuracy: 0.9306 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
      "Epoch 199/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0584 - accuracy: 0.9687 - val_loss: 0.0237 - val_accuracy: 1.0000\n",
      "Epoch 200/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1031 - accuracy: 0.9416 - val_loss: 0.0219 - val_accuracy: 1.0000\n",
      "Epoch 201/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0461 - accuracy: 0.9800 - val_loss: 0.0240 - val_accuracy: 1.0000\n",
      "Epoch 202/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0943 - accuracy: 0.9397 - val_loss: 0.0231 - val_accuracy: 1.0000\n",
      "Epoch 203/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0998 - accuracy: 0.9400 - val_loss: 0.0230 - val_accuracy: 1.0000\n",
      "Epoch 204/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0613 - accuracy: 0.9718 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
      "Epoch 205/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1003 - accuracy: 0.9348 - val_loss: 0.0226 - val_accuracy: 1.0000\n",
      "Epoch 206/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0899 - accuracy: 0.9486 - val_loss: 0.0223 - val_accuracy: 1.0000\n",
      "Epoch 207/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0899 - accuracy: 0.9437 - val_loss: 0.0231 - val_accuracy: 1.0000\n",
      "Epoch 208/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0847 - accuracy: 0.9485 - val_loss: 0.0224 - val_accuracy: 1.0000\n",
      "Epoch 209/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0904 - accuracy: 0.9432 - val_loss: 0.0219 - val_accuracy: 1.0000\n",
      "Epoch 210/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0712 - accuracy: 0.9574 - val_loss: 0.0226 - val_accuracy: 1.0000\n",
      "Epoch 211/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1015 - accuracy: 0.9406 - val_loss: 0.0224 - val_accuracy: 1.0000\n",
      "Epoch 212/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0785 - accuracy: 0.9510 - val_loss: 0.0229 - val_accuracy: 1.0000\n",
      "Epoch 213/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0914 - accuracy: 0.9408 - val_loss: 0.0226 - val_accuracy: 1.0000\n",
      "Epoch 214/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0666 - accuracy: 0.9627 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
      "Epoch 215/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0626 - accuracy: 0.9627 - val_loss: 0.0237 - val_accuracy: 1.0000\n",
      "Epoch 216/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0592 - accuracy: 0.9716 - val_loss: 0.0216 - val_accuracy: 1.0000\n",
      "Epoch 217/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0823 - accuracy: 0.9612 - val_loss: 0.0218 - val_accuracy: 1.0000\n",
      "Epoch 218/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0693 - accuracy: 0.9606 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
      "Epoch 219/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1065 - accuracy: 0.9192 - val_loss: 0.0230 - val_accuracy: 1.0000\n",
      "Epoch 220/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0881 - accuracy: 0.9412 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
      "Epoch 221/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0689 - accuracy: 0.9631 - val_loss: 0.0213 - val_accuracy: 1.0000\n",
      "Epoch 222/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0981 - accuracy: 0.9397 - val_loss: 0.0219 - val_accuracy: 1.0000\n",
      "Epoch 223/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.1338 - accuracy: 0.9050 - val_loss: 0.0230 - val_accuracy: 1.0000\n",
      "Epoch 224/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1204 - accuracy: 0.9363 - val_loss: 0.0228 - val_accuracy: 1.0000\n",
      "Epoch 225/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0703 - accuracy: 0.9625 - val_loss: 0.0213 - val_accuracy: 1.0000\n",
      "Epoch 226/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1269 - accuracy: 0.9307 - val_loss: 0.0222 - val_accuracy: 1.0000\n",
      "Epoch 227/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1104 - accuracy: 0.9398 - val_loss: 0.0225 - val_accuracy: 1.0000\n",
      "Epoch 228/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0737 - accuracy: 0.9626 - val_loss: 0.0217 - val_accuracy: 1.0000\n",
      "Epoch 229/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0603 - accuracy: 0.9689 - val_loss: 0.0236 - val_accuracy: 1.0000\n",
      "Epoch 230/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0665 - accuracy: 0.9503 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
      "Epoch 231/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.1014 - accuracy: 0.9370 - val_loss: 0.0218 - val_accuracy: 1.0000\n",
      "Epoch 232/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1324 - accuracy: 0.8975 - val_loss: 0.0218 - val_accuracy: 1.0000\n",
      "Epoch 233/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0828 - accuracy: 0.9487 - val_loss: 0.0221 - val_accuracy: 1.0000\n",
      "Epoch 234/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0740 - accuracy: 0.9518 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
      "Epoch 235/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0672 - accuracy: 0.9604 - val_loss: 0.0225 - val_accuracy: 1.0000\n",
      "Epoch 00235: early stopping\n",
      "\n",
      "Duration :  00:00:29 346ms\n"
     ]
    }
   ],
   "source": [
    "vID.chrono_start()\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "ANNmodel=define_model( (2,)) # 2 neurones d'entrée\n",
    "ANNmodel.summary()\n",
    "vID.chrono_start()\n",
    "ANNhistory = ANNmodel.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs          = 700,\n",
    "                    batch_size      = 5,\n",
    "                    verbose         = 1,\n",
    "                    validation_data = (x_test, y_test),\n",
    "                    callbacks=[es])\n",
    "vID.chrono_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5583a598-8890-41f9-b52e-ff4cd70b519c",
   "metadata": {},
   "source": [
    "On va tracer l'évolution de la précision et de la fonction loss au cours des cycles d'apprentissage. On va le faire à la fois pour les données qui ont servi à l'apprentissage (*train*) et pour celles du \"contrôle continu\" (*test*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fdc7340-0493-4693-a01e-1ee3049c2247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc502051ee0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# c'est pratique d'utiliser un dataframe et les fonctions de tracé associées\n",
    "df=pd.DataFrame(data=ANNhistory.history)\n",
    "plt.rcParams[\"figure.figsize\"] = (14,6)\n",
    "# on va d'abord tracer les courbes de la fonction qui a été minimisée au cours de l'apprentissage\n",
    "figLOSS=df.plot(y=[\"loss\",\"val_loss\"],linestyle='-', marker='o',fontsize=14)\n",
    "figLOSS.set_xlabel('epoch',fontdict={'fontsize':16})\n",
    "figLOSS.set_ylabel('loss',fontdict={'fontsize':16})\n",
    "figLOSS.set_ylim([0.0,1.0])\n",
    "figLOSS.set_title(\"Fonction minimisée au cours de l'apprentissage (doit -> 0)\", size=20)\n",
    "figLOSS.legend(loc='upper right', shadow=True, fontsize='x-large')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96e02ed-8735-4180-b0e6-06c02e866e00",
   "metadata": {},
   "source": [
    "#conversion into a dataframe<div style=\"warn\">\n",
    "<li> <span style=\"color:#1a6495\">l'algorithme apprend de mieux en mieux à reconnaître les iris au cours des cycles d'apprentissage (courbe \"loss\")</span>\n",
    "<li> <span style=\"color:#c1600b\">il est aussi capable de convenablement classer les espèces qu'il ne connaît pas (c'est-à-dire qui n'ont pas été utilisées pour son apprentissage, courbe \"val_loss\")</span>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ba66cf2-c7fb-4702-be56-66fac1b8de5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc500554df0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# création d'un nouveau dataFrame qui permet de juger de l'évolution de la précision de l'algortihme\n",
    "figACC=df.plot(y=[\"accuracy\",\"val_accuracy\"],linestyle='-', marker='o',fontsize=14)\n",
    "figACC.set_xlabel('epoch',fontdict={'fontsize':16})\n",
    "figACC.set_ylabel('mae',fontdict={'fontsize':16})\n",
    "figACC.set_ylim([0.0,1.0])\n",
    "figACC.set_title(\"Précision de l'algorithme (doit -> 1)\", size=20)\n",
    "figACC.legend(loc='lower right', shadow=True, fontsize='x-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de0ca33-cd36-43fa-a522-bf8cf0d38657",
   "metadata": {},
   "source": [
    "<div style=\"warn\">\n",
    "<li> <span style=\"color:#1a6495\">on a confirmation que l'algorithme apprend de mieux en mieux à reconnaître les iris au cours des cycles d'apprentissage (courbe \"accuracy\"). Il ne commet quasiment aucune erreur en fin d'apprentissage puisque ce paramètre tend vers 1.</span>\n",
    "<li> <span style=\"color:#c1600b\">la courbe \"val_accuracy\" montre que l'algorithme commet également peu d'erreurs d'identification des espèces qui n'ont pas servi à son apprentissage</span>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916c59d-3cc8-466f-b776-ec09e2038f9a",
   "metadata": {},
   "source": [
    "### 3.4. Évaluation de la précision du réseau de neurones après apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f927a6-ce28-443a-b4f9-0c47140e51ef",
   "metadata": {},
   "source": [
    "#### 3.4.a. Évaluation numérique globale\n",
    "On introduit ici la fonction `evaluate` de `Keras`. On lui fournit un jeu de données d'entrée et le résultat attendu. Elle va se servir du modèle optimisé (car on fait appel à `ANNmodel.evaluate`) afin de renvoyer les valeurs des erreurs \"loss\" et \"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe7d2f5f-14a3-4484-9cc6-69a080e0212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mx_train / loss      : 0.0840\u001b[0m\n",
      "\u001b[92mx_train/ accuracy  : 0.9500\u001b[0m\n",
      "\n",
      "\u001b[94mx_train / loss      : 0.0225\u001b[0m\n",
      "\u001b[94mx_train/ accuracy  : 1.0000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "evalANN_on_TrainSet = ANNmodel.evaluate(x_train, y_train, verbose=0)\n",
    "print(f\"{color.GREEN}x_train / loss      : {evalANN_on_TrainSet[0]:5.4f}{color.OFF}\")\n",
    "print(f\"{color.GREEN}x_train/ accuracy  : {evalANN_on_TrainSet[1]:5.4f}{color.OFF}\")\n",
    "print()\n",
    "evalANN_on_TestSet = ANNmodel.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"{color.BLUE}x_train / loss      : {evalANN_on_TestSet[0]:5.4f}{color.OFF}\")\n",
    "print(f\"{color.BLUE}x_train/ accuracy  : {evalANN_on_TestSet[1]:5.4f}{color.OFF}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f108f68d-e9e0-45f1-8f7c-8e8fc80ba38d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.4.b. Utilisation de l'algorithme ANN comme prédicteur\n",
    "On avait posé cet exercice dans la deuxième partie (\"statistiques et régression\")\n",
    "<div class=\"rq\"><b>Problème. </b>\n",
    "Un botaniste a découvert au fond de son jardin un Iris de la famille <i>setosa</i>, dont les pétales sont de largeur 0.25 cm.\n",
    "<ol>\n",
    "    <li> quelle est la longueur de pétale peut-on prédire, à partir de la base de données \"iris\"?\n",
    "    <li> la longueur mesurée est en fait de 1.45 cm. Quelle est l'erreur quadratique commise par le modèle statistique ?\n",
    "</ol> \n",
    "</div>\n",
    "<br>\n",
    "<div class=\"warn\"><b>Peut-on répondre à ces questions à l'aide du réseau de neurones optimisé?</b>\n",
    "<br>Non. Ce modèle statistique n'est pas conçu pour ça.</div>\n",
    "<br>\n",
    "On va en revanche pouvoir répondre à la question suivante:\n",
    "<div class=\"rq\"><b>Problème. </b>\n",
    "À l'occasion d'une randonnée, vous venez de découvrir un Iris, dont les pétales sont de dimension moyenne 1.45 cm x 0.25 cm. Vous êtes nul en botanique, mais grâce aux petites applications d'IA \"maison\" que vous avez sauvées sur votre smartphone, vous allez en déduire de quelle espèce il s'agit.\n",
    "</div>\n",
    "\n",
    "On va simplement utiliser la fonction `predict`, qui va renvoyer trois valeurs de probabilité (dans l'ordre setosa, versicolor, virginica) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f255f4bd-66e3-48d5-adc2-f10c7352fe2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilités: [[8.3257782e-04 3.5322297e-02 9.6384507e-01]]\n"
     ]
    }
   ],
   "source": [
    "# attention : les données sont entrées dans l'ordre longueur puis largeur, sous forme de tableau au format numpy\n",
    "sample = [0.25, 1.45]\n",
    "print(f\"probabilités: {ANNmodel.predict([sample])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f898e5-43fa-4f6b-b87a-9592f05f92d8",
   "metadata": {},
   "source": [
    "<div class=\"warn\">L'algorithme n'a pas de doute sur le fait que c'est un iris de type <i>virginica</i>. <b>Êtes-vous d'accord?</b> (on suggère de se reporter à la figure reliant longueur et largeur, reportée dans l'introduction)<br>\n",
    "<br>\n",
    "En fait, on a oublié une étape préliminaire, la <b><span style=\"color:red\">standardisation des données...</span></b>\n",
    "<br>L'algorithme a été optimisé sur la base de données d'entrées standardisées. Toute nouvelle donnée qui lui est soumise doit elle aussi être standardisée. On va pour cela à nouveau utiliser la commande <code>scaler.transform()</code> de la librairie <code>scikit learn</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1b6f096-04c5-4679-8d5d-204ec3c620b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "échantillon avant la standardisation : [0.25, 1.45]\n",
      "probabilités avant standardisation: [[8.3257782e-04 3.5322297e-02 9.6384507e-01]]\n",
      "\n",
      "échantillon après la standardisation : [[-2.02375117  0.3284725 ]]\n",
      "probabilités après standardisation: [[0.9677852  0.03017354 0.00204137]]\n"
     ]
    }
   ],
   "source": [
    "sample = [0.25, 1.45]\n",
    "print(f\"échantillon avant la standardisation : {sample}\")\n",
    "print(f\"probabilités avant standardisation: {ANNmodel.predict([sample])}\")\n",
    "print()\n",
    "\n",
    "sample = scaler.transform([sample])\n",
    "print(f\"échantillon après la standardisation : {sample}\")\n",
    "print(f\"probabilités après standardisation: {ANNmodel.predict([sample])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b214e5e0-0213-4bf6-9519-6cabdb1850f0",
   "metadata": {},
   "source": [
    "<div class=\"warn\">Ça change tout. Et c'est maintenant une prédiction correcte (<i>en principe, sauf si l'algorithme a par malchance mal appris sa leçon. il n'y a aucune assurance avec un si petit jeu de données d'apprentissage</i>).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9c4c51-26fa-47b5-85e9-2e67578e6f82",
   "metadata": {},
   "source": [
    "On va utiliser dans le paragraphe suivant les fonctions `argmax` de `numpy `, ainsi que la fonction `unique` de `pandas`. On va regarder leur comportement ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43bcb78a-3835-4c77-93f1-ace3b0120d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probas:  [[0.9677852  0.03017354 0.00204137]]\n",
      "argmax(probas):  0\n"
     ]
    }
   ],
   "source": [
    "probas = ANNmodel.predict([sample])\n",
    "print(\"probas: \", probas)\n",
    "print(\"argmax(probas): \",np.argmax(probas)) #va renvoyer l'indice de l'élément qui a la valeur la plus élevée de toute la liste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126cf2e8-9d12-4fee-ade4-cf56201b7a6a",
   "metadata": {},
   "source": [
    "`unique`, dont on montre l'utilité ci-dessous, a aussi été utilisé pour fabriquer les dernières colonnes de la base de données utilisée dans ce notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "44b6135c-cfd2-49df-8c3b-f815e28f23b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "usp = dfi['species'].unique() # usp = Unique SPecies\n",
    "print(usp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1993958-03e2-4a04-b00a-8114584e3ba2",
   "metadata": {},
   "source": [
    "C'est-à-dire que l'élement 0 de usp contient le nom de l'espèce\n",
    "\n",
    "Donc si on utilise la résultat de `argmax(probas)` en argument de usp, ça va directement renvoyer le nom de l'espèce plutôt qu'une liste de probabilités :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "531f7e1a-2318-46c1-9f26-da55123279ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l'iris observé est de type : setosa\n"
     ]
    }
   ],
   "source": [
    "print(\"l'iris observé est de type :\",usp[np.argmax(probas)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149669f-0ee6-4f8f-9f40-1cb91e4929ad",
   "metadata": {},
   "source": [
    "#### 3.4.c. Comportement du modèle vis-à-vis de chaque espèce d'iris\n",
    "La partie ci-dessous est un peu longue, mais pas compliquée. L'objectif est d'afficher les probailités pour chacun des échantillons du jeu d'apprentissage (train). Elles sont converties en espèce grâce à `argmax` et à la liste `usp`. On peut ainsi directement comparer avec les espèces réellement observées. La fin du script permet de comptabiliser les erreurs.\n",
    "\n",
    "Puis il est fait de même pour les échantillons du jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35c740a5-53e5-4776-8cea-b53af7589366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[92mCatégories uniques d'iris :\u001b[0m ['setosa' 'versicolor' 'virginica']\n",
      "\u001b[1m\u001b[94mTest Set\u001b[0m. Comparaison entre espèce prédite et espèce observée\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "      <th>Espèce prédite</th>\n",
       "      <th>Espèce observée</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.63</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.27</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.22</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.03</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.06</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.10</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.82</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.11</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.82</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "      <td>virginica</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.36</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.72</td>\n",
       "      <td>virginica</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.07</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>virginica</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     setosa  versicolor  virginica Espèce prédite Espèce observée\n",
       "108    0.00        0.00       0.99      virginica       virginica\n",
       "143    0.00        0.00       1.00      virginica       virginica\n",
       "88     0.00        1.00       0.00     versicolor      versicolor\n",
       "35     1.00        0.00       0.00         setosa          setosa\n",
       "100    0.00        0.00       1.00      virginica       virginica\n",
       "31     1.00        0.00       0.00         setosa          setosa\n",
       "62     0.00        1.00       0.00     versicolor      versicolor\n",
       "20     1.00        0.00       0.00         setosa          setosa\n",
       "74     0.00        1.00       0.00     versicolor      versicolor\n",
       "49     1.00        0.00       0.00         setosa          setosa\n",
       "95     0.00        1.00       0.00     versicolor      versicolor\n",
       "82     0.00        1.00       0.00     versicolor      versicolor\n",
       "7      1.00        0.00       0.00         setosa          setosa\n",
       "134    0.00        0.36       0.63      virginica       virginica\n",
       "101    0.00        0.03       0.97      virginica       virginica\n",
       "55     0.00        0.99       0.01     versicolor      versicolor\n",
       "33     1.00        0.00       0.00         setosa          setosa\n",
       "121    0.00        0.03       0.97      virginica       virginica\n",
       "93     0.00        0.99       0.00     versicolor      versicolor\n",
       "119    0.00        0.72       0.27     versicolor       virginica\n",
       "29     1.00        0.00       0.00         setosa          setosa\n",
       "110    0.00        0.01       0.99      virginica       virginica\n",
       "56     0.00        0.78       0.22     versicolor      versicolor\n",
       "138    0.00        0.25       0.75      virginica       virginica\n",
       "25     1.00        0.00       0.00         setosa          setosa\n",
       "91     0.00        0.97       0.03     versicolor      versicolor\n",
       "120    0.00        0.00       1.00      virginica       virginica\n",
       "76     0.00        0.94       0.06     versicolor      versicolor\n",
       "27     1.00        0.00       0.00         setosa          setosa\n",
       "37     1.00        0.00       0.00         setosa          setosa\n",
       "15     1.00        0.00       0.00         setosa          setosa\n",
       "97     0.00        1.00       0.00     versicolor      versicolor\n",
       "135    0.00        0.00       1.00      virginica       virginica\n",
       "86     0.00        0.90       0.10     versicolor      versicolor\n",
       "147    0.00        0.01       0.99      virginica       virginica\n",
       "123    0.00        0.18       0.82      virginica       virginica\n",
       "85     0.00        0.89       0.11     versicolor      versicolor\n",
       "60     0.00        1.00       0.00     versicolor      versicolor\n",
       "11     1.00        0.00       0.00         setosa          setosa\n",
       "127    0.00        0.18       0.82      virginica       virginica\n",
       "48     1.00        0.00       0.00         setosa          setosa\n",
       "116    0.00        0.02       0.98      virginica       virginica\n",
       "124    0.00        0.00       1.00      virginica       virginica\n",
       "107    0.00        0.00       1.00      virginica       virginica\n",
       "34     1.00        0.00       0.00         setosa          setosa\n",
       "46     1.00        0.00       0.00         setosa          setosa\n",
       "2      1.00        0.00       0.00         setosa          setosa\n",
       "12     1.00        0.00       0.00         setosa          setosa\n",
       "38     1.00        0.00       0.00         setosa          setosa\n",
       "39     1.00        0.00       0.00         setosa          setosa\n",
       "61     0.00        0.99       0.01     versicolor      versicolor\n",
       "50     0.00        0.96       0.04     versicolor      versicolor\n",
       "17     1.00        0.00       0.00         setosa          setosa\n",
       "118    0.00        0.00       1.00      virginica       virginica\n",
       "23     1.00        0.00       0.00         setosa          setosa\n",
       "83     0.00        0.40       0.60      virginica      versicolor\n",
       "5      1.00        0.00       0.00         setosa          setosa\n",
       "142    0.00        0.03       0.97      virginica       virginica\n",
       "59     0.00        1.00       0.00     versicolor      versicolor\n",
       "1      1.00        0.00       0.00         setosa          setosa\n",
       "99     0.00        1.00       0.00     versicolor      versicolor\n",
       "94     0.00        1.00       0.00     versicolor      versicolor\n",
       "133    0.00        0.63       0.36     versicolor       virginica\n",
       "77     0.00        0.27       0.72      virginica      versicolor\n",
       "140    0.00        0.00       1.00      virginica       virginica\n",
       "65     0.00        0.99       0.01     versicolor      versicolor\n",
       "104    0.00        0.00       1.00      virginica       virginica\n",
       "28     1.00        0.00       0.00         setosa          setosa\n",
       "128    0.00        0.00       1.00      virginica       virginica\n",
       "78     0.00        0.95       0.04     versicolor      versicolor\n",
       "132    0.00        0.00       1.00      virginica       virginica\n",
       "52     0.00        0.80       0.20     versicolor      versicolor\n",
       "72     0.00        0.80       0.20     versicolor      versicolor\n",
       "113    0.00        0.02       0.98      virginica       virginica\n",
       "36     1.00        0.00       0.00         setosa          setosa\n",
       "122    0.00        0.00       1.00      virginica       virginica\n",
       "145    0.00        0.00       1.00      virginica       virginica\n",
       "41     1.00        0.00       0.00         setosa          setosa\n",
       "44     1.00        0.00       0.00         setosa          setosa\n",
       "106    0.00        0.75       0.24     versicolor       virginica\n",
       "129    0.00        0.03       0.97      virginica       virginica\n",
       "21     1.00        0.00       0.00         setosa          setosa\n",
       "114    0.00        0.00       1.00      virginica       virginica\n",
       "54     0.00        0.93       0.07     versicolor      versicolor\n",
       "4      1.00        0.00       0.00         setosa          setosa\n",
       "141    0.00        0.00       1.00      virginica       virginica\n",
       "87     0.00        0.99       0.00     versicolor      versicolor\n",
       "45     1.00        0.00       0.00         setosa          setosa\n",
       "73     0.00        0.99       0.01     versicolor      versicolor\n",
       "70     0.00        0.25       0.75      virginica      versicolor\n",
       "115    0.00        0.00       1.00      virginica       virginica\n",
       "98     0.01        0.99       0.00     versicolor      versicolor\n",
       "148    0.00        0.00       1.00      virginica       virginica\n",
       "144    0.00        0.00       1.00      virginica       virginica\n",
       "30     1.00        0.00       0.00         setosa          setosa\n",
       "67     0.00        1.00       0.00     versicolor      versicolor\n",
       "71     0.00        1.00       0.00     versicolor      versicolor\n",
       "117    0.00        0.00       1.00      virginica       virginica\n",
       "8      1.00        0.00       0.00         setosa          setosa\n",
       "9      1.00        0.00       0.00         setosa          setosa\n",
       "43     1.00        0.00       0.00         setosa          setosa\n",
       "90     0.00        1.00       0.00     versicolor      versicolor\n",
       "47     1.00        0.00       0.00         setosa          setosa\n",
       "131    0.00        0.00       1.00      virginica       virginica\n",
       "66     0.00        0.95       0.04     versicolor      versicolor\n",
       "80     0.00        1.00       0.00     versicolor      versicolor\n",
       "102    0.00        0.00       1.00      virginica       virginica\n",
       "79     0.00        1.00       0.00     versicolor      versicolor\n",
       "53     0.00        1.00       0.00     versicolor      versicolor\n",
       "40     1.00        0.00       0.00         setosa          setosa\n",
       "125    0.00        0.00       1.00      virginica       virginica\n",
       "22     1.00        0.00       0.00         setosa          setosa\n",
       "69     0.00        1.00       0.00     versicolor      versicolor\n",
       "96     0.00        1.00       0.00     versicolor      versicolor\n",
       "89     0.00        1.00       0.00     versicolor      versicolor\n",
       "92     0.00        1.00       0.00     versicolor      versicolor\n",
       "14     1.00        0.00       0.00         setosa          setosa\n",
       "103    0.00        0.01       0.99      virginica       virginica\n",
       "10     1.00        0.00       0.00         setosa          setosa\n",
       "75     0.00        0.99       0.01     versicolor      versicolor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'erreurs : 6\n",
      "\n",
      "\u001b[1m\u001b[91mTest Set\u001b[0m. Comparaison entre espèce prédite et espèce observée.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "      <th>Espèce prédite</th>\n",
       "      <th>Espèce observée</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.91</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     setosa  versicolor  virginica Espèce prédite Espèce observée\n",
       "0      1.00        0.00       0.00         setosa          setosa\n",
       "3      1.00        0.00       0.00         setosa          setosa\n",
       "6      1.00        0.00       0.00         setosa          setosa\n",
       "13     1.00        0.00       0.00         setosa          setosa\n",
       "16     1.00        0.00       0.00         setosa          setosa\n",
       "18     1.00        0.00       0.00         setosa          setosa\n",
       "19     1.00        0.00       0.00         setosa          setosa\n",
       "24     1.00        0.00       0.00         setosa          setosa\n",
       "26     1.00        0.00       0.00         setosa          setosa\n",
       "32     1.00        0.00       0.00         setosa          setosa\n",
       "42     1.00        0.00       0.00         setosa          setosa\n",
       "51     0.00        0.95       0.04     versicolor      versicolor\n",
       "57     0.00        0.99       0.00     versicolor      versicolor\n",
       "58     0.00        0.99       0.01     versicolor      versicolor\n",
       "63     0.00        0.96       0.04     versicolor      versicolor\n",
       "64     0.00        1.00       0.00     versicolor      versicolor\n",
       "68     0.00        0.95       0.04     versicolor      versicolor\n",
       "81     0.00        1.00       0.00     versicolor      versicolor\n",
       "84     0.00        0.95       0.04     versicolor      versicolor\n",
       "105    0.00        0.00       1.00      virginica       virginica\n",
       "109    0.00        0.00       1.00      virginica       virginica\n",
       "111    0.00        0.01       0.98      virginica       virginica\n",
       "112    0.00        0.00       1.00      virginica       virginica\n",
       "126    0.00        0.25       0.75      virginica       virginica\n",
       "130    0.00        0.00       1.00      virginica       virginica\n",
       "136    0.00        0.00       1.00      virginica       virginica\n",
       "137    0.00        0.02       0.98      virginica       virginica\n",
       "139    0.00        0.00       1.00      virginica       virginica\n",
       "146    0.00        0.05       0.95      virginica       virginica\n",
       "149    0.00        0.09       0.91      virginica       virginica"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'erreurs : 0\n"
     ]
    }
   ],
   "source": [
    "usp = dfi['species'].unique()\n",
    "print(f\"{color.BOLD}{color.GREEN}Catégories uniques d'iris :{color.OFF} {usp}\")\n",
    "# cette correspondance élément 0 <-> setosa ; élément 1 <-> versicolor ; élément 2 <-> virginica\n",
    "# va servir à transformer les probabilités les plus élevées en espèce d'iris\n",
    "\n",
    "y_train_hat=ANNmodel.predict(x_train)\n",
    "ytr_hD = pd.DataFrame(y_train_hat, columns=usp, index=y_train.index)\n",
    "# argmax renvoie l'indice de la valeur maximale selon un axe (0 = ligne / 1 = colonne)\n",
    "# ça va ici nous renvoyer pour chaque ligne le numéro de la colonne qui contient la plus grande valeur de probabilité\n",
    "# on convertit maintenant 0, 1 & 2 en espèce d'iris\n",
    "iris_tr_hat = usp[np.argmax(y_train_hat,axis=1)]\n",
    "# on ajoute cette colonne au dataframe ytr_hD\n",
    "ytr_hD['Espèce prédite'] = pd.DataFrame(iris_tr_hat, index=y_train.index)\n",
    "ytr_hD['Espèce observée'] = pd.DataFrame(y_train_species, index=y_train.index)\n",
    "print(f\"{color.BOLD}{color.BLUE}Test Set{color.OFF}. Comparaison entre espèce prédite et espèce observée\")\n",
    "pd.set_option('display.max_rows', None) #on veut afficher toutes les lignes des DataFrames\n",
    "pd.options.display.float_format = '{:,.2f}'.format #on formatte les probabilités, sinon c'est illisible\n",
    "display(ytr_hD)\n",
    "pd.set_option('display.max_rows', 10) #reset de l'affichage: 5 premières et 5 dernières lignes\n",
    "pd.options.display.float_format = None #reset de l'affichage: format standard \n",
    "diff_Pred_Obs=np.where(ytr_hD['Espèce prédite'] == ytr_hD['Espèce observée'], 0, 1)\n",
    "print(f\"Nombre d'erreurs : {np.sum(diff_Pred_Obs)}\")\n",
    "\n",
    "print()\n",
    "y_test_hat=ANNmodel.predict(x_test)\n",
    "ytt_hD = pd.DataFrame(y_test_hat, columns=usp, index=y_test.index)\n",
    "# argmax renvoie l'indice de la valeur maximale selon un axe (0 = ligne / 1 = colonne)\n",
    "# ça va ici nous renvoyer pour chaque ligne le numéro de la colonne qui contient la plus grande valeur de probabilité\n",
    "# on convertit maintenant 0, 1 & 2 en espèce d'iris\n",
    "iris_tt_hat = usp[np.argmax(y_test_hat,axis=1)]\n",
    "# on ajoute cette colonne au dataframe ytt_hD\n",
    "ytt_hD['Espèce prédite'] = pd.DataFrame(iris_tt_hat, index=y_test.index)\n",
    "ytt_hD['Espèce observée'] = pd.DataFrame(y_test_species, index=y_test.index)\n",
    "print(f\"{color.BOLD}{color.RED}Test Set{color.OFF}. Comparaison entre espèce prédite et espèce observée.\")\n",
    "pd.set_option('display.max_rows', None) #on veut afficher toutes les lignes des DataFrames\n",
    "pd.options.display.float_format = '{:,.2f}'.format #on formatte les probabilités, sinon c'est illisible\n",
    "display(ytt_hD)\n",
    "pd.set_option('display.max_rows', 10) #reset de l'affichage: 5 premières et 5 dernières lignes\n",
    "pd.options.display.float_format = None #reset de l'affichage: \n",
    "diff_Pred_Obs=np.where(ytt_hD['Espèce prédite'] == ytt_hD['Espèce observée'], 0, 1)\n",
    "print(f\"Nombre d'erreurs : {np.sum(diff_Pred_Obs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28139876-6004-473c-8002-081abbb8331c",
   "metadata": {},
   "source": [
    "<div class=\"warn\">Ce serait assez fastidieux à analyser si la base de données était constituée de milliers d'échantillons. La libraririe <code>scikit learn</code> permet de visualiser de façon graphique les succcès et erreurs d'un algorithme. Il s'agit de <b style=\"color:red\">matrices de confusion</b>. C'est l'objet du paragraphe suivant.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55fe30e-ce3d-49bf-b488-6fe9bff3291b",
   "metadata": {},
   "source": [
    "#### 3.4.d. Bilan de la performance du modèle prédictif sous forme de matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "866bd474-9393-4311-b165-0f66d021e221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set. Matrice de confusion\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set. Matrice de confusion\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_labels = dfi['species'].unique()\n",
    "print(\"Training set. Matrice de confusion\")\n",
    "cm_tr = confusion_matrix(np.argmax(y_train.to_numpy(),axis=1), np.argmax(y_train_hat,axis=1))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_tr, display_labels=cm_labels)\n",
    "disp.plot(cmap=plt.cm.Blues) #ici cm = diminutif de colormap dans matplotlib\n",
    "plt.show()\n",
    "print(\"Test set. Matrice de confusion\")\n",
    "cm_tt = confusion_matrix(np.argmax(y_test.to_numpy(),axis=1), np.argmax(y_test_hat,axis=1))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_tt, display_labels=cm_labels)\n",
    "disp.plot(cmap=plt.cm.Blues) #ici cm = diminutif de colormap dans matplotlib\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7221903b-b5ee-47f7-b3ef-8f1da76c68ab",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "On rappelle à quoi ressemble la corrélation entre longueurs et largeurs de pétales, et l'apparente classification par espèce qui nous a amenés à bâtir une IA d'identification à partir de ces caractéristiques de dimension de pétales:\n",
    "    <p style=\"text-align: center\"><img width=\"500px\" src=\"./svg/jointplot_petals.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_jpc\"/></p>\n",
    "<li> il n'est pas surprenant que l'algorithme ne se trompe jamais dans l'identification des iris <i>setosa</i>, qui sont nettement plus petits que les autres\n",
    "<li> les seules erreurs sont une confusion entre <i>versicolor</i> et <i>virginica</i>\n",
    "<li> si on prend le temps de remonter aux échantillons concernés, on s'aperçoit que les erreurs sont le plus ouvent commises pour des espèces dont les dimension de pétales sont environ 5 x 1.9 cm, c'est-à-dire dans la zone de recouvrement entre les deux espèces dans la figure ci-dessus. <b> Il n'y a donc pas de miracle, sur la base des données qu'on lui a fournies, l'IA ne peut pas faire mieux que nous.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f1edc9-8140-4039-9755-35e9a09d277d",
   "metadata": {},
   "source": [
    "### Bilan de la troisième partie\n",
    "<div class='rq'>\n",
    "\n",
    "La figure ci-dessous résume comment est en général développé et optimisé un algorithme d'apprentissage automatique. C'est ce \"workflow\" que nous venons d'appliquer.  \n",
    "<p style=\"text-align: center\"><img width=\"750px\" src=\"./svg/BilanML-Classification.svg\" style=\"margin-left:auto; margin-right:auto\" id=\"img_jpc\"/></p>\n",
    " \n",
    "On a développé ici un algorithme de reconnaissance basé sur deux caractéristiques, les longueur et largeur des pétales d'iris. C'est un assez gros effort algorithmique alors qu'il suffit finalement d'exploiter le graphique reliant longueur et largeur, avec le code couleur qui permet de classer les espèces en fonction de cela. En résumé, les petits iris sont des setosa, les grands sont des virginica et les moyens sont des versicolor. <b>Dit comme ça, ça semble facile, mais ce sont des compétences (mesure, tracé de graphique, exploitation d'un graphique) qu'on a nous-mêmes dévelopé quelque part entre fin de l'école élémentaire et le collège</b>. <span style=\"color:red\">On a donc pour le moment une IA niveau \"*entrée en sixième*\".</span> \n",
    "\n",
    "L'exercice qui est proposé en complément vise à développer un algorithme capable d'établir une corrélation plus complexe, en prenant simultanément en compte les longueurs et largeurs des pétales et des sépales. Et ça permet effectivement de diminuer l'erreur commise. On conviendra que ça devient plus compliqué pour nos cerveaux de rechercher si les corrélations croisées, reportées dans la figure ci-dessous, permettent une meilleure classification. C'est pour ce type de tâche, au-delà de corrélations simples, que l'utilisation de ces algorithmes prend tout son sens. \n",
    "<p style=\"text-align: center\"><img width=\"850px\" src=\"./svg/pairplot_Iris.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_jpc\"/></p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af354bcb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Fin à:** Wednesday 22 June 2022, 11:08:08  \n",
       "**Durée:** 00:22:23 935ms"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"./svg/logoFin.svg\" style=\"margin-left:auto; margin-right:auto\"/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vID.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fad06d-6413-4793-b945-a7d707839fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
