{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de690ce-49d6-4387-b242-fe36280ca9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: bold;\n",
       "}\n",
       "body {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: 200;\n",
       "}\n",
       "h1 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 30px ;\n",
       "  color: white;\n",
       "  background: #b11d01;\n",
       "  text-align: center;\n",
       "}\n",
       "h2 {\n",
       "  border: 3px solid #333;\n",
       "  padding: 18px ;\n",
       "  color: #b11d01;\n",
       "  background: #ffffff;\n",
       "  text-align: center;\n",
       "}\n",
       "h3 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 12px ;\n",
       "  color: #000000;\n",
       "  background: #c1c1c1;\n",
       "  text-align: left;\n",
       "}\n",
       "h4 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #d9fffc;\n",
       "  text-align: left;\n",
       "}\n",
       "h5 {\n",
       "  border: 1px solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #ffffff;\n",
       "  text-align: left;\n",
       "}\n",
       ".warn {    \n",
       "    background-color: #fcf2f2;\n",
       "    border-color: #dFb5b4;\n",
       "    border-left: 5px solid #dfb5b4;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       ".rq {    \n",
       "    background-color: #e2e2e2;\n",
       "    border-color: #969696;\n",
       "    border-left: 5px solid #969696;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Début à:** Sunday 30 October 2022, 18:49:38  \n",
       "**Hostname:** localhost.localdomain (Linux)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"./config/svg/logoDebut.svg\" style=\"margin-left:auto; margin-right:auto\"/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "cwd0 = './config/'\n",
    "sys.path.append(cwd0)\n",
    "\n",
    "import visualID as vID\n",
    "from visualID import color\n",
    "vID.init(cwd0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2f60c2-8759-4019-8dea-b692e73eaee6",
   "metadata": {},
   "source": [
    "### Apprentissage supervisé (*supervised Machine Learning*) appliqué à la classification. Suggestion de petite(s) application(s) en autonomie. Corrigé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02aa070-ddf4-4bb4-bb26-5043ee7ca53f",
   "metadata": {},
   "source": [
    "#### Importation des librairies utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6568ba3-8b12-4725-a890-132a9de260b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7334ed83-f6aa-4f7e-b6a1-7e49b8b9da66",
   "metadata": {},
   "source": [
    "#### a. Lecture de la base de données qui ont été adaptées au problème"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef2fe4da-b86e-4c95-8f97-84f8c0f56215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dfi. Structure (shape) :(150, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species  setosa  \\\n",
       "0             5.1          3.5           1.4          0.2     setosa     1.0   \n",
       "1             4.9          3.0           1.4          0.2     setosa     1.0   \n",
       "2             4.7          3.2           1.3          0.2     setosa     1.0   \n",
       "3             4.6          3.1           1.5          0.2     setosa     1.0   \n",
       "4             5.0          3.6           1.4          0.2     setosa     1.0   \n",
       "..            ...          ...           ...          ...        ...     ...   \n",
       "145           6.7          3.0           5.2          2.3  virginica     0.0   \n",
       "146           6.3          2.5           5.0          1.9  virginica     0.0   \n",
       "147           6.5          3.0           5.2          2.0  virginica     0.0   \n",
       "148           6.2          3.4           5.4          2.3  virginica     0.0   \n",
       "149           5.9          3.0           5.1          1.8  virginica     0.0   \n",
       "\n",
       "     versicolor  virginica  \n",
       "0           0.0        0.0  \n",
       "1           0.0        0.0  \n",
       "2           0.0        0.0  \n",
       "3           0.0        0.0  \n",
       "4           0.0        0.0  \n",
       "..          ...        ...  \n",
       "145         0.0        1.0  \n",
       "146         0.0        1.0  \n",
       "147         0.0        1.0  \n",
       "148         0.0        1.0  \n",
       "149         0.0        1.0  \n",
       "\n",
       "[150 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfi=pd.read_csv('./iris-data/iris_ohe.csv', sep=\"\\t\") #les colonnes sont séparées par des tabulations\n",
    "print(f\"Dfi. Structure (shape) :{dfi.shape}\")\n",
    "display(dfi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139cdc56-49f8-4775-abb6-1e9b2ff1b292",
   "metadata": {},
   "source": [
    "#### b. Séparation des données en deux sous-ensembles d'apprentissage et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0470f71-6f34-4ccf-9e56-836928e32004",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train :  (120, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>7.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width\n",
       "68            6.2          2.2           4.5          1.5\n",
       "113           5.7          2.5           5.0          2.0\n",
       "38            4.4          3.0           1.3          0.2\n",
       "141           6.9          3.1           5.1          2.3\n",
       "102           7.1          3.0           5.9          2.1\n",
       "..            ...          ...           ...          ...\n",
       "63            6.1          2.9           4.7          1.4\n",
       "110           6.5          3.2           5.1          2.0\n",
       "40            5.0          3.5           1.3          0.3\n",
       "78            6.0          2.9           4.5          1.5\n",
       "98            5.1          2.5           3.0          1.1\n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train :  (120, 3) y_train_species :  (120, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     setosa  versicolor  virginica\n",
       "68      0.0         1.0        0.0\n",
       "113     0.0         0.0        1.0\n",
       "38      1.0         0.0        0.0\n",
       "141     0.0         0.0        1.0\n",
       "102     0.0         0.0        1.0\n",
       "..      ...         ...        ...\n",
       "63      0.0         1.0        0.0\n",
       "110     0.0         0.0        1.0\n",
       "40      1.0         0.0        0.0\n",
       "78      0.0         1.0        0.0\n",
       "98      0.0         1.0        0.0\n",
       "\n",
       "[120 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        species\n",
       "68   versicolor\n",
       "113   virginica\n",
       "38       setosa\n",
       "141   virginica\n",
       "102   virginica\n",
       "..          ...\n",
       "63   versicolor\n",
       "110   virginica\n",
       "40       setosa\n",
       "78   versicolor\n",
       "98   versicolor\n",
       "\n",
       "[120 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train = dfi.sample(frac=0.8, axis='index') # on sélectionne au hasard 80% de l'échantillon\n",
    "data_test  = dfi.drop(data_train.index) # on sélectionne le reste\n",
    "\n",
    "x_train = data_train[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "y_train = data_train[['setosa','versicolor','virginica']]\n",
    "y_train_species = data_train[['species']] #sera utile à la fin pour comparer la prédiction et l'espèce réelle\n",
    "\n",
    "x_test  = data_test[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "y_test  = data_test[['setosa','versicolor','virginica']]\n",
    "y_test_species = data_test[['species']] #sera utile à la fin pour comparer la prédiction et l'espèce réelle\n",
    "\n",
    "print('x_train : ',x_train.shape)\n",
    "display(x_train)\n",
    "print('y_train : ',y_train.shape,'y_train_species : ',y_train_species.shape)\n",
    "display(y_train, y_train_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b4f9b-46d6-4988-a3f3-ab75734b6ffa",
   "metadata": {},
   "source": [
    "#### c. Adaptation des données à la régression logistique par le réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70779f6b-4820-427f-a2ce-96ac775809a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_eaf52\">\n",
       "  <caption>Training set après normalisation (avec scikit-learn):</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_eaf52_level0_col0\" class=\"col_heading level0 col0\" >sepal_length</th>\n",
       "      <th id=\"T_eaf52_level0_col1\" class=\"col_heading level0 col1\" >sepal_width</th>\n",
       "      <th id=\"T_eaf52_level0_col2\" class=\"col_heading level0 col2\" >petal_length</th>\n",
       "      <th id=\"T_eaf52_level0_col3\" class=\"col_heading level0 col3\" >petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_eaf52_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_eaf52_row0_col0\" class=\"data row0 col0\" >120.00</td>\n",
       "      <td id=\"T_eaf52_row0_col1\" class=\"data row0 col1\" >120.00</td>\n",
       "      <td id=\"T_eaf52_row0_col2\" class=\"data row0 col2\" >120.00</td>\n",
       "      <td id=\"T_eaf52_row0_col3\" class=\"data row0 col3\" >120.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eaf52_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_eaf52_row1_col0\" class=\"data row1 col0\" >0.00</td>\n",
       "      <td id=\"T_eaf52_row1_col1\" class=\"data row1 col1\" >-0.00</td>\n",
       "      <td id=\"T_eaf52_row1_col2\" class=\"data row1 col2\" >0.00</td>\n",
       "      <td id=\"T_eaf52_row1_col3\" class=\"data row1 col3\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eaf52_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_eaf52_row2_col0\" class=\"data row2 col0\" >1.00</td>\n",
       "      <td id=\"T_eaf52_row2_col1\" class=\"data row2 col1\" >1.00</td>\n",
       "      <td id=\"T_eaf52_row2_col2\" class=\"data row2 col2\" >1.00</td>\n",
       "      <td id=\"T_eaf52_row2_col3\" class=\"data row2 col3\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eaf52_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_eaf52_row3_col0\" class=\"data row3 col0\" >-1.83</td>\n",
       "      <td id=\"T_eaf52_row3_col1\" class=\"data row3 col1\" >-2.43</td>\n",
       "      <td id=\"T_eaf52_row3_col2\" class=\"data row3 col2\" >-1.56</td>\n",
       "      <td id=\"T_eaf52_row3_col3\" class=\"data row3 col3\" >-1.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eaf52_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_eaf52_row4_col0\" class=\"data row4 col0\" >-0.87</td>\n",
       "      <td id=\"T_eaf52_row4_col1\" class=\"data row4 col1\" >-0.52</td>\n",
       "      <td id=\"T_eaf52_row4_col2\" class=\"data row4 col2\" >-1.23</td>\n",
       "      <td id=\"T_eaf52_row4_col3\" class=\"data row4 col3\" >-1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eaf52_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_eaf52_row5_col0\" class=\"data row5 col0\" >-0.04</td>\n",
       "      <td id=\"T_eaf52_row5_col1\" class=\"data row5 col1\" >-0.05</td>\n",
       "      <td id=\"T_eaf52_row5_col2\" class=\"data row5 col2\" >0.35</td>\n",
       "      <td id=\"T_eaf52_row5_col3\" class=\"data row5 col3\" >0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eaf52_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_eaf52_row6_col0\" class=\"data row6 col0\" >0.67</td>\n",
       "      <td id=\"T_eaf52_row6_col1\" class=\"data row6 col1\" >0.49</td>\n",
       "      <td id=\"T_eaf52_row6_col2\" class=\"data row6 col2\" >0.74</td>\n",
       "      <td id=\"T_eaf52_row6_col3\" class=\"data row6 col3\" >0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eaf52_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_eaf52_row7_col0\" class=\"data row7 col0\" >2.46</td>\n",
       "      <td id=\"T_eaf52_row7_col1\" class=\"data row7 col1\" >2.81</td>\n",
       "      <td id=\"T_eaf52_row7_col2\" class=\"data row7 col2\" >1.75</td>\n",
       "      <td id=\"T_eaf52_row7_col3\" class=\"data row7 col3\" >1.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f58e9f40f40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c3818\">\n",
       "  <caption>Test set after après normalisation (avec scikit-learn):</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c3818_level0_col0\" class=\"col_heading level0 col0\" >sepal_length</th>\n",
       "      <th id=\"T_c3818_level0_col1\" class=\"col_heading level0 col1\" >sepal_width</th>\n",
       "      <th id=\"T_c3818_level0_col2\" class=\"col_heading level0 col2\" >petal_length</th>\n",
       "      <th id=\"T_c3818_level0_col3\" class=\"col_heading level0 col3\" >petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c3818_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_c3818_row0_col0\" class=\"data row0 col0\" >30.00</td>\n",
       "      <td id=\"T_c3818_row0_col1\" class=\"data row0 col1\" >30.00</td>\n",
       "      <td id=\"T_c3818_row0_col2\" class=\"data row0 col2\" >30.00</td>\n",
       "      <td id=\"T_c3818_row0_col3\" class=\"data row0 col3\" >30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c3818_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_c3818_row1_col0\" class=\"data row1 col0\" >0.05</td>\n",
       "      <td id=\"T_c3818_row1_col1\" class=\"data row1 col1\" >0.40</td>\n",
       "      <td id=\"T_c3818_row1_col2\" class=\"data row1 col2\" >-0.05</td>\n",
       "      <td id=\"T_c3818_row1_col3\" class=\"data row1 col3\" >-0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c3818_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_c3818_row2_col0\" class=\"data row2 col0\" >0.92</td>\n",
       "      <td id=\"T_c3818_row2_col1\" class=\"data row2 col1\" >1.10</td>\n",
       "      <td id=\"T_c3818_row2_col2\" class=\"data row2 col2\" >0.94</td>\n",
       "      <td id=\"T_c3818_row2_col3\" class=\"data row2 col3\" >0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c3818_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_c3818_row3_col0\" class=\"data row3 col0\" >-1.47</td>\n",
       "      <td id=\"T_c3818_row3_col1\" class=\"data row3 col1\" >-1.48</td>\n",
       "      <td id=\"T_c3818_row3_col2\" class=\"data row3 col2\" >-1.33</td>\n",
       "      <td id=\"T_c3818_row3_col3\" class=\"data row3 col3\" >-1.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c3818_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_c3818_row4_col0\" class=\"data row4 col0\" >-0.52</td>\n",
       "      <td id=\"T_c3818_row4_col1\" class=\"data row4 col1\" >-0.29</td>\n",
       "      <td id=\"T_c3818_row4_col2\" class=\"data row4 col2\" >-1.21</td>\n",
       "      <td id=\"T_c3818_row4_col3\" class=\"data row4 col3\" >-1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c3818_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_c3818_row5_col0\" class=\"data row5 col0\" >-0.10</td>\n",
       "      <td id=\"T_c3818_row5_col1\" class=\"data row5 col1\" >-0.05</td>\n",
       "      <td id=\"T_c3818_row5_col2\" class=\"data row5 col2\" >0.21</td>\n",
       "      <td id=\"T_c3818_row5_col3\" class=\"data row5 col3\" >0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c3818_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_c3818_row6_col0\" class=\"data row6 col0\" >0.67</td>\n",
       "      <td id=\"T_c3818_row6_col1\" class=\"data row6 col1\" >0.91</td>\n",
       "      <td id=\"T_c3818_row6_col2\" class=\"data row6 col2\" >0.60</td>\n",
       "      <td id=\"T_c3818_row6_col3\" class=\"data row6 col3\" >0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c3818_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_c3818_row7_col0\" class=\"data row7 col0\" >2.22</td>\n",
       "      <td id=\"T_c3818_row7_col1\" class=\"data row7 col1\" >3.29</td>\n",
       "      <td id=\"T_c3818_row7_col2\" class=\"data row7 col2\" >1.64</td>\n",
       "      <td id=\"T_c3818_row7_col3\" class=\"data row7 col3\" >1.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f58ea185190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(x_train.values)\n",
    "x_trainS = scaler.transform(x_train.values) #returns a numpy array\n",
    "x_testS = scaler.transform(x_test.values) #returns a numpy array\n",
    "x_trainD = pd.DataFrame(x_trainS, columns=x_train.columns, index=x_train.index)\n",
    "x_testD = pd.DataFrame(x_testS, columns=x_test.columns, index=x_test.index)\n",
    "display(x_trainD.describe().style.format(\"{0:.2f}\").set_caption(\"Training set après normalisation (avec scikit-learn):\"))\n",
    "display(x_testD.describe().style.format(\"{0:.2f}\").set_caption(\"Test set after après normalisation (avec scikit-learn):\"))\n",
    "x_train = x_trainS\n",
    "x_test = x_testS\n",
    "del x_trainD, x_testD, x_trainS, x_testS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7111b5f-5db1-457a-a4ed-a93902334ebe",
   "metadata": {},
   "source": [
    "#### d. Définition du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "776ea4a4-a5f4-4898-886d-cec103569b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(NE): #NE = nombre de neurones d'entrée\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(NE, name='iLayer'))\n",
    "    model.add(keras.layers.Dense(7, activation='relu', name='hLayer1'))\n",
    "    model.add(keras.layers.Dense(5, activation='relu', name='hLayer2'))\n",
    "    model.add(keras.layers.Dense(3, activation='softmax', name='oLayer'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'categorical_crossentropy',\n",
    "                  metrics   = ['accuracy'] )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32938fc-ef05-46d6-9f26-80a727d0bc8f",
   "metadata": {},
   "source": [
    "#### e. Apprentissage supervisé du réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "877b00e0-83b3-4540-ad09-ffa631fde346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train. Structure (shape) : (120, 4)\n",
      "x_test. Structure (shape) : (30, 4)\n",
      "y_train. Structure (shape) : (120, 3)\n",
      "y_test. Structure (shape) : (30, 3)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hLayer1 (Dense)              (None, 7)                 35        \n",
      "_________________________________________________________________\n",
      "hLayer2 (Dense)              (None, 5)                 40        \n",
      "_________________________________________________________________\n",
      "oLayer (Dense)               (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 93\n",
      "Trainable params: 93\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 09:40:11.692156: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-06-24 09:40:11.692498: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-24 09:40:11.693137: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\n",
      "2022-06-24 09:40:11.809219: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-06-24 09:40:11.809653: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2099940000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 33ms/step - loss: 1.1498 - accuracy: 0.3103 - val_loss: 1.0356 - val_accuracy: 0.3333\n",
      "Epoch 2/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1.1020 - accuracy: 0.3068 - val_loss: 0.9679 - val_accuracy: 0.3667\n",
      "Epoch 3/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1.0792 - accuracy: 0.3341 - val_loss: 0.9117 - val_accuracy: 0.4333\n",
      "Epoch 4/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.8258 - accuracy: 0.4857 - val_loss: 0.8690 - val_accuracy: 0.4667\n",
      "Epoch 5/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.9863 - accuracy: 0.4500 - val_loss: 0.8305 - val_accuracy: 0.4667\n",
      "Epoch 6/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.8460 - accuracy: 0.5302 - val_loss: 0.8012 - val_accuracy: 0.5333\n",
      "Epoch 7/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.8316 - accuracy: 0.5705 - val_loss: 0.7757 - val_accuracy: 0.5333\n",
      "Epoch 8/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.7599 - accuracy: 0.5969 - val_loss: 0.7552 - val_accuracy: 0.5667\n",
      "Epoch 9/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.7942 - accuracy: 0.5209 - val_loss: 0.7369 - val_accuracy: 0.5667\n",
      "Epoch 10/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.8714 - accuracy: 0.4891 - val_loss: 0.7206 - val_accuracy: 0.6000\n",
      "Epoch 11/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.7534 - accuracy: 0.5328 - val_loss: 0.7078 - val_accuracy: 0.6333\n",
      "Epoch 12/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.6850 - accuracy: 0.6843 - val_loss: 0.6964 - val_accuracy: 0.6667\n",
      "Epoch 13/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.7124 - accuracy: 0.6200 - val_loss: 0.6857 - val_accuracy: 0.6667\n",
      "Epoch 14/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.7049 - accuracy: 0.6840 - val_loss: 0.6759 - val_accuracy: 0.7000\n",
      "Epoch 15/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.6583 - accuracy: 0.7515 - val_loss: 0.6669 - val_accuracy: 0.7000\n",
      "Epoch 16/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.7154 - accuracy: 0.6505 - val_loss: 0.6593 - val_accuracy: 0.6667\n",
      "Epoch 17/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.6277 - accuracy: 0.7704 - val_loss: 0.6507 - val_accuracy: 0.7000\n",
      "Epoch 18/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.6261 - accuracy: 0.8176 - val_loss: 0.6434 - val_accuracy: 0.7000\n",
      "Epoch 19/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.6189 - accuracy: 0.7368 - val_loss: 0.6350 - val_accuracy: 0.7333\n",
      "Epoch 20/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5825 - accuracy: 0.8392 - val_loss: 0.6273 - val_accuracy: 0.7333\n",
      "Epoch 21/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.6025 - accuracy: 0.7828 - val_loss: 0.6196 - val_accuracy: 0.6667\n",
      "Epoch 22/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.6010 - accuracy: 0.8351 - val_loss: 0.6111 - val_accuracy: 0.7000\n",
      "Epoch 23/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.6195 - accuracy: 0.8481 - val_loss: 0.6021 - val_accuracy: 0.7333\n",
      "Epoch 24/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5656 - accuracy: 0.8158 - val_loss: 0.5937 - val_accuracy: 0.7333\n",
      "Epoch 25/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.5337 - accuracy: 0.8908 - val_loss: 0.5848 - val_accuracy: 0.7667\n",
      "Epoch 26/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5362 - accuracy: 0.8675 - val_loss: 0.5763 - val_accuracy: 0.7667\n",
      "Epoch 27/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5350 - accuracy: 0.8557 - val_loss: 0.5673 - val_accuracy: 0.8000\n",
      "Epoch 28/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5055 - accuracy: 0.8807 - val_loss: 0.5596 - val_accuracy: 0.8000\n",
      "Epoch 29/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.5317 - accuracy: 0.8605 - val_loss: 0.5503 - val_accuracy: 0.8000\n",
      "Epoch 30/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.5660 - accuracy: 0.8434 - val_loss: 0.5424 - val_accuracy: 0.8000\n",
      "Epoch 31/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5487 - accuracy: 0.9164 - val_loss: 0.5331 - val_accuracy: 0.8000\n",
      "Epoch 32/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4853 - accuracy: 0.8847 - val_loss: 0.5242 - val_accuracy: 0.8000\n",
      "Epoch 33/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5124 - accuracy: 0.8260 - val_loss: 0.5187 - val_accuracy: 0.8000\n",
      "Epoch 34/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4588 - accuracy: 0.8964 - val_loss: 0.5102 - val_accuracy: 0.8000\n",
      "Epoch 35/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5136 - accuracy: 0.9102 - val_loss: 0.5041 - val_accuracy: 0.8000\n",
      "Epoch 36/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4623 - accuracy: 0.9039 - val_loss: 0.4968 - val_accuracy: 0.8000\n",
      "Epoch 37/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4850 - accuracy: 0.8796 - val_loss: 0.4894 - val_accuracy: 0.8333\n",
      "Epoch 38/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4180 - accuracy: 0.9179 - val_loss: 0.4821 - val_accuracy: 0.8333\n",
      "Epoch 39/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4176 - accuracy: 0.9288 - val_loss: 0.4758 - val_accuracy: 0.8333\n",
      "Epoch 40/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4232 - accuracy: 0.9219 - val_loss: 0.4696 - val_accuracy: 0.8333\n",
      "Epoch 41/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4716 - accuracy: 0.8704 - val_loss: 0.4608 - val_accuracy: 0.8333\n",
      "Epoch 42/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4753 - accuracy: 0.8790 - val_loss: 0.4546 - val_accuracy: 0.8333\n",
      "Epoch 43/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4503 - accuracy: 0.8770 - val_loss: 0.4470 - val_accuracy: 0.8333\n",
      "Epoch 44/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.3886 - accuracy: 0.9245 - val_loss: 0.4397 - val_accuracy: 0.8333\n",
      "Epoch 45/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4061 - accuracy: 0.9482 - val_loss: 0.4320 - val_accuracy: 0.8667\n",
      "Epoch 46/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.3463 - accuracy: 0.9027 - val_loss: 0.4242 - val_accuracy: 0.8667\n",
      "Epoch 47/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.4750 - accuracy: 0.8822 - val_loss: 0.4201 - val_accuracy: 0.8667\n",
      "Epoch 48/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4066 - accuracy: 0.9015 - val_loss: 0.4134 - val_accuracy: 0.8667\n",
      "Epoch 49/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.4295 - accuracy: 0.8671 - val_loss: 0.4043 - val_accuracy: 0.9000\n",
      "Epoch 50/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4173 - accuracy: 0.8635 - val_loss: 0.4007 - val_accuracy: 0.9000\n",
      "Epoch 51/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.4288 - accuracy: 0.8877 - val_loss: 0.3944 - val_accuracy: 0.9333\n",
      "Epoch 52/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.4171 - accuracy: 0.8901 - val_loss: 0.3881 - val_accuracy: 0.9333\n",
      "Epoch 53/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.4102 - accuracy: 0.8660 - val_loss: 0.3796 - val_accuracy: 0.9333\n",
      "Epoch 54/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3767 - accuracy: 0.8973 - val_loss: 0.3762 - val_accuracy: 0.9333\n",
      "Epoch 55/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3353 - accuracy: 0.8976 - val_loss: 0.3699 - val_accuracy: 0.9333\n",
      "Epoch 56/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2872 - accuracy: 0.9502 - val_loss: 0.3647 - val_accuracy: 0.9333\n",
      "Epoch 57/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3376 - accuracy: 0.9450 - val_loss: 0.3565 - val_accuracy: 0.9333\n",
      "Epoch 58/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3395 - accuracy: 0.9534 - val_loss: 0.3499 - val_accuracy: 0.9333\n",
      "Epoch 59/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3163 - accuracy: 0.9579 - val_loss: 0.3439 - val_accuracy: 0.9333\n",
      "Epoch 60/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3384 - accuracy: 0.9170 - val_loss: 0.3367 - val_accuracy: 0.9333\n",
      "Epoch 61/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3399 - accuracy: 0.9034 - val_loss: 0.3312 - val_accuracy: 0.9333\n",
      "Epoch 62/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3154 - accuracy: 0.9538 - val_loss: 0.3243 - val_accuracy: 0.9333\n",
      "Epoch 63/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3202 - accuracy: 0.9328 - val_loss: 0.3191 - val_accuracy: 0.9333\n",
      "Epoch 64/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3410 - accuracy: 0.8769 - val_loss: 0.3132 - val_accuracy: 0.9333\n",
      "Epoch 65/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2717 - accuracy: 0.9378 - val_loss: 0.3090 - val_accuracy: 0.9333\n",
      "Epoch 66/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3582 - accuracy: 0.9145 - val_loss: 0.3031 - val_accuracy: 0.9333\n",
      "Epoch 67/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2897 - accuracy: 0.9542 - val_loss: 0.2968 - val_accuracy: 0.9333\n",
      "Epoch 68/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3169 - accuracy: 0.9332 - val_loss: 0.2909 - val_accuracy: 0.9333\n",
      "Epoch 69/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2874 - accuracy: 0.9459 - val_loss: 0.2851 - val_accuracy: 0.9333\n",
      "Epoch 70/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.3108 - accuracy: 0.9292 - val_loss: 0.2791 - val_accuracy: 0.9333\n",
      "Epoch 71/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3249 - accuracy: 0.9451 - val_loss: 0.2744 - val_accuracy: 0.9333\n",
      "Epoch 72/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.3002 - accuracy: 0.9210 - val_loss: 0.2685 - val_accuracy: 0.9333\n",
      "Epoch 73/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2837 - accuracy: 0.9292 - val_loss: 0.2631 - val_accuracy: 0.9333\n",
      "Epoch 74/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2441 - accuracy: 0.9766 - val_loss: 0.2575 - val_accuracy: 0.9333\n",
      "Epoch 75/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2781 - accuracy: 0.9658 - val_loss: 0.2533 - val_accuracy: 0.9333\n",
      "Epoch 76/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2506 - accuracy: 0.9459 - val_loss: 0.2484 - val_accuracy: 0.9333\n",
      "Epoch 77/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2419 - accuracy: 0.9694 - val_loss: 0.2422 - val_accuracy: 0.9333\n",
      "Epoch 78/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.2272 - accuracy: 0.9732 - val_loss: 0.2378 - val_accuracy: 0.9333\n",
      "Epoch 79/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.2386 - accuracy: 0.9657 - val_loss: 0.2333 - val_accuracy: 0.9333\n",
      "Epoch 80/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2770 - accuracy: 0.9192 - val_loss: 0.2283 - val_accuracy: 0.9333\n",
      "Epoch 81/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.2463 - accuracy: 0.9275 - val_loss: 0.2235 - val_accuracy: 0.9333\n",
      "Epoch 82/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2043 - accuracy: 0.9574 - val_loss: 0.2188 - val_accuracy: 0.9333\n",
      "Epoch 83/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2532 - accuracy: 0.9435 - val_loss: 0.2120 - val_accuracy: 0.9333\n",
      "Epoch 84/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1968 - accuracy: 0.9589 - val_loss: 0.2080 - val_accuracy: 0.9333\n",
      "Epoch 85/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2211 - accuracy: 0.9552 - val_loss: 0.2032 - val_accuracy: 0.9667\n",
      "Epoch 86/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1914 - accuracy: 0.9705 - val_loss: 0.1977 - val_accuracy: 0.9667\n",
      "Epoch 87/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.2194 - accuracy: 0.9688 - val_loss: 0.1940 - val_accuracy: 0.9333\n",
      "Epoch 88/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.2471 - accuracy: 0.9491 - val_loss: 0.1890 - val_accuracy: 0.9667\n",
      "Epoch 89/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.2003 - accuracy: 0.9764 - val_loss: 0.1844 - val_accuracy: 0.9667\n",
      "Epoch 90/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1869 - accuracy: 0.9904 - val_loss: 0.1823 - val_accuracy: 0.9333\n",
      "Epoch 91/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1697 - accuracy: 0.9668 - val_loss: 0.1776 - val_accuracy: 0.9333\n",
      "Epoch 92/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1759 - accuracy: 0.9893 - val_loss: 0.1746 - val_accuracy: 0.9333\n",
      "Epoch 93/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.1914 - accuracy: 0.9633 - val_loss: 0.1721 - val_accuracy: 0.9333\n",
      "Epoch 94/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1638 - accuracy: 0.9833 - val_loss: 0.1652 - val_accuracy: 0.9667\n",
      "Epoch 95/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1401 - accuracy: 0.9840 - val_loss: 0.1644 - val_accuracy: 0.9667\n",
      "Epoch 96/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1626 - accuracy: 0.9781 - val_loss: 0.1624 - val_accuracy: 0.9667\n",
      "Epoch 97/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1877 - accuracy: 0.9581 - val_loss: 0.1579 - val_accuracy: 0.9667\n",
      "Epoch 98/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1814 - accuracy: 0.9612 - val_loss: 0.1577 - val_accuracy: 0.9667\n",
      "Epoch 99/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1883 - accuracy: 0.9452 - val_loss: 0.1561 - val_accuracy: 0.9667\n",
      "Epoch 100/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1558 - accuracy: 0.9862 - val_loss: 0.1533 - val_accuracy: 0.9667\n",
      "Epoch 101/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1576 - accuracy: 0.9857 - val_loss: 0.1535 - val_accuracy: 0.9667\n",
      "Epoch 102/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1861 - accuracy: 0.9491 - val_loss: 0.1493 - val_accuracy: 0.9667\n",
      "Epoch 103/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1668 - accuracy: 0.9646 - val_loss: 0.1463 - val_accuracy: 0.9667\n",
      "Epoch 104/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1506 - accuracy: 0.9599 - val_loss: 0.1463 - val_accuracy: 0.9667\n",
      "Epoch 105/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1458 - accuracy: 0.9751 - val_loss: 0.1490 - val_accuracy: 0.9667\n",
      "Epoch 106/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1349 - accuracy: 0.9716 - val_loss: 0.1426 - val_accuracy: 0.9667\n",
      "Epoch 107/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1194 - accuracy: 0.9633 - val_loss: 0.1406 - val_accuracy: 0.9667\n",
      "Epoch 108/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1382 - accuracy: 0.9867 - val_loss: 0.1466 - val_accuracy: 0.9667\n",
      "Epoch 109/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1394 - accuracy: 0.9764 - val_loss: 0.1397 - val_accuracy: 0.9667\n",
      "Epoch 110/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1345 - accuracy: 0.9845 - val_loss: 0.1374 - val_accuracy: 0.9667\n",
      "Epoch 111/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1115 - accuracy: 0.9915 - val_loss: 0.1353 - val_accuracy: 0.9667\n",
      "Epoch 112/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1360 - accuracy: 0.9932 - val_loss: 0.1343 - val_accuracy: 0.9667\n",
      "Epoch 113/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1360 - accuracy: 0.9878 - val_loss: 0.1354 - val_accuracy: 0.9667\n",
      "Epoch 114/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1223 - accuracy: 0.9960 - val_loss: 0.1331 - val_accuracy: 0.9667\n",
      "Epoch 115/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1111 - accuracy: 0.9826 - val_loss: 0.1293 - val_accuracy: 0.9667\n",
      "Epoch 116/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1032 - accuracy: 0.9804 - val_loss: 0.1339 - val_accuracy: 0.9667\n",
      "Epoch 117/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1142 - accuracy: 0.9833 - val_loss: 0.1259 - val_accuracy: 0.9667\n",
      "Epoch 118/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1060 - accuracy: 0.9847 - val_loss: 0.1279 - val_accuracy: 0.9667\n",
      "Epoch 119/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1147 - accuracy: 0.9936 - val_loss: 0.1186 - val_accuracy: 0.9667\n",
      "Epoch 120/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1093 - accuracy: 0.9891 - val_loss: 0.1183 - val_accuracy: 0.9667\n",
      "Epoch 121/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0911 - accuracy: 0.9912 - val_loss: 0.1153 - val_accuracy: 0.9667\n",
      "Epoch 122/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0852 - accuracy: 0.9965 - val_loss: 0.1125 - val_accuracy: 0.9667\n",
      "Epoch 123/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0775 - accuracy: 0.9936 - val_loss: 0.1146 - val_accuracy: 0.9667\n",
      "Epoch 124/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0747 - accuracy: 0.9993 - val_loss: 0.1014 - val_accuracy: 0.9667\n",
      "Epoch 125/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0856 - accuracy: 0.9943 - val_loss: 0.1109 - val_accuracy: 0.9667\n",
      "Epoch 126/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0745 - accuracy: 0.9990 - val_loss: 0.1032 - val_accuracy: 0.9667\n",
      "Epoch 127/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 0.9936 - val_loss: 0.1101 - val_accuracy: 0.9667\n",
      "Epoch 128/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0721 - accuracy: 0.9978 - val_loss: 0.1038 - val_accuracy: 0.9667\n",
      "Epoch 129/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.9921 - val_loss: 0.1063 - val_accuracy: 0.9667\n",
      "Epoch 130/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0570 - accuracy: 0.9970 - val_loss: 0.1015 - val_accuracy: 0.9667\n",
      "Epoch 131/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0813 - accuracy: 0.9877 - val_loss: 0.1041 - val_accuracy: 0.9667\n",
      "Epoch 132/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0611 - accuracy: 0.9982 - val_loss: 0.1013 - val_accuracy: 0.9667\n",
      "Epoch 133/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0732 - accuracy: 0.9891 - val_loss: 0.0976 - val_accuracy: 0.9667\n",
      "Epoch 134/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0492 - accuracy: 0.9990 - val_loss: 0.0968 - val_accuracy: 0.9667\n",
      "Epoch 135/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0490 - accuracy: 0.9993 - val_loss: 0.0951 - val_accuracy: 0.9667\n",
      "Epoch 136/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0516 - accuracy: 0.9955 - val_loss: 0.1018 - val_accuracy: 0.9667\n",
      "Epoch 137/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0499 - accuracy: 0.9921 - val_loss: 0.1037 - val_accuracy: 0.9333\n",
      "Epoch 138/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0419 - accuracy: 0.9974 - val_loss: 0.1018 - val_accuracy: 0.9667\n",
      "Epoch 139/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0428 - accuracy: 0.9929 - val_loss: 0.1075 - val_accuracy: 0.9333\n",
      "Epoch 140/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0418 - accuracy: 0.9936 - val_loss: 0.0994 - val_accuracy: 0.9667\n",
      "Epoch 141/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0628 - accuracy: 0.9929 - val_loss: 0.1029 - val_accuracy: 0.9333\n",
      "Epoch 142/700\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0439 - accuracy: 0.9982 - val_loss: 0.0964 - val_accuracy: 0.9667\n",
      "Epoch 143/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0346 - accuracy: 0.9993 - val_loss: 0.1016 - val_accuracy: 0.9333\n",
      "Epoch 144/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0517 - accuracy: 0.9955 - val_loss: 0.1080 - val_accuracy: 0.9333\n",
      "Epoch 145/700\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0520 - accuracy: 0.9960 - val_loss: 0.1025 - val_accuracy: 0.9333\n",
      "Epoch 00145: early stopping\n",
      "\n",
      "Duration :  00:00:17 174ms\n"
     ]
    }
   ],
   "source": [
    "vID.chrono_start()\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "print(f\"x_train. Structure (shape) : {x_train.shape}\")\n",
    "print(f\"x_test. Structure (shape) : {x_test.shape}\")\n",
    "print(f\"y_train. Structure (shape) : {y_train.shape}\")\n",
    "print(f\"y_test. Structure (shape) : {y_test.shape}\")\n",
    "ANNmodel=get_model( (4,)) # 4 neurones d'entrée\n",
    "ANNmodel.summary()\n",
    "vID.chrono_start()\n",
    "ANNhistory = ANNmodel.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs          = 700,\n",
    "                    batch_size      = 5,\n",
    "                    verbose         = 1,\n",
    "                    validation_data = (x_test, y_test),\n",
    "                    callbacks=[es])\n",
    "vID.chrono_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f927a6-ce28-443a-b4f9-0c47140e51ef",
   "metadata": {},
   "source": [
    "#### f. Évaluation numérique globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe7d2f5f-14a3-4484-9cc6-69a080e0212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mx_train / loss      : 0.0481\u001b[0m\n",
      "\u001b[92mx_train/ accurracy  : 0.9917\u001b[0m\n",
      "\n",
      "\u001b[94mx_train / loss      : 0.1025\u001b[0m\n",
      "\u001b[94mx_train/ accurracy  : 0.9333\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "evalANN_on_Train = ANNmodel.evaluate(x_train, y_train, verbose=0)\n",
    "print(f\"{color.GREEN}x_train / loss      : {evalANN_on_Train[0]:5.4f}{color.OFF}\")\n",
    "print(f\"{color.GREEN}x_train/ accurracy  : {evalANN_on_Train[1]:5.4f}{color.OFF}\")\n",
    "print()\n",
    "evalANN_on_Test = ANNmodel.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"{color.BLUE}x_test / loss      : {evalANN_on_Test[0]:5.4f}{color.OFF}\")\n",
    "print(f\"{color.BLUE}x_test/ accurracy  : {evalANN_on_Test[1]:5.4f}{color.OFF}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149669f-0ee6-4f8f-9f40-1cb91e4929ad",
   "metadata": {},
   "source": [
    "#### g. Comportement du modèle vis-à-vis de chaque espèce d'iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35c740a5-53e5-4776-8cea-b53af7589366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[92mCatégories uniques d'iris :\u001b[0m ['setosa' 'versicolor' 'virginica']\n",
      "\u001b[1m\u001b[94mTest Set\u001b[0m. Comparaison entre espèce prédite et espèce observée\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "      <th>Espèce prédite</th>\n",
       "      <th>Espèce observée</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.16</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.97</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.03</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.72</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.96</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.96</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.19</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.96</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.94</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.60</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.03</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.94</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.70</td>\n",
       "      <td>virginica</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.25</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.76</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.97</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.40</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.94</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.77</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.95</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.96</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.96</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.96</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.95</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.02</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     setosa  versicolor  virginica Espèce prédite Espèce observée\n",
       "68     0.00        0.83       0.16     versicolor      versicolor\n",
       "113    0.00        0.01       0.98      virginica       virginica\n",
       "38     1.00        0.00       0.00         setosa          setosa\n",
       "141    0.00        0.04       0.96      virginica       virginica\n",
       "102    0.00        0.02       0.97      virginica       virginica\n",
       "20     1.00        0.00       0.00         setosa          setosa\n",
       "2      1.00        0.00       0.00         setosa          setosa\n",
       "16     1.00        0.00       0.00         setosa          setosa\n",
       "84     0.01        0.96       0.03     versicolor      versicolor\n",
       "138    0.01        0.27       0.72      virginica       virginica\n",
       "69     0.00        1.00       0.00     versicolor      versicolor\n",
       "31     1.00        0.00       0.00         setosa          setosa\n",
       "90     0.00        1.00       0.00     versicolor      versicolor\n",
       "44     1.00        0.00       0.00         setosa          setosa\n",
       "27     1.00        0.00       0.00         setosa          setosa\n",
       "29     1.00        0.00       0.00         setosa          setosa\n",
       "0      1.00        0.00       0.00         setosa          setosa\n",
       "103    0.00        0.03       0.96      virginica       virginica\n",
       "145    0.00        0.03       0.96      virginica       virginica\n",
       "46     1.00        0.00       0.00         setosa          setosa\n",
       "135    0.00        0.02       0.98      virginica       virginica\n",
       "72     0.00        0.80       0.19     versicolor      versicolor\n",
       "118    0.00        0.01       0.99      virginica       virginica\n",
       "52     0.00        0.99       0.01     versicolor      versicolor\n",
       "94     0.00        1.00       0.00     versicolor      versicolor\n",
       "131    0.00        0.04       0.96      virginica       virginica\n",
       "106    0.00        0.03       0.97      virginica       virginica\n",
       "101    0.00        0.02       0.98      virginica       virginica\n",
       "54     0.00        0.99       0.01     versicolor      versicolor\n",
       "39     1.00        0.00       0.00         setosa          setosa\n",
       "49     1.00        0.00       0.00         setosa          setosa\n",
       "109    0.00        0.04       0.96      virginica       virginica\n",
       "9      1.00        0.00       0.00         setosa          setosa\n",
       "30     1.00        0.00       0.00         setosa          setosa\n",
       "92     0.00        1.00       0.00     versicolor      versicolor\n",
       "140    0.00        0.03       0.96      virginica       virginica\n",
       "148    0.01        0.05       0.94      virginica       virginica\n",
       "55     0.00        0.99       0.01     versicolor      versicolor\n",
       "82     0.00        1.00       0.00     versicolor      versicolor\n",
       "11     1.00        0.00       0.00         setosa          setosa\n",
       "18     1.00        0.00       0.00         setosa          setosa\n",
       "8      1.00        0.00       0.00         setosa          setosa\n",
       "111    0.00        0.02       0.98      virginica       virginica\n",
       "17     1.00        0.00       0.00         setosa          setosa\n",
       "86     0.00        0.99       0.01     versicolor      versicolor\n",
       "59     0.00        0.98       0.02     versicolor      versicolor\n",
       "76     0.00        1.00       0.00     versicolor      versicolor\n",
       "93     0.00        1.00       0.00     versicolor      versicolor\n",
       "107    0.00        0.02       0.98      virginica       virginica\n",
       "48     1.00        0.00       0.00         setosa          setosa\n",
       "57     0.00        0.99       0.00     versicolor      versicolor\n",
       "75     0.00        1.00       0.00     versicolor      versicolor\n",
       "37     1.00        0.00       0.00         setosa          setosa\n",
       "134    0.01        0.39       0.60      virginica       virginica\n",
       "96     0.00        1.00       0.00     versicolor      versicolor\n",
       "43     1.00        0.00       0.00         setosa          setosa\n",
       "41     0.98        0.02       0.00         setosa          setosa\n",
       "33     1.00        0.00       0.00         setosa          setosa\n",
       "147    0.00        0.04       0.96      virginica       virginica\n",
       "7      1.00        0.00       0.00         setosa          setosa\n",
       "119    0.00        0.25       0.75      virginica       virginica\n",
       "28     1.00        0.00       0.00         setosa          setosa\n",
       "89     0.00        1.00       0.00     versicolor      versicolor\n",
       "56     0.01        0.97       0.03     versicolor      versicolor\n",
       "3      1.00        0.00       0.00         setosa          setosa\n",
       "137    0.01        0.06       0.94      virginica       virginica\n",
       "47     1.00        0.00       0.00         setosa          setosa\n",
       "146    0.00        0.01       0.98      virginica       virginica\n",
       "95     0.00        1.00       0.00     versicolor      versicolor\n",
       "83     0.01        0.29       0.70      virginica      versicolor\n",
       "77     0.01        0.74       0.25     versicolor      versicolor\n",
       "105    0.00        0.02       0.98      virginica       virginica\n",
       "14     1.00        0.00       0.00         setosa          setosa\n",
       "34     1.00        0.00       0.00         setosa          setosa\n",
       "45     1.00        0.00       0.00         setosa          setosa\n",
       "73     0.00        1.00       0.00     versicolor      versicolor\n",
       "127    0.01        0.23       0.76      virginica       virginica\n",
       "97     0.00        1.00       0.00     versicolor      versicolor\n",
       "53     0.00        0.99       0.01     versicolor      versicolor\n",
       "121    0.00        0.03       0.97      virginica       virginica\n",
       "23     1.00        0.00       0.00         setosa          setosa\n",
       "19     1.00        0.00       0.00         setosa          setosa\n",
       "132    0.00        0.02       0.97      virginica       virginica\n",
       "4      1.00        0.00       0.00         setosa          setosa\n",
       "104    0.00        0.03       0.97      virginica       virginica\n",
       "112    0.00        0.03       0.97      virginica       virginica\n",
       "80     0.00        1.00       0.00     versicolor      versicolor\n",
       "136    0.01        0.05       0.95      virginica       virginica\n",
       "42     1.00        0.00       0.00         setosa          setosa\n",
       "149    0.01        0.11       0.88      virginica       virginica\n",
       "70     0.02        0.58       0.40     versicolor      versicolor\n",
       "100    0.00        0.04       0.96      virginica       virginica\n",
       "114    0.00        0.02       0.98      virginica       virginica\n",
       "25     1.00        0.00       0.00         setosa          setosa\n",
       "116    0.01        0.06       0.94      virginica       virginica\n",
       "36     1.00        0.00       0.00         setosa          setosa\n",
       "126    0.01        0.22       0.77      virginica       virginica\n",
       "125    0.00        0.04       0.95      virginica       virginica\n",
       "91     0.00        0.99       0.01     versicolor      versicolor\n",
       "51     0.00        0.99       0.01     versicolor      versicolor\n",
       "60     0.00        1.00       0.00     versicolor      versicolor\n",
       "62     0.00        1.00       0.00     versicolor      versicolor\n",
       "87     0.00        1.00       0.00     versicolor      versicolor\n",
       "108    0.00        0.01       0.99      virginica       virginica\n",
       "122    0.00        0.01       0.99      virginica       virginica\n",
       "79     0.00        1.00       0.00     versicolor      versicolor\n",
       "139    0.00        0.03       0.96      virginica       virginica\n",
       "71     0.00        1.00       0.00     versicolor      versicolor\n",
       "13     1.00        0.00       0.00         setosa          setosa\n",
       "143    0.00        0.03       0.96      virginica       virginica\n",
       "35     1.00        0.00       0.00         setosa          setosa\n",
       "115    0.01        0.04       0.96      virginica       virginica\n",
       "120    0.00        0.03       0.96      virginica       virginica\n",
       "130    0.00        0.02       0.98      virginica       virginica\n",
       "22     1.00        0.00       0.00         setosa          setosa\n",
       "63     0.00        0.99       0.01     versicolor      versicolor\n",
       "110    0.01        0.04       0.95      virginica       virginica\n",
       "40     1.00        0.00       0.00         setosa          setosa\n",
       "78     0.00        0.97       0.02     versicolor      versicolor\n",
       "98     0.01        0.99       0.00     versicolor      versicolor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'erreurs : 1\n",
      "\n",
      "\u001b[1m\u001b[91mTest Set\u001b[0m. Comparaison entre espèce prédite et espèce observée.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "      <th>Espèce prédite</th>\n",
       "      <th>Espèce observée</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.02</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.02</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.85</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.97</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.48</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.96</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     setosa  versicolor  virginica Espèce prédite Espèce observée\n",
       "1      1.00        0.00       0.00         setosa          setosa\n",
       "5      1.00        0.00       0.00         setosa          setosa\n",
       "6      1.00        0.00       0.00         setosa          setosa\n",
       "10     1.00        0.00       0.00         setosa          setosa\n",
       "12     1.00        0.00       0.00         setosa          setosa\n",
       "15     1.00        0.00       0.00         setosa          setosa\n",
       "21     1.00        0.00       0.00         setosa          setosa\n",
       "24     1.00        0.00       0.00         setosa          setosa\n",
       "26     1.00        0.00       0.00         setosa          setosa\n",
       "32     1.00        0.00       0.00         setosa          setosa\n",
       "50     0.00        1.00       0.00     versicolor      versicolor\n",
       "58     0.00        1.00       0.00     versicolor      versicolor\n",
       "61     0.00        0.99       0.01     versicolor      versicolor\n",
       "64     0.00        0.99       0.00     versicolor      versicolor\n",
       "65     0.00        1.00       0.00     versicolor      versicolor\n",
       "66     0.01        0.97       0.02     versicolor      versicolor\n",
       "67     0.00        1.00       0.00     versicolor      versicolor\n",
       "74     0.00        1.00       0.00     versicolor      versicolor\n",
       "81     0.00        1.00       0.00     versicolor      versicolor\n",
       "85     0.01        0.97       0.02     versicolor      versicolor\n",
       "88     0.00        1.00       0.00     versicolor      versicolor\n",
       "99     0.00        1.00       0.00     versicolor      versicolor\n",
       "117    0.01        0.04       0.96      virginica       virginica\n",
       "123    0.00        0.14       0.85      virginica       virginica\n",
       "124    0.01        0.04       0.96      virginica       virginica\n",
       "128    0.00        0.02       0.97      virginica       virginica\n",
       "129    0.01        0.51       0.48     versicolor       virginica\n",
       "133    0.01        0.85       0.15     versicolor       virginica\n",
       "142    0.00        0.02       0.98      virginica       virginica\n",
       "144    0.00        0.03       0.96      virginica       virginica"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'erreurs : 2\n"
     ]
    }
   ],
   "source": [
    "usp = dfi['species'].unique()\n",
    "print(f\"{color.BOLD}{color.GREEN}Catégories uniques d'iris :{color.OFF} {usp}\")\n",
    "# cette correspondance élément 0 <-> setosa ; élément 1 <-> versicolor ; élément 2 <-> virginica\n",
    "# va servir à transformer les probabilités les plus élevées en espèce d'iris\n",
    "\n",
    "y_train_hat=ANNmodel.predict(x_train)\n",
    "ytr_hD = pd.DataFrame(y_train_hat, columns=usp, index=y_train.index)\n",
    "# argmax renvoie l'indice de la valeur maximale selon un axe (0 = ligne / 1 = colonne)\n",
    "# ça va ici nous renvoyer pour chaque ligne le numéro de la colonne qui contient la plus grande valeur de probabilité\n",
    "# on convertit maintenant 0, 1 & 2 en espèce d'iris\n",
    "iris_tr_hat = usp[np.argmax(y_train_hat,axis=1)]\n",
    "# on ajoute cette colonne au dataframe ytr_hD\n",
    "ytr_hD['Espèce prédite'] = pd.DataFrame(iris_tr_hat, index=y_train.index)\n",
    "ytr_hD['Espèce observée'] = pd.DataFrame(y_train_species, index=y_train.index)\n",
    "print(f\"{color.BOLD}{color.BLUE}Test Set{color.OFF}. Comparaison entre espèce prédite et espèce observée\")\n",
    "pd.set_option('display.max_rows', None) #on veut afficher toutes les lignes des DataFrames\n",
    "pd.options.display.float_format = '{:,.2f}'.format #on formatte les probabilités, sinon c'est illisible\n",
    "display(ytr_hD)\n",
    "pd.set_option('display.max_rows', 10) #reset de l'affichage: 5 premières et 5 dernières lignes\n",
    "pd.options.display.float_format = None #reset de l'affichage: format standard \n",
    "diff_Pred_Obs=np.where(ytr_hD['Espèce prédite'] == ytr_hD['Espèce observée'], 0, 1)\n",
    "print(f\"Nombre d'erreurs : {np.sum(diff_Pred_Obs)}\")\n",
    "\n",
    "print()\n",
    "y_test_hat=ANNmodel.predict(x_test)\n",
    "ytt_hD = pd.DataFrame(y_test_hat, columns=usp, index=y_test.index)\n",
    "# argmax renvoie l'indice de la valeur maximale selon un axe (0 = ligne / 1 = colonne)\n",
    "# ça va ici nous renvoyer pour chaque ligne le numéro de la colonne qui contient la plus grande valeur de probabilité\n",
    "# on convertit maintenant 0, 1 & 2 en espèce d'iris\n",
    "iris_tt_hat = usp[np.argmax(y_test_hat,axis=1)]\n",
    "# on ajoute cette colonne au dataframe ytt_hD\n",
    "ytt_hD['Espèce prédite'] = pd.DataFrame(iris_tt_hat, index=y_test.index)\n",
    "ytt_hD['Espèce observée'] = pd.DataFrame(y_test_species, index=y_test.index)\n",
    "print(f\"{color.BOLD}{color.RED}Test Set{color.OFF}. Comparaison entre espèce prédite et espèce observée.\")\n",
    "pd.set_option('display.max_rows', None) #on veut afficher toutes les lignes des DataFrames\n",
    "pd.options.display.float_format = '{:,.2f}'.format #on formatte les probabilités, sinon c'est illisible\n",
    "display(ytt_hD)\n",
    "pd.set_option('display.max_rows', 10) #reset de l'affichage: 5 premières et 5 dernières lignes\n",
    "pd.options.display.float_format = None #reset de l'affichage: \n",
    "diff_Pred_Obs=np.where(ytt_hD['Espèce prédite'] == ytt_hD['Espèce observée'], 0, 1)\n",
    "print(f\"Nombre d'erreurs : {np.sum(diff_Pred_Obs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55fe30e-ce3d-49bf-b488-6fe9bff3291b",
   "metadata": {},
   "source": [
    "#### h. Bilan de la performance du modèle prédictif sous forme de matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "866bd474-9393-4311-b165-0f66d021e221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sb\n",
    "\n",
    "cm_labels = dfi['species'].unique()\n",
    "cm_tr = confusion_matrix(np.argmax(y_train.to_numpy(),axis=1), np.argmax(y_train_hat,axis=1))\n",
    "cm_tt = confusion_matrix(np.argmax(y_test.to_numpy(),axis=1), np.argmax(y_test_hat,axis=1))\n",
    "\n",
    "fig=plt.figure(figsize=(12, 5))\n",
    "gs = gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[1, 1])\n",
    "\n",
    "ax00 = fig.add_subplot(gs[0, 0], title=\"Training set. Matrice de confusion\")\n",
    "sb.heatmap(pd.DataFrame(cm_tr, columns=cm_labels, index=cm_labels), ax=ax00, cmap=plt.cm.Blues, annot = True)\n",
    "ax00.set_xlabel(\"prédit\", fontsize = 20)\n",
    "ax00.set_ylabel(\"réel\", fontsize = 20)\n",
    "\n",
    "ax01=fig.add_subplot(gs[0, 1], title=\"Test set. Matrice de confusion\")\n",
    "sb.heatmap(pd.DataFrame(cm_tt, columns=cm_labels, index=cm_labels), ax=ax01, cmap=plt.cm.Blues, annot = True)\n",
    "ax01.set_xlabel(\"prédit\", fontsize = 20)\n",
    "ax01.set_ylabel(\"réel\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af354bcb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Fin à:** Sunday 30 October 2022, 18:42:45  \n",
       "**Durée:** 00:00:10 435ms"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"./config/svg/logoFin.svg\" style=\"margin-left:auto; margin-right:auto\"/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vID.end(cwd0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fad06d-6413-4793-b945-a7d707839fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
