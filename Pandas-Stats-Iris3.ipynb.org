{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8de690ce-49d6-4387-b242-fe36280ca9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: bold;\n",
       "}\n",
       "body {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: 200;\n",
       "}\n",
       "h1 {\n",
       "  border: 0px solid #333;\n",
       "  padding: 30px ;\n",
       "  color: white;\n",
       "  background: #b11d01;\n",
       "  text-align: center;\n",
       "}\n",
       "h2 {\n",
       "  border: 3px solid #333;\n",
       "  padding: 18px ;\n",
       "  color: #b11d01;\n",
       "  background: #ffffff;\n",
       "  text-align: center;\n",
       "}\n",
       "h3 {\n",
       "  border: 0px solid #333;\n",
       "  padding: 12px ;\n",
       "  color: #000000;\n",
       "  background: #c1c1c1;\n",
       "  text-align: left;\n",
       "}\n",
       "h4 {\n",
       "  border: 0px solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #d9fffc;\n",
       "  text-align: left;\n",
       "}\n",
       "div.warn {    \n",
       "    background-color: #fcf2f2;\n",
       "    border-color: #dFb5b4;\n",
       "    border-left: 5px solid #dfb5b4;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       "div.rq {    \n",
       "    background-color: #e2e2e2;\n",
       "    border-color: #969696;\n",
       "    border-left: 5px solid #969696;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Début à:** Wednesday 25 May 2022, 15:57:15  \n",
       "**Hostname:** insa-11357 (Linux)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"./svg/logoBegin.svg\" style=\"margin-left:auto; margin-right:auto\"></img></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Démarrage du thème 3\n",
    "import visualID as vID\n",
    "from visualID import color\n",
    "vID.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b71cea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Traitement statistique de données\n",
    "## 3. Apprentissage supervisé (*supervised Machine Learning*) appliqué à la classification<br>(régression logistique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43eab3-6d78-4c15-96c5-797e75ee940a",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "    <b style='color:red'>Ce thème n'est pas si complexe, mais l'analyse du code est réservée aux plus curieux et motivés.</b>\n",
    "    <br>Il a pour objectif de montrer qu'il est possible de prédire une valeur sur la base d'une corrélation multifactorielle entre une <b>classe d'objets</b> (ici des espèces d'iris) et des <b>propriétés</b> (ou descripteurs, ici les largeurs et longueurs des pétales et des sépales) \n",
    "    <br><b style='color:red'>Les moins curieux doivent <i>a minima</i> lire les commentaires et exécuter ce code pour en comprendre le principe.</b>\n",
    "   </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2f60c2-8759-4019-8dea-b692e73eaee6",
   "metadata": {},
   "source": [
    "### 3.1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a5449-7e81-4541-a826-da0b12074809",
   "metadata": {},
   "source": [
    "#### 3.1.a. Principe général"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02aa070-ddf4-4bb4-bb26-5043ee7ca53f",
   "metadata": {},
   "source": [
    "#### 3.1.b. Importation des librairies utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6568ba3-8b12-4725-a890-132a9de260b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a625d707-1cc5-45eb-bdc0-8be3cfe2e9b2",
   "metadata": {},
   "source": [
    "### 3.2. Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139cdc56-49f8-4775-abb6-1e9b2ff1b292",
   "metadata": {},
   "source": [
    "#### 3.2.a. Séparation des données en deux sous-ensembles d'apprentissage et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0470f71-6f34-4ccf-9e56-836928e32004",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dfi. Structure (shape) :(150, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species\n",
       "0             5.1          3.5           1.4          0.2     setosa\n",
       "1             4.9          3.0           1.4          0.2     setosa\n",
       "2             4.7          3.2           1.3          0.2     setosa\n",
       "3             4.6          3.1           1.5          0.2     setosa\n",
       "4             5.0          3.6           1.4          0.2     setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  virginica\n",
       "146           6.3          2.5           5.0          1.9  virginica\n",
       "147           6.5          3.0           5.2          2.0  virginica\n",
       "148           6.2          3.4           5.4          2.3  virginica\n",
       "149           5.9          3.0           5.1          1.8  virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train :  (120, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>7.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width\n",
       "46            5.1          3.8           1.6          0.2\n",
       "38            4.4          3.0           1.3          0.2\n",
       "114           5.8          2.8           5.1          2.4\n",
       "76            6.8          2.8           4.8          1.4\n",
       "8             4.4          2.9           1.4          0.2\n",
       "..            ...          ...           ...          ...\n",
       "107           7.3          2.9           6.3          1.8\n",
       "97            6.2          2.9           4.3          1.3\n",
       "23            5.1          3.3           1.7          0.5\n",
       "36            5.5          3.5           1.3          0.2\n",
       "104           6.5          3.0           5.8          2.2\n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train :  (120,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46         setosa\n",
       "38         setosa\n",
       "114     virginica\n",
       "76     versicolor\n",
       "8          setosa\n",
       "          ...    \n",
       "107     virginica\n",
       "97     versicolor\n",
       "23         setosa\n",
       "36         setosa\n",
       "104     virginica\n",
       "Name: species, Length: 120, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfi=pd.read_csv('./iris-data/iris.csv', sep=\";\")\n",
    "print(f\"Dfi. Structure (shape) :{dfi.shape}\")\n",
    "display(dfi)\n",
    "data_train = dfi.sample(frac=0.8, axis='index')\n",
    "data_test  = dfi.drop(data_train.index)\n",
    "x_train = data_train.drop(['species'],axis=1)\n",
    "y_train = data_train['species']\n",
    "x_test  = data_test.drop(['species'],axis=1)\n",
    "y_test  = data_test['species']\n",
    "print('x_train : ',x_train.shape)\n",
    "display(x_train)\n",
    "print('y_train : ',y_train.shape)\n",
    "display(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b4f9b-46d6-4988-a3f3-ab75734b6ffa",
   "metadata": {},
   "source": [
    "#### 3.2.b. Adaptation des données à la régression logistique par le réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70779f6b-4820-427f-a2ce-96ac775809a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catégories uniques : ['setosa' 'versicolor' 'virginica']\n",
      "Correspondance entre chaque catégorie unique et un entier : {'setosa': 0, 'versicolor': 1, 'virginica': 2}\n",
      "Structure (shape) des tableaux renvoyés par categorize1C_2ohe. y1 : (120, 3), y2 : (30, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_718df\">\n",
       "  <caption>Training set après normalisation (avec scikit-learn):</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_718df_level0_col0\" class=\"col_heading level0 col0\" >sepal_length</th>\n",
       "      <th id=\"T_718df_level0_col1\" class=\"col_heading level0 col1\" >sepal_width</th>\n",
       "      <th id=\"T_718df_level0_col2\" class=\"col_heading level0 col2\" >petal_length</th>\n",
       "      <th id=\"T_718df_level0_col3\" class=\"col_heading level0 col3\" >petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_718df_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_718df_row0_col0\" class=\"data row0 col0\" >120.00</td>\n",
       "      <td id=\"T_718df_row0_col1\" class=\"data row0 col1\" >120.00</td>\n",
       "      <td id=\"T_718df_row0_col2\" class=\"data row0 col2\" >120.00</td>\n",
       "      <td id=\"T_718df_row0_col3\" class=\"data row0 col3\" >120.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_718df_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_718df_row1_col0\" class=\"data row1 col0\" >0.00</td>\n",
       "      <td id=\"T_718df_row1_col1\" class=\"data row1 col1\" >-0.00</td>\n",
       "      <td id=\"T_718df_row1_col2\" class=\"data row1 col2\" >0.00</td>\n",
       "      <td id=\"T_718df_row1_col3\" class=\"data row1 col3\" >-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_718df_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_718df_row2_col0\" class=\"data row2 col0\" >1.00</td>\n",
       "      <td id=\"T_718df_row2_col1\" class=\"data row2 col1\" >1.00</td>\n",
       "      <td id=\"T_718df_row2_col2\" class=\"data row2 col2\" >1.00</td>\n",
       "      <td id=\"T_718df_row2_col3\" class=\"data row2 col3\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_718df_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_718df_row3_col0\" class=\"data row3 col0\" >-1.87</td>\n",
       "      <td id=\"T_718df_row3_col1\" class=\"data row3 col1\" >-2.49</td>\n",
       "      <td id=\"T_718df_row3_col2\" class=\"data row3 col2\" >-1.51</td>\n",
       "      <td id=\"T_718df_row3_col3\" class=\"data row3 col3\" >-1.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_718df_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_718df_row4_col0\" class=\"data row4 col0\" >-0.89</td>\n",
       "      <td id=\"T_718df_row4_col1\" class=\"data row4 col1\" >-0.60</td>\n",
       "      <td id=\"T_718df_row4_col2\" class=\"data row4 col2\" >-1.24</td>\n",
       "      <td id=\"T_718df_row4_col3\" class=\"data row4 col3\" >-1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_718df_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_718df_row5_col0\" class=\"data row5 col0\" >-0.10</td>\n",
       "      <td id=\"T_718df_row5_col1\" class=\"data row5 col1\" >-0.13</td>\n",
       "      <td id=\"T_718df_row5_col2\" class=\"data row5 col2\" >0.31</td>\n",
       "      <td id=\"T_718df_row5_col3\" class=\"data row5 col3\" >0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_718df_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_718df_row6_col0\" class=\"data row6 col0\" >0.69</td>\n",
       "      <td id=\"T_718df_row6_col1\" class=\"data row6 col1\" >0.58</td>\n",
       "      <td id=\"T_718df_row6_col2\" class=\"data row6 col2\" >0.76</td>\n",
       "      <td id=\"T_718df_row6_col3\" class=\"data row6 col3\" >0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_718df_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_718df_row7_col0\" class=\"data row7 col0\" >2.52</td>\n",
       "      <td id=\"T_718df_row7_col1\" class=\"data row7 col1\" >2.71</td>\n",
       "      <td id=\"T_718df_row7_col2\" class=\"data row7 col2\" >1.79</td>\n",
       "      <td id=\"T_718df_row7_col3\" class=\"data row7 col3\" >1.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f0e14073700>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_34499\">\n",
       "  <caption>Test set after après normalisation (avec scikit-learn):</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_34499_level0_col0\" class=\"col_heading level0 col0\" >sepal_length</th>\n",
       "      <th id=\"T_34499_level0_col1\" class=\"col_heading level0 col1\" >sepal_width</th>\n",
       "      <th id=\"T_34499_level0_col2\" class=\"col_heading level0 col2\" >petal_length</th>\n",
       "      <th id=\"T_34499_level0_col3\" class=\"col_heading level0 col3\" >petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_34499_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_34499_row0_col0\" class=\"data row0 col0\" >30.00</td>\n",
       "      <td id=\"T_34499_row0_col1\" class=\"data row0 col1\" >30.00</td>\n",
       "      <td id=\"T_34499_row0_col2\" class=\"data row0 col2\" >30.00</td>\n",
       "      <td id=\"T_34499_row0_col3\" class=\"data row0 col3\" >30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34499_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_34499_row1_col0\" class=\"data row1 col0\" >0.08</td>\n",
       "      <td id=\"T_34499_row1_col1\" class=\"data row1 col1\" >-0.00</td>\n",
       "      <td id=\"T_34499_row1_col2\" class=\"data row1 col2\" >0.00</td>\n",
       "      <td id=\"T_34499_row1_col3\" class=\"data row1 col3\" >0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34499_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_34499_row2_col0\" class=\"data row2 col0\" >1.05</td>\n",
       "      <td id=\"T_34499_row2_col1\" class=\"data row2 col1\" >1.12</td>\n",
       "      <td id=\"T_34499_row2_col2\" class=\"data row2 col2\" >1.02</td>\n",
       "      <td id=\"T_34499_row2_col3\" class=\"data row2 col3\" >0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34499_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_34499_row3_col0\" class=\"data row3 col0\" >-1.74</td>\n",
       "      <td id=\"T_34499_row3_col1\" class=\"data row3 col1\" >-2.02</td>\n",
       "      <td id=\"T_34499_row3_col2\" class=\"data row3 col2\" >-1.57</td>\n",
       "      <td id=\"T_34499_row3_col3\" class=\"data row3 col3\" >-1.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34499_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_34499_row4_col0\" class=\"data row4 col0\" >-0.98</td>\n",
       "      <td id=\"T_34499_row4_col1\" class=\"data row4 col1\" >-0.60</td>\n",
       "      <td id=\"T_34499_row4_col2\" class=\"data row4 col2\" >-1.23</td>\n",
       "      <td id=\"T_34499_row4_col3\" class=\"data row4 col3\" >-1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34499_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_34499_row5_col0\" class=\"data row5 col0\" >0.27</td>\n",
       "      <td id=\"T_34499_row5_col1\" class=\"data row5 col1\" >0.11</td>\n",
       "      <td id=\"T_34499_row5_col2\" class=\"data row5 col2\" >0.37</td>\n",
       "      <td id=\"T_34499_row5_col3\" class=\"data row5 col3\" >0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34499_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_34499_row6_col0\" class=\"data row6 col0\" >0.69</td>\n",
       "      <td id=\"T_34499_row6_col1\" class=\"data row6 col1\" >0.58</td>\n",
       "      <td id=\"T_34499_row6_col2\" class=\"data row6 col2\" >0.69</td>\n",
       "      <td id=\"T_34499_row6_col3\" class=\"data row6 col3\" >0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34499_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_34499_row7_col0\" class=\"data row7 col0\" >2.28</td>\n",
       "      <td id=\"T_34499_row7_col1\" class=\"data row7 col1\" >3.18</td>\n",
       "      <td id=\"T_34499_row7_col2\" class=\"data row7 col2\" >1.68</td>\n",
       "      <td id=\"T_34499_row7_col3\" class=\"data row7 col3\" >1.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f0d855517c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# one-hot-encoding des espèces de y_train & y_test. \n",
    "# on utilise une fonction maison, qui elle-même appelle la fonction to_categorical de keras\n",
    "# cette fonction, categorizeY_2ohe, est définie dans le package visualID importé au début ce ce notebook\n",
    "y_train_ohe, y_test_ohe = vID.categorizeY_2ohe(dfi[\"species\"], y_train, y_test)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(x_train.values)\n",
    "x_trainS = scaler.transform(x_train.values) #returns a numpy array\n",
    "x_testS = scaler.transform(x_test.values) #returns a numpy array\n",
    "x_trainD = pd.DataFrame(x_trainS, columns=x_train.columns, index=x_train.index)\n",
    "x_testD = pd.DataFrame(x_testS, columns=x_test.columns, index=x_test.index)\n",
    "display(x_trainD.describe().style.format(\"{0:.2f}\").set_caption(\"Training set après normalisation (avec scikit-learn):\"))\n",
    "display(x_testD.describe().style.format(\"{0:.2f}\").set_caption(\"Test set after après normalisation (avec scikit-learn):\"))\n",
    "x_train = x_trainS\n",
    "x_test = x_testS\n",
    "del x_trainD, x_testD, x_trainS, x_testS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d9cd3-ebdc-4bb3-bb75-640600b7d2de",
   "metadata": {},
   "source": [
    "### 3.3. Modèle de réseau de neurones (ANN = artificial neural network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7111b5f-5db1-457a-a4ed-a93902334ebe",
   "metadata": {},
   "source": [
    "#### 3.3.a. Définition du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "776ea4a4-a5f4-4898-886d-cec103569b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(shape):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name='iLayer'))\n",
    "    model.add(keras.layers.Dense(7, activation='relu', name='hLayer1'))\n",
    "    model.add(keras.layers.Dense(5, activation='relu', name='hLayer2'))\n",
    "    model.add(keras.layers.Dense(3, activation='softmax', name='oLayer'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'categorical_crossentropy',\n",
    "                  metrics   = ['accuracy'] )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32938fc-ef05-46d6-9f26-80a727d0bc8f",
   "metadata": {},
   "source": [
    "#### 3.3.b. Apprentissage supervisé du réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "877b00e0-83b3-4540-ad09-ffa631fde346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train. Structure (shape) : (120, 4)\n",
      "x_test. Structure (shape) : (30, 4)\n",
      "y_train_ohe. Structure (shape) : (120, 3)\n",
      "y_test_ohe. Structure (shape) : (30, 3)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hLayer1 (Dense)              (None, 7)                 35        \n",
      "_________________________________________________________________\n",
      "hLayer2 (Dense)              (None, 5)                 40        \n",
      "_________________________________________________________________\n",
      "oLayer (Dense)               (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 93\n",
      "Trainable params: 93\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/700\n",
      "24/24 [==============================] - 1s 22ms/step - loss: 1.7168 - accuracy: 0.3479 - val_loss: 1.7394 - val_accuracy: 0.3000\n",
      "Epoch 2/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 1.7000 - accuracy: 0.2851 - val_loss: 1.5060 - val_accuracy: 0.3000\n",
      "Epoch 3/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 1.3962 - accuracy: 0.3426 - val_loss: 1.3342 - val_accuracy: 0.3000\n",
      "Epoch 4/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 1.3107 - accuracy: 0.3252 - val_loss: 1.1977 - val_accuracy: 0.3000\n",
      "Epoch 5/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 1.1873 - accuracy: 0.3052 - val_loss: 1.0814 - val_accuracy: 0.3000\n",
      "Epoch 6/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 1.0629 - accuracy: 0.3509 - val_loss: 0.9836 - val_accuracy: 0.3000\n",
      "Epoch 7/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.9501 - accuracy: 0.3551 - val_loss: 0.9070 - val_accuracy: 0.3000\n",
      "Epoch 8/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.8698 - accuracy: 0.3744 - val_loss: 0.8477 - val_accuracy: 0.3000\n",
      "Epoch 9/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.8401 - accuracy: 0.3194 - val_loss: 0.7976 - val_accuracy: 0.3000\n",
      "Epoch 10/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.7343 - accuracy: 0.5486 - val_loss: 0.7622 - val_accuracy: 0.5000\n",
      "Epoch 11/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.7370 - accuracy: 0.5972 - val_loss: 0.7257 - val_accuracy: 0.6333\n",
      "Epoch 12/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.7191 - accuracy: 0.6587 - val_loss: 0.6895 - val_accuracy: 0.6333\n",
      "Epoch 13/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.6416 - accuracy: 0.8240 - val_loss: 0.6619 - val_accuracy: 0.6333\n",
      "Epoch 14/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.6170 - accuracy: 0.7570 - val_loss: 0.6324 - val_accuracy: 0.6667\n",
      "Epoch 15/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.6169 - accuracy: 0.7695 - val_loss: 0.6039 - val_accuracy: 0.7000\n",
      "Epoch 16/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5755 - accuracy: 0.7726 - val_loss: 0.5817 - val_accuracy: 0.7000\n",
      "Epoch 17/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.5641 - accuracy: 0.8199 - val_loss: 0.5568 - val_accuracy: 0.7000\n",
      "Epoch 18/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.5439 - accuracy: 0.7913 - val_loss: 0.5337 - val_accuracy: 0.7000\n",
      "Epoch 19/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5100 - accuracy: 0.8278 - val_loss: 0.5201 - val_accuracy: 0.7000\n",
      "Epoch 20/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4962 - accuracy: 0.8405 - val_loss: 0.5030 - val_accuracy: 0.7000\n",
      "Epoch 21/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4549 - accuracy: 0.8309 - val_loss: 0.4862 - val_accuracy: 0.7000\n",
      "Epoch 22/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.3985 - accuracy: 0.8394 - val_loss: 0.4727 - val_accuracy: 0.7000\n",
      "Epoch 23/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3713 - accuracy: 0.8864 - val_loss: 0.4647 - val_accuracy: 0.7000\n",
      "Epoch 24/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4131 - accuracy: 0.8689 - val_loss: 0.4490 - val_accuracy: 0.7667\n",
      "Epoch 25/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4359 - accuracy: 0.8297 - val_loss: 0.4405 - val_accuracy: 0.7667\n",
      "Epoch 26/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.3851 - accuracy: 0.8712 - val_loss: 0.4338 - val_accuracy: 0.7667\n",
      "Epoch 27/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3891 - accuracy: 0.8558 - val_loss: 0.4259 - val_accuracy: 0.7667\n",
      "Epoch 28/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4031 - accuracy: 0.8738 - val_loss: 0.4190 - val_accuracy: 0.7667\n",
      "Epoch 29/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3570 - accuracy: 0.8544 - val_loss: 0.4143 - val_accuracy: 0.7667\n",
      "Epoch 30/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4082 - accuracy: 0.8468 - val_loss: 0.4083 - val_accuracy: 0.7667\n",
      "Epoch 31/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3996 - accuracy: 0.8466 - val_loss: 0.4040 - val_accuracy: 0.7667\n",
      "Epoch 32/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3855 - accuracy: 0.8306 - val_loss: 0.3989 - val_accuracy: 0.7667\n",
      "Epoch 33/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.3671 - accuracy: 0.8492 - val_loss: 0.3924 - val_accuracy: 0.7667\n",
      "Epoch 34/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.3729 - accuracy: 0.8875 - val_loss: 0.3881 - val_accuracy: 0.7667\n",
      "Epoch 35/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3431 - accuracy: 0.8742 - val_loss: 0.3858 - val_accuracy: 0.7667\n",
      "Epoch 36/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3578 - accuracy: 0.8126 - val_loss: 0.3772 - val_accuracy: 0.7667\n",
      "Epoch 37/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.3252 - accuracy: 0.9294 - val_loss: 0.3782 - val_accuracy: 0.7667\n",
      "Epoch 38/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2928 - accuracy: 0.9292 - val_loss: 0.3710 - val_accuracy: 0.7667\n",
      "Epoch 39/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2978 - accuracy: 0.9181 - val_loss: 0.3658 - val_accuracy: 0.7667\n",
      "Epoch 40/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.3353 - accuracy: 0.9021 - val_loss: 0.3609 - val_accuracy: 0.7667\n",
      "Epoch 41/700\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.3275 - accuracy: 0.9331 - val_loss: 0.3585 - val_accuracy: 0.7667\n",
      "Epoch 42/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3035 - accuracy: 0.9429 - val_loss: 0.3573 - val_accuracy: 0.7667\n",
      "Epoch 43/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3164 - accuracy: 0.9085 - val_loss: 0.3504 - val_accuracy: 0.7667\n",
      "Epoch 44/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.3474 - accuracy: 0.8742 - val_loss: 0.3463 - val_accuracy: 0.7667\n",
      "Epoch 45/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2919 - accuracy: 0.9377 - val_loss: 0.3436 - val_accuracy: 0.7667\n",
      "Epoch 46/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2954 - accuracy: 0.8947 - val_loss: 0.3386 - val_accuracy: 0.7667\n",
      "Epoch 47/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2819 - accuracy: 0.9376 - val_loss: 0.3382 - val_accuracy: 0.7667\n",
      "Epoch 48/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2982 - accuracy: 0.9042 - val_loss: 0.3363 - val_accuracy: 0.7667\n",
      "Epoch 49/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3226 - accuracy: 0.9048 - val_loss: 0.3289 - val_accuracy: 0.7667\n",
      "Epoch 50/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2485 - accuracy: 0.9479 - val_loss: 0.3272 - val_accuracy: 0.7667\n",
      "Epoch 51/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3431 - accuracy: 0.8820 - val_loss: 0.3199 - val_accuracy: 0.8000\n",
      "Epoch 52/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.2970 - accuracy: 0.9073 - val_loss: 0.3201 - val_accuracy: 0.8000\n",
      "Epoch 53/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3093 - accuracy: 0.8927 - val_loss: 0.3169 - val_accuracy: 0.8000\n",
      "Epoch 54/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2955 - accuracy: 0.9193 - val_loss: 0.3106 - val_accuracy: 0.8000\n",
      "Epoch 55/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2653 - accuracy: 0.9148 - val_loss: 0.3096 - val_accuracy: 0.8000\n",
      "Epoch 56/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2562 - accuracy: 0.9302 - val_loss: 0.3052 - val_accuracy: 0.8000\n",
      "Epoch 57/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3037 - accuracy: 0.8805 - val_loss: 0.3044 - val_accuracy: 0.8000\n",
      "Epoch 58/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2620 - accuracy: 0.9255 - val_loss: 0.2977 - val_accuracy: 0.8333\n",
      "Epoch 59/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2590 - accuracy: 0.9255 - val_loss: 0.2937 - val_accuracy: 0.8667\n",
      "Epoch 60/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2588 - accuracy: 0.9264 - val_loss: 0.2928 - val_accuracy: 0.8667\n",
      "Epoch 61/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2563 - accuracy: 0.9256 - val_loss: 0.2870 - val_accuracy: 0.8667\n",
      "Epoch 62/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.2475 - accuracy: 0.9208 - val_loss: 0.2856 - val_accuracy: 0.8667\n",
      "Epoch 63/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2589 - accuracy: 0.9244 - val_loss: 0.2827 - val_accuracy: 0.8667\n",
      "Epoch 64/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2370 - accuracy: 0.9215 - val_loss: 0.2780 - val_accuracy: 0.8667\n",
      "Epoch 65/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2166 - accuracy: 0.9349 - val_loss: 0.2765 - val_accuracy: 0.8667\n",
      "Epoch 66/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1945 - accuracy: 0.9472 - val_loss: 0.2708 - val_accuracy: 0.8667\n",
      "Epoch 67/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2458 - accuracy: 0.9333 - val_loss: 0.2636 - val_accuracy: 0.8667\n",
      "Epoch 68/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2099 - accuracy: 0.9612 - val_loss: 0.2653 - val_accuracy: 0.8667\n",
      "Epoch 69/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2405 - accuracy: 0.9365 - val_loss: 0.2605 - val_accuracy: 0.8667\n",
      "Epoch 70/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2187 - accuracy: 0.9506 - val_loss: 0.2559 - val_accuracy: 0.8667\n",
      "Epoch 71/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2255 - accuracy: 0.9364 - val_loss: 0.2544 - val_accuracy: 0.8667\n",
      "Epoch 72/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1924 - accuracy: 0.9666 - val_loss: 0.2486 - val_accuracy: 0.9000\n",
      "Epoch 73/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2425 - accuracy: 0.9091 - val_loss: 0.2451 - val_accuracy: 0.9333\n",
      "Epoch 74/700\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.2034 - accuracy: 0.9356 - val_loss: 0.2445 - val_accuracy: 0.9333\n",
      "Epoch 75/700\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1822 - accuracy: 0.9435 - val_loss: 0.2429 - val_accuracy: 0.9333\n",
      "Epoch 76/700\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1840 - accuracy: 0.9346 - val_loss: 0.2395 - val_accuracy: 0.9333\n",
      "Epoch 77/700\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2168 - accuracy: 0.9281 - val_loss: 0.2318 - val_accuracy: 0.9333\n",
      "Epoch 78/700\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1749 - accuracy: 0.9351 - val_loss: 0.2338 - val_accuracy: 0.9333\n",
      "Epoch 79/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1531 - accuracy: 0.9586 - val_loss: 0.2334 - val_accuracy: 0.9333\n",
      "Epoch 80/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2069 - accuracy: 0.9062 - val_loss: 0.2207 - val_accuracy: 0.9333\n",
      "Epoch 81/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2061 - accuracy: 0.9285 - val_loss: 0.2186 - val_accuracy: 0.9333\n",
      "Epoch 82/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1692 - accuracy: 0.9543 - val_loss: 0.2217 - val_accuracy: 0.9333\n",
      "Epoch 83/700\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.1785 - accuracy: 0.9585 - val_loss: 0.2199 - val_accuracy: 0.9333\n",
      "Epoch 84/700\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2032 - accuracy: 0.9402 - val_loss: 0.2131 - val_accuracy: 0.9333\n",
      "Epoch 85/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1864 - accuracy: 0.9462 - val_loss: 0.2131 - val_accuracy: 0.9333\n",
      "Epoch 86/700\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1838 - accuracy: 0.9546 - val_loss: 0.2068 - val_accuracy: 0.9333\n",
      "Epoch 87/700\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1462 - accuracy: 0.9835 - val_loss: 0.2081 - val_accuracy: 0.9333\n",
      "Epoch 88/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1785 - accuracy: 0.9554 - val_loss: 0.2024 - val_accuracy: 0.9333\n",
      "Epoch 89/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1170 - accuracy: 0.9888 - val_loss: 0.2020 - val_accuracy: 0.9333\n",
      "Epoch 90/700\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.1707 - accuracy: 0.9775 - val_loss: 0.1982 - val_accuracy: 0.9333\n",
      "Epoch 91/700\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1527 - accuracy: 0.9738 - val_loss: 0.1968 - val_accuracy: 0.9333\n",
      "Epoch 92/700\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1572 - accuracy: 0.9708 - val_loss: 0.1934 - val_accuracy: 0.9333\n",
      "Epoch 93/700\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.1839 - accuracy: 0.9634 - val_loss: 0.1905 - val_accuracy: 0.9333\n",
      "Epoch 94/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1954 - accuracy: 0.9493 - val_loss: 0.1865 - val_accuracy: 0.9333\n",
      "Epoch 95/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1743 - accuracy: 0.9526 - val_loss: 0.1864 - val_accuracy: 0.9333\n",
      "Epoch 96/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1449 - accuracy: 0.9727 - val_loss: 0.1856 - val_accuracy: 0.9333\n",
      "Epoch 97/700\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.1412 - accuracy: 0.9816 - val_loss: 0.1853 - val_accuracy: 0.9333\n",
      "Epoch 98/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1344 - accuracy: 0.9805 - val_loss: 0.1802 - val_accuracy: 0.9333\n",
      "Epoch 99/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1666 - accuracy: 0.9789 - val_loss: 0.1813 - val_accuracy: 0.9333\n",
      "Epoch 100/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1215 - accuracy: 0.9859 - val_loss: 0.1743 - val_accuracy: 0.9333\n",
      "Epoch 101/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1312 - accuracy: 0.9808 - val_loss: 0.1738 - val_accuracy: 0.9333\n",
      "Epoch 102/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1488 - accuracy: 0.9697 - val_loss: 0.1727 - val_accuracy: 0.9333\n",
      "Epoch 103/700\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1488 - accuracy: 0.9736 - val_loss: 0.1692 - val_accuracy: 0.9333\n",
      "Epoch 104/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1070 - accuracy: 0.9905 - val_loss: 0.1678 - val_accuracy: 0.9333\n",
      "Epoch 105/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1614 - accuracy: 0.9628 - val_loss: 0.1684 - val_accuracy: 0.9333\n",
      "Epoch 106/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1512 - accuracy: 0.9729 - val_loss: 0.1620 - val_accuracy: 0.9333\n",
      "Epoch 107/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1482 - accuracy: 0.9554 - val_loss: 0.1591 - val_accuracy: 0.9333\n",
      "Epoch 108/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1670 - accuracy: 0.9398 - val_loss: 0.1608 - val_accuracy: 0.9333\n",
      "Epoch 109/700\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.1160 - accuracy: 0.9791 - val_loss: 0.1592 - val_accuracy: 0.9333\n",
      "Epoch 110/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1321 - accuracy: 0.9699 - val_loss: 0.1600 - val_accuracy: 0.9333\n",
      "Epoch 111/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1205 - accuracy: 0.9810 - val_loss: 0.1566 - val_accuracy: 0.9333\n",
      "Epoch 112/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1054 - accuracy: 0.9886 - val_loss: 0.1543 - val_accuracy: 0.9333\n",
      "Epoch 113/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1144 - accuracy: 0.9764 - val_loss: 0.1516 - val_accuracy: 0.9333\n",
      "Epoch 114/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1344 - accuracy: 0.9886 - val_loss: 0.1576 - val_accuracy: 0.9333\n",
      "Epoch 115/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1139 - accuracy: 0.9759 - val_loss: 0.1530 - val_accuracy: 0.9333\n",
      "Epoch 116/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1130 - accuracy: 0.9793 - val_loss: 0.1476 - val_accuracy: 0.9333\n",
      "Epoch 117/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1124 - accuracy: 0.9651 - val_loss: 0.1489 - val_accuracy: 0.9333\n",
      "Epoch 118/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1107 - accuracy: 0.9812 - val_loss: 0.1484 - val_accuracy: 0.9333\n",
      "Epoch 119/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1083 - accuracy: 0.9757 - val_loss: 0.1509 - val_accuracy: 0.9333\n",
      "Epoch 120/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0975 - accuracy: 0.9855 - val_loss: 0.1464 - val_accuracy: 0.9333\n",
      "Epoch 121/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1191 - accuracy: 0.9549 - val_loss: 0.1430 - val_accuracy: 0.9333\n",
      "Epoch 122/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0831 - accuracy: 0.9827 - val_loss: 0.1430 - val_accuracy: 0.9333\n",
      "Epoch 123/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0971 - accuracy: 0.9783 - val_loss: 0.1443 - val_accuracy: 0.9333\n",
      "Epoch 124/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0985 - accuracy: 0.9739 - val_loss: 0.1435 - val_accuracy: 0.9333\n",
      "Epoch 125/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0921 - accuracy: 0.9792 - val_loss: 0.1415 - val_accuracy: 0.9333\n",
      "Epoch 126/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0860 - accuracy: 0.9855 - val_loss: 0.1430 - val_accuracy: 0.9333\n",
      "Epoch 127/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0950 - accuracy: 0.9941 - val_loss: 0.1450 - val_accuracy: 0.9333\n",
      "Epoch 128/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1203 - accuracy: 0.9551 - val_loss: 0.1349 - val_accuracy: 0.9333\n",
      "Epoch 129/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1192 - accuracy: 0.9463 - val_loss: 0.1360 - val_accuracy: 0.9333\n",
      "Epoch 130/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0959 - accuracy: 0.9860 - val_loss: 0.1422 - val_accuracy: 0.9333\n",
      "Epoch 131/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0764 - accuracy: 0.9885 - val_loss: 0.1375 - val_accuracy: 0.9333\n",
      "Epoch 132/700\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1107 - accuracy: 0.9661 - val_loss: 0.1377 - val_accuracy: 0.9333\n",
      "Epoch 133/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1151 - accuracy: 0.9487 - val_loss: 0.1351 - val_accuracy: 0.9333\n",
      "Epoch 134/700\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1227 - accuracy: 0.9484 - val_loss: 0.1381 - val_accuracy: 0.9333\n",
      "Epoch 135/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1052 - accuracy: 0.9639 - val_loss: 0.1371 - val_accuracy: 0.9333\n",
      "Epoch 136/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1029 - accuracy: 0.9642 - val_loss: 0.1393 - val_accuracy: 0.9333\n",
      "Epoch 137/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0870 - accuracy: 0.9745 - val_loss: 0.1316 - val_accuracy: 0.9333\n",
      "Epoch 138/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0833 - accuracy: 0.9699 - val_loss: 0.1359 - val_accuracy: 0.9333\n",
      "Epoch 139/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1079 - accuracy: 0.9426 - val_loss: 0.1310 - val_accuracy: 0.9333\n",
      "Epoch 140/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0704 - accuracy: 0.9770 - val_loss: 0.1339 - val_accuracy: 0.9333\n",
      "Epoch 141/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1023 - accuracy: 0.9709 - val_loss: 0.1321 - val_accuracy: 0.9333\n",
      "Epoch 142/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1205 - accuracy: 0.9458 - val_loss: 0.1344 - val_accuracy: 0.9333\n",
      "Epoch 143/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0840 - accuracy: 0.9749 - val_loss: 0.1304 - val_accuracy: 0.9333\n",
      "Epoch 144/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0836 - accuracy: 0.9784 - val_loss: 0.1346 - val_accuracy: 0.9333\n",
      "Epoch 145/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0797 - accuracy: 0.9658 - val_loss: 0.1333 - val_accuracy: 0.9333\n",
      "Epoch 146/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0951 - accuracy: 0.9464 - val_loss: 0.1309 - val_accuracy: 0.9333\n",
      "Epoch 147/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1071 - accuracy: 0.9459 - val_loss: 0.1328 - val_accuracy: 0.9333\n",
      "Epoch 148/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1100 - accuracy: 0.9488 - val_loss: 0.1297 - val_accuracy: 0.9333\n",
      "Epoch 149/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0778 - accuracy: 0.9821 - val_loss: 0.1269 - val_accuracy: 0.9333\n",
      "Epoch 150/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0857 - accuracy: 0.9576 - val_loss: 0.1296 - val_accuracy: 0.9333\n",
      "Epoch 151/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0720 - accuracy: 0.9738 - val_loss: 0.1316 - val_accuracy: 0.9333\n",
      "Epoch 152/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0801 - accuracy: 0.9702 - val_loss: 0.1271 - val_accuracy: 0.9333\n",
      "Epoch 153/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1048 - accuracy: 0.9423 - val_loss: 0.1290 - val_accuracy: 0.9333\n",
      "Epoch 154/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0838 - accuracy: 0.9676 - val_loss: 0.1317 - val_accuracy: 0.9333\n",
      "Epoch 155/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0952 - accuracy: 0.9565 - val_loss: 0.1287 - val_accuracy: 0.9333\n",
      "Epoch 156/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0994 - accuracy: 0.9344 - val_loss: 0.1271 - val_accuracy: 0.9333\n",
      "Epoch 157/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0727 - accuracy: 0.9795 - val_loss: 0.1282 - val_accuracy: 0.9333\n",
      "Epoch 158/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0553 - accuracy: 0.9883 - val_loss: 0.1340 - val_accuracy: 0.9333\n",
      "Epoch 159/700\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0679 - accuracy: 0.9716 - val_loss: 0.1255 - val_accuracy: 0.9333\n",
      "Epoch 160/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0592 - accuracy: 0.9880 - val_loss: 0.1214 - val_accuracy: 0.9333\n",
      "Epoch 161/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0847 - accuracy: 0.9666 - val_loss: 0.1262 - val_accuracy: 0.9333\n",
      "Epoch 162/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0666 - accuracy: 0.9710 - val_loss: 0.1264 - val_accuracy: 0.9333\n",
      "Epoch 163/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0430 - accuracy: 0.9945 - val_loss: 0.1329 - val_accuracy: 0.9333\n",
      "Epoch 164/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0885 - accuracy: 0.9465 - val_loss: 0.1266 - val_accuracy: 0.9333\n",
      "Epoch 165/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0789 - accuracy: 0.9750 - val_loss: 0.1236 - val_accuracy: 0.9333\n",
      "Epoch 166/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0665 - accuracy: 0.9793 - val_loss: 0.1262 - val_accuracy: 0.9333\n",
      "Epoch 167/700\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0802 - accuracy: 0.9654 - val_loss: 0.1229 - val_accuracy: 0.9333\n",
      "Epoch 168/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0771 - accuracy: 0.9641 - val_loss: 0.1279 - val_accuracy: 0.9333\n",
      "Epoch 169/700\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0894 - accuracy: 0.9607 - val_loss: 0.1263 - val_accuracy: 0.9333\n",
      "Epoch 170/700\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0532 - accuracy: 0.9776 - val_loss: 0.1232 - val_accuracy: 0.9333\n",
      "Epoch 00170: early stopping\n",
      "\n",
      "Duration :  00:00:34 472ms\n"
     ]
    }
   ],
   "source": [
    "vID.chrono_start()\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "print(f\"x_train. Structure (shape) : {x_train.shape}\")\n",
    "print(f\"x_test. Structure (shape) : {x_test.shape}\")\n",
    "print(f\"y_train_ohe. Structure (shape) : {y_train_ohe.shape}\")\n",
    "print(f\"y_test_ohe. Structure (shape) : {y_test_ohe.shape}\")\n",
    "ANNmodel=get_model( (4,))\n",
    "ANNmodel.summary()\n",
    "vID.chrono_start()\n",
    "ANNhistory = ANNmodel.fit(x_train,\n",
    "                    y_train_ohe,\n",
    "                    epochs          = 700,\n",
    "                    batch_size      = 5,\n",
    "                    verbose         = 1,\n",
    "                    validation_data = (x_test, y_test_ohe),\n",
    "                    callbacks=[es])\n",
    "vID.chrono_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916c59d-3cc8-466f-b776-ec09e2038f9a",
   "metadata": {},
   "source": [
    "### 3.4. Évaluation de la précision du réseau de neurones après apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f927a6-ce28-443a-b4f9-0c47140e51ef",
   "metadata": {},
   "source": [
    "#### 3.4.a. Évaluation numérique globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe7d2f5f-14a3-4484-9cc6-69a080e0212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mx_train / loss      : 0.0709\u001b[0m\n",
      "\u001b[92mx_train/ accurracy  : 0.9750\u001b[0m\n",
      "\n",
      "\u001b[94mx_train / loss      : 0.1232\u001b[0m\n",
      "\u001b[94mx_train/ accurracy  : 0.9333\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "evalANN_on_Train = ANNmodel.evaluate(x_train, y_train_ohe, verbose=0)\n",
    "print(f\"{color.GREEN}x_train / loss      : {evalANN_on_Train[0]:5.4f}{color.OFF}\")\n",
    "print(f\"{color.GREEN}x_train/ accurracy  : {evalANN_on_Train[1]:5.4f}{color.OFF}\")\n",
    "print()\n",
    "evalANN_on_Test = ANNmodel.evaluate(x_test, y_test_ohe, verbose=0)\n",
    "print(f\"{color.BLUE}x_train / loss      : {evalANN_on_Test[0]:5.4f}{color.OFF}\")\n",
    "print(f\"{color.BLUE}x_train/ accurracy  : {evalANN_on_Test[1]:5.4f}{color.OFF}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149669f-0ee6-4f8f-9f40-1cb91e4929ad",
   "metadata": {},
   "source": [
    "#### 3.4.b. Comportement du modèle vis-à-vis de chaque espèce d'iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35c740a5-53e5-4776-8cea-b53af7589366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[92mCatégories uniques d'iris :\u001b[0m ['setosa' 'versicolor' 'virginica']\n",
      "\u001b[1m\u001b[94mTest Set\u001b[0m. Comparaison entre espèce prédite et espèce observée\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "      <th>Espèce prédite</th>\n",
       "      <th>Espèce observée</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.21</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.08</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.12</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.12</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.91</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.92</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.09</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.83</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.81</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.73</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.06</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "      <td>virginica</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.47</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.91</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.93</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.94</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.08</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.09</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>virginica</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.82</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.47</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.68</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     setosa  versicolor  virginica Espèce prédite Espèce observée\n",
       "46     1.00        0.00       0.00         setosa          setosa\n",
       "38     1.00        0.00       0.00         setosa          setosa\n",
       "114    0.00        0.00       1.00      virginica       virginica\n",
       "76     0.00        0.85       0.15     versicolor      versicolor\n",
       "8      1.00        0.00       0.00         setosa          setosa\n",
       "54     0.00        0.79       0.21     versicolor      versicolor\n",
       "66     0.00        0.92       0.08     versicolor      versicolor\n",
       "35     1.00        0.00       0.00         setosa          setosa\n",
       "20     1.00        0.00       0.00         setosa          setosa\n",
       "43     1.00        0.00       0.00         setosa          setosa\n",
       "136    0.00        0.00       1.00      virginica       virginica\n",
       "144    0.00        0.00       1.00      virginica       virginica\n",
       "80     0.00        0.96       0.04     versicolor      versicolor\n",
       "148    0.00        0.01       0.99      virginica       virginica\n",
       "29     1.00        0.00       0.00         setosa          setosa\n",
       "45     1.00        0.00       0.00         setosa          setosa\n",
       "111    0.00        0.02       0.98      virginica       virginica\n",
       "98     0.00        0.96       0.04     versicolor      versicolor\n",
       "142    0.00        0.02       0.98      virginica       virginica\n",
       "25     1.00        0.00       0.00         setosa          setosa\n",
       "103    0.00        0.04       0.96      virginica       virginica\n",
       "52     0.00        0.88       0.12     versicolor      versicolor\n",
       "60     0.00        0.95       0.05     versicolor      versicolor\n",
       "141    0.00        0.01       0.99      virginica       virginica\n",
       "117    0.00        0.01       0.99      virginica       virginica\n",
       "64     0.01        0.95       0.05     versicolor      versicolor\n",
       "65     0.01        0.95       0.05     versicolor      versicolor\n",
       "9      1.00        0.00       0.00         setosa          setosa\n",
       "19     1.00        0.00       0.00         setosa          setosa\n",
       "78     0.00        0.88       0.12     versicolor      versicolor\n",
       "93     0.00        0.96       0.04     versicolor      versicolor\n",
       "94     0.00        0.95       0.05     versicolor      versicolor\n",
       "99     0.00        0.95       0.04     versicolor      versicolor\n",
       "119    0.00        0.09       0.91      virginica       virginica\n",
       "116    0.00        0.08       0.92      virginica       virginica\n",
       "96     0.00        0.95       0.05     versicolor      versicolor\n",
       "85     0.00        0.95       0.05     versicolor      versicolor\n",
       "63     0.00        0.91       0.09     versicolor      versicolor\n",
       "120    0.00        0.00       1.00      virginica       virginica\n",
       "33     1.00        0.00       0.00         setosa          setosa\n",
       "90     0.00        0.95       0.05     versicolor      versicolor\n",
       "147    0.00        0.03       0.97      virginica       virginica\n",
       "27     1.00        0.00       0.00         setosa          setosa\n",
       "57     0.00        0.96       0.04     versicolor      versicolor\n",
       "6      1.00        0.00       0.00         setosa          setosa\n",
       "95     0.03        0.92       0.05     versicolor      versicolor\n",
       "106    0.00        0.17       0.83      virginica       virginica\n",
       "129    0.00        0.19       0.81      virginica       virginica\n",
       "62     0.00        0.95       0.05     versicolor      versicolor\n",
       "139    0.00        0.01       0.99      virginica       virginica\n",
       "32     1.00        0.00       0.00         setosa          setosa\n",
       "44     1.00        0.00       0.00         setosa          setosa\n",
       "13     1.00        0.00       0.00         setosa          setosa\n",
       "127    0.00        0.27       0.73      virginica       virginica\n",
       "28     1.00        0.00       0.00         setosa          setosa\n",
       "89     0.00        0.94       0.06     versicolor      versicolor\n",
       "67     0.02        0.93       0.05     versicolor      versicolor\n",
       "34     1.00        0.00       0.00         setosa          setosa\n",
       "77     0.00        0.40       0.60      virginica      versicolor\n",
       "16     1.00        0.00       0.00         setosa          setosa\n",
       "108    0.00        0.00       1.00      virginica       virginica\n",
       "140    0.00        0.00       1.00      virginica       virginica\n",
       "59     0.00        0.95       0.05     versicolor      versicolor\n",
       "101    0.00        0.02       0.98      virginica       virginica\n",
       "135    0.00        0.00       1.00      virginica       virginica\n",
       "41     0.98        0.00       0.02         setosa          setosa\n",
       "70     0.00        0.53       0.47     versicolor      versicolor\n",
       "73     0.00        0.95       0.04     versicolor      versicolor\n",
       "79     0.01        0.94       0.05     versicolor      versicolor\n",
       "124    0.00        0.02       0.98      virginica       virginica\n",
       "123    0.00        0.09       0.91      virginica       virginica\n",
       "69     0.00        0.95       0.04     versicolor      versicolor\n",
       "115    0.00        0.01       0.99      virginica       virginica\n",
       "3      1.00        0.00       0.00         setosa          setosa\n",
       "102    0.00        0.00       1.00      virginica       virginica\n",
       "2      1.00        0.00       0.00         setosa          setosa\n",
       "48     1.00        0.00       0.00         setosa          setosa\n",
       "131    0.00        0.07       0.93      virginica       virginica\n",
       "0      1.00        0.00       0.00         setosa          setosa\n",
       "58     0.00        0.95       0.04     versicolor      versicolor\n",
       "125    0.00        0.06       0.94      virginica       virginica\n",
       "49     1.00        0.00       0.00         setosa          setosa\n",
       "128    0.00        0.00       1.00      virginica       virginica\n",
       "109    0.00        0.00       1.00      virginica       virginica\n",
       "86     0.00        0.92       0.08     versicolor      versicolor\n",
       "17     1.00        0.00       0.00         setosa          setosa\n",
       "84     0.00        0.91       0.09     versicolor      versicolor\n",
       "121    0.00        0.03       0.97      virginica       virginica\n",
       "91     0.00        0.95       0.05     versicolor      versicolor\n",
       "55     0.00        0.95       0.05     versicolor      versicolor\n",
       "83     0.00        0.24       0.76      virginica      versicolor\n",
       "132    0.00        0.00       1.00      virginica       virginica\n",
       "88     0.01        0.94       0.05     versicolor      versicolor\n",
       "146    0.00        0.01       0.99      virginica       virginica\n",
       "149    0.00        0.18       0.82      virginica       virginica\n",
       "39     1.00        0.00       0.00         setosa          setosa\n",
       "10     1.00        0.00       0.00         setosa          setosa\n",
       "14     1.00        0.00       0.00         setosa          setosa\n",
       "134    0.00        0.24       0.76      virginica       virginica\n",
       "4      1.00        0.00       0.00         setosa          setosa\n",
       "145    0.00        0.00       1.00      virginica       virginica\n",
       "71     0.00        0.95       0.05     versicolor      versicolor\n",
       "118    0.00        0.00       1.00      virginica       virginica\n",
       "1      1.00        0.00       0.00         setosa          setosa\n",
       "74     0.00        0.95       0.05     versicolor      versicolor\n",
       "133    0.00        0.53       0.47     versicolor       virginica\n",
       "92     0.00        0.95       0.04     versicolor      versicolor\n",
       "5      1.00        0.00       0.00         setosa          setosa\n",
       "47     1.00        0.00       0.00         setosa          setosa\n",
       "31     1.00        0.00       0.00         setosa          setosa\n",
       "138    0.00        0.32       0.68      virginica       virginica\n",
       "24     1.00        0.00       0.00         setosa          setosa\n",
       "105    0.00        0.00       1.00      virginica       virginica\n",
       "18     1.00        0.00       0.00         setosa          setosa\n",
       "12     1.00        0.00       0.00         setosa          setosa\n",
       "107    0.00        0.01       0.99      virginica       virginica\n",
       "97     0.00        0.95       0.05     versicolor      versicolor\n",
       "23     1.00        0.00       0.00         setosa          setosa\n",
       "36     1.00        0.00       0.00         setosa          setosa\n",
       "104    0.00        0.00       1.00      virginica       virginica"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'erreurs : 3\n",
      "\n",
      "\u001b[1m\u001b[91mTest Set\u001b[0m. Comparaison entre espèce prédite et espèce observée.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "      <th>Espèce prédite</th>\n",
       "      <th>Espèce observée</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.10</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.08</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>virginica</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.68</td>\n",
       "      <td>virginica</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.22</td>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.90</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.84</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     setosa  versicolor  virginica Espèce prédite Espèce observée\n",
       "7      1.00        0.00       0.00         setosa          setosa\n",
       "11     1.00        0.00       0.00         setosa          setosa\n",
       "15     1.00        0.00       0.00         setosa          setosa\n",
       "21     1.00        0.00       0.00         setosa          setosa\n",
       "22     1.00        0.00       0.00         setosa          setosa\n",
       "26     1.00        0.00       0.00         setosa          setosa\n",
       "30     1.00        0.00       0.00         setosa          setosa\n",
       "37     1.00        0.00       0.00         setosa          setosa\n",
       "40     1.00        0.00       0.00         setosa          setosa\n",
       "42     1.00        0.00       0.00         setosa          setosa\n",
       "50     0.01        0.95       0.05     versicolor      versicolor\n",
       "51     0.00        0.95       0.04     versicolor      versicolor\n",
       "53     0.00        0.90       0.10     versicolor      versicolor\n",
       "56     0.00        0.92       0.08     versicolor      versicolor\n",
       "61     0.00        0.95       0.05     versicolor      versicolor\n",
       "68     0.00        0.25       0.75      virginica      versicolor\n",
       "72     0.00        0.32       0.68      virginica      versicolor\n",
       "75     0.00        0.95       0.04     versicolor      versicolor\n",
       "81     0.00        0.95       0.04     versicolor      versicolor\n",
       "82     0.00        0.95       0.05     versicolor      versicolor\n",
       "87     0.00        0.78       0.22     versicolor      versicolor\n",
       "100    0.00        0.00       1.00      virginica       virginica\n",
       "110    0.00        0.10       0.90      virginica       virginica\n",
       "112    0.00        0.01       0.99      virginica       virginica\n",
       "113    0.00        0.01       0.99      virginica       virginica\n",
       "122    0.00        0.00       1.00      virginica       virginica\n",
       "126    0.00        0.16       0.84      virginica       virginica\n",
       "130    0.00        0.00       1.00      virginica       virginica\n",
       "137    0.00        0.12       0.88      virginica       virginica\n",
       "143    0.00        0.00       1.00      virginica       virginica"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'erreurs : 2\n"
     ]
    }
   ],
   "source": [
    "usp = dfi['species'].unique()\n",
    "print(f\"{color.BOLD}{color.GREEN}Catégories uniques d'iris :{color.OFF} {usp}\")\n",
    "# cette correspondance élément 0 <-> setosa ; élément 1 <-> versicolor ; élément 2 <-> virginica\n",
    "# va servir à transformer les probabilités les plus élevées en espèce d'iris\n",
    "\n",
    "y_train_hat=ANNmodel.predict(x_train)\n",
    "ytr_hD = pd.DataFrame(y_train_hat, columns=usp, index=y_train.index)\n",
    "# argmax renvoie l'indice de la valeur maximale selon un axe (0 = ligne / 1 = colonne)\n",
    "# ça va ici nous renvoyer pour chaque ligne le numéro de la colonne qui contient la plus grande valeur de probabilité\n",
    "# on convertit maintenant 0, 1 & 2 en espèce d'iris\n",
    "iris_tr_hat = usp[np.argmax(y_train_hat,axis=1)]\n",
    "# on ajoute cette colonne au dataframe ythd\n",
    "ytr_hD['Espèce prédite'] = pd.DataFrame(iris_tr_hat, index=y_train.index)\n",
    "ytr_hD['Espèce observée'] = pd.DataFrame(y_train, index=y_train.index)\n",
    "print(f\"{color.BOLD}{color.BLUE}Test Set{color.OFF}. Comparaison entre espèce prédite et espèce observée\")\n",
    "pd.set_option('display.max_rows', None) #on veut afficher toutes les lignes des DataFrames\n",
    "pd.options.display.float_format = '{:,.2f}'.format #on formatte les probabilités, sinon c'est illisible\n",
    "display(ytr_hD)\n",
    "pd.set_option('display.max_rows', 10) #reset de l'affichage: 5 premières et 5 dernières lignes\n",
    "pd.options.display.float_format = None #reset de l'affichage: format standard \n",
    "diff_Pred_Obs=np.where(ytr_hD['Espèce prédite'] == ytr_hD['Espèce observée'], 0, 1)\n",
    "print(f\"Nombre d'erreurs : {np.sum(diff_Pred_Obs)}\")\n",
    "\n",
    "print()\n",
    "y_test_hat=ANNmodel.predict(x_test)\n",
    "ytt_hD = pd.DataFrame(y_test_hat, columns=usp, index=y_test.index)\n",
    "# argmax renvoie l'indice de la valeur maximale selon un axe (0 = ligne / 1 = colonne)\n",
    "# ça va ici nous renvoyer pour chaque ligne le numéro de la colonne qui contient la plus grande valeur de probabilité\n",
    "# on convertit maintenant 0, 1 & 2 en espèce d'iris\n",
    "iris_tt_hat = usp[np.argmax(y_test_hat,axis=1)]\n",
    "# on ajoute cette colonne au dataframe ythd\n",
    "ytt_hD['Espèce prédite'] = pd.DataFrame(iris_tt_hat, index=y_test.index)\n",
    "ytt_hD['Espèce observée'] = pd.DataFrame(y_test, index=y_test.index)\n",
    "print(f\"{color.BOLD}{color.RED}Test Set{color.OFF}. Comparaison entre espèce prédite et espèce observée.\")\n",
    "pd.set_option('display.max_rows', None) #on veut afficher toutes les lignes des DataFrames\n",
    "pd.options.display.float_format = '{:,.2f}'.format #on formatte les probabilités, sinon c'est illisible\n",
    "display(ytt_hD)\n",
    "pd.set_option('display.max_rows', 10) #reset de l'affichage: 5 premières et 5 dernières lignes\n",
    "pd.options.display.float_format = None #reset de l'affichage: \n",
    "diff_Pred_Obs=np.where(ytt_hD['Espèce prédite'] == ytt_hD['Espèce observée'], 0, 1)\n",
    "print(f\"Nombre d'erreurs : {np.sum(diff_Pred_Obs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55fe30e-ce3d-49bf-b488-6fe9bff3291b",
   "metadata": {},
   "source": [
    "#### 3.4.c. Bilan de la performance du modèle prédictif sous forme de matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "866bd474-9393-4311-b165-0f66d021e221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set. Matrice de confusion\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set. Matrice de confusion\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#print(np.argmax(y_train_ohe,axis=1))\n",
    "#print(np.argmax(y_train_hat,axis=1))\n",
    "cm_labels = dfi['species'].unique()\n",
    "\n",
    "print(\"Training set. Matrice de confusion\")\n",
    "cm_tr = confusion_matrix(np.argmax(y_train_ohe,axis=1), np.argmax(y_train_hat,axis=1))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_tr, display_labels=cm_labels)\n",
    "disp.plot(cmap=plt.cm.Blues) #ici cm = diminutif de colormap dans matplotlib\n",
    "plt.show()\n",
    "print(\"Test set. Matrice de confusion\")\n",
    "cm_tt = confusion_matrix(np.argmax(y_test_ohe,axis=1), np.argmax(y_test_hat,axis=1))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_tt, display_labels=cm_labels)\n",
    "disp.plot(cmap=plt.cm.Blues) #ici cm = diminutif de colormap dans matplotlib\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f1edc9-8140-4039-9755-35e9a09d277d",
   "metadata": {},
   "source": [
    "### Bilan de la troisième partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af354bcb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Fin à:** Wednesday 25 May 2022, 14:56:58  \n",
       "**Durée:** 00:00:50 914ms"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"./svg/logoEnd.svg\" style=\"margin-left:auto; margin-right:auto\"></img></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vID.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fad06d-6413-4793-b945-a7d707839fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
