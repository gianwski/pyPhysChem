{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10024ca9-7e94-4f5c-a982-b4537b919d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: bold;\n",
       "}\n",
       "body {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: 200;\n",
       "}\n",
       "h1 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 30px ;\n",
       "  color: white;\n",
       "  background: #b11d01;\n",
       "  text-align: center;\n",
       "}\n",
       "h2 {\n",
       "  border: 3px solid #333;\n",
       "  padding: 18px ;\n",
       "  color: #b11d01;\n",
       "  background: #ffffff;\n",
       "  text-align: center;\n",
       "}\n",
       "h3 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 12px ;\n",
       "  color: #000000;\n",
       "  background: #c1c1c1;\n",
       "  text-align: left;\n",
       "}\n",
       "h4 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #d9fffc;\n",
       "  text-align: left;\n",
       "}\n",
       "h5 {\n",
       "  border: 1px solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #ffffff;\n",
       "  text-align: left;\n",
       "}\n",
       ".warn {    \n",
       "    background-color: #fcf2f2;\n",
       "    border-color: #dFb5b4;\n",
       "    border-left: 5px solid #dfb5b4;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       ".rq {    \n",
       "    background-color: #e2e2e2;\n",
       "    border-color: #969696;\n",
       "    border-left: 5px solid #969696;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Start at:** Thursday 30 June 2022, 12:30:03  \n",
       "**Hostname:** localhost.localdomain (Linux)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"./svg/logoPytChem.svg\" style=\"margin-left:auto; margin-right:auto\"/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import visualID_Eng as vID\n",
    "vID.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e4b9b",
   "metadata": {},
   "source": [
    "# Prediction by an artificial neural network of the solubility of CO<sub>2</sub> in ionic liquids\n",
    "\n",
    "<div class=\"rq\">\n",
    "<b>Reference</b>: \n",
    "Z. Song, H. Shi, X. Zhang & T. Zhou (**2020**), Prediction of CO<sub>2</sub> solubility in ionic liquids using machine learning methods, [<i>Chem. Eng. Sci.</i> <b>223</b>: 115752](https://www.doi.org/10.1016/j.ces.2020.115752) \n",
    "<br>\n",
    "<p style=\"text-align: center\"><img width=\"650px\" src=\"./CO2-images/AbstractANNCO2-SongEtal.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_AbstractSong\"></p>\n",
    "<br>\n",
    "The main results are graphically reported below.\n",
    "<br>\n",
    "<p style=\"text-align: center\"><img width=\"900px\" src=\"./CO2-images/ANNCO2-SongEtal-Results.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_ResultsSong\"></p>\n",
    "<br>\n",
    "Yet, it seems sthat no standardization process of the data has been applied. \n",
    "    \n",
    "<span style=\"color:red\">Moreover, a spurious separation of the data between training and test sets has been applied: \"<i>Instead of performing random selection, we employ a hybrid artificial-random strategy to decompose the dataset. Specifically, the data points consisting of the least frequently used groups are equally divided into five folders\"</i></span> \n",
    "<br><br>\n",
    "<b>It raises doubts about the stability of the algorithm developped in this paper (*unless the authors forgot to mention that data were standardized*).</b>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84860c4-3fcf-45f5-80a3-607ead68da80",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "<span style=\"font-weight:bold\">The goal of this exercise is to apply the <i>K</i>-fold cross-validation the ANN part of this article, <i>i.e.</i> without standardized data. </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af39c092-f4b7-460b-b698-32e0f2c55461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 12:30:06.185760: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-30 12:30:06.185808: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os,sys\n",
    "from IPython.display import display\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   OFF = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89a89d-fb12-4ba1-8382-1609a0c4f848",
   "metadata": {},
   "source": [
    "<a id=\"data-read\"></a>\n",
    "## **1.** Database reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7d2251b-1f22-42f5-b930-e6032fe55170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IL</th>\n",
       "      <th>cation</th>\n",
       "      <th>anion</th>\n",
       "      <th>x_CO2</th>\n",
       "      <th>T (K)</th>\n",
       "      <th>P (bar)</th>\n",
       "      <th>[CH3]</th>\n",
       "      <th>[CH2]</th>\n",
       "      <th>[CH]</th>\n",
       "      <th>[OCH2]</th>\n",
       "      <th>...</th>\n",
       "      <th>[MeSO3]</th>\n",
       "      <th>[TfO]</th>\n",
       "      <th>[NfO]</th>\n",
       "      <th>[TDfO]</th>\n",
       "      <th>[TOS]</th>\n",
       "      <th>[C12PhSO3]</th>\n",
       "      <th>[DMPO4]</th>\n",
       "      <th>[DEPO4]</th>\n",
       "      <th>[DBPO4]</th>\n",
       "      <th>[methide]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>363.15</td>\n",
       "      <td>246.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.500</td>\n",
       "      <td>383.15</td>\n",
       "      <td>235.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>353.15</td>\n",
       "      <td>223.30</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.500</td>\n",
       "      <td>373.15</td>\n",
       "      <td>198.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>343.15</td>\n",
       "      <td>188.50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10111</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.592</td>\n",
       "      <td>298.15</td>\n",
       "      <td>35.86</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10112</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.239</td>\n",
       "      <td>343.15</td>\n",
       "      <td>27.54</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10113</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.396</td>\n",
       "      <td>298.15</td>\n",
       "      <td>20.15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10114</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.140</td>\n",
       "      <td>343.15</td>\n",
       "      <td>17.93</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10115</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.139</td>\n",
       "      <td>323.15</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10116 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 IL  cation   anion  x_CO2   T (K)  P (bar)  [CH3]  [CH2]  \\\n",
       "0       [BMIM][BF4]  [BMIM]   [BF4]  0.610  363.15   246.00      1      3   \n",
       "1       [BMIM][BF4]  [BMIM]   [BF4]  0.500  383.15   235.00      1      3   \n",
       "2       [BMIM][BF4]  [BMIM]   [BF4]  0.610  353.15   223.30      1      3   \n",
       "3       [BMIM][BF4]  [BMIM]   [BF4]  0.500  373.15   198.00      1      3   \n",
       "4       [BMIM][BF4]  [BMIM]   [BF4]  0.610  343.15   188.50      1      3   \n",
       "...             ...     ...     ...    ...     ...      ...    ...    ...   \n",
       "10111  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.592  298.15    35.86      1      5   \n",
       "10112  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.239  343.15    27.54      1      5   \n",
       "10113  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.396  298.15    20.15      1      5   \n",
       "10114  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.140  343.15    17.93      1      5   \n",
       "10115  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.139  323.15     8.00      1      5   \n",
       "\n",
       "       [CH]  [OCH2]  ...  [MeSO3]  [TfO]  [NfO]  [TDfO]  [TOS]  [C12PhSO3]  \\\n",
       "0         0       0  ...        0      0      0       0      0           0   \n",
       "1         0       0  ...        0      0      0       0      0           0   \n",
       "2         0       0  ...        0      0      0       0      0           0   \n",
       "3         0       0  ...        0      0      0       0      0           0   \n",
       "4         0       0  ...        0      0      0       0      0           0   \n",
       "...     ...     ...  ...      ...    ...    ...     ...    ...         ...   \n",
       "10111     0       0  ...        0      0      0       0      0           0   \n",
       "10112     0       0  ...        0      0      0       0      0           0   \n",
       "10113     0       0  ...        0      0      0       0      0           0   \n",
       "10114     0       0  ...        0      0      0       0      0           0   \n",
       "10115     0       0  ...        0      0      0       0      0           0   \n",
       "\n",
       "       [DMPO4]  [DEPO4]  [DBPO4]  [methide]  \n",
       "0            0        0        0          0  \n",
       "1            0        0        0          0  \n",
       "2            0        0        0          0  \n",
       "3            0        0        0          0  \n",
       "4            0        0        0          0  \n",
       "...        ...      ...      ...        ...  \n",
       "10111        0        0        0          0  \n",
       "10112        0        0        0          0  \n",
       "10113        0        0        0          0  \n",
       "10114        0        0        0          0  \n",
       "10115        0        0        0          0  \n",
       "\n",
       "[10116 rows x 57 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_16f7d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_16f7d_level0_col0\" class=\"col_heading level0 col0\" >x_CO2</th>\n",
       "      <th id=\"T_16f7d_level0_col1\" class=\"col_heading level0 col1\" >T (K)</th>\n",
       "      <th id=\"T_16f7d_level0_col2\" class=\"col_heading level0 col2\" >P (bar)</th>\n",
       "      <th id=\"T_16f7d_level0_col3\" class=\"col_heading level0 col3\" >[CH3]</th>\n",
       "      <th id=\"T_16f7d_level0_col4\" class=\"col_heading level0 col4\" >[CH2]</th>\n",
       "      <th id=\"T_16f7d_level0_col5\" class=\"col_heading level0 col5\" >[CH]</th>\n",
       "      <th id=\"T_16f7d_level0_col6\" class=\"col_heading level0 col6\" >[OCH2]</th>\n",
       "      <th id=\"T_16f7d_level0_col7\" class=\"col_heading level0 col7\" >[OCH3]</th>\n",
       "      <th id=\"T_16f7d_level0_col8\" class=\"col_heading level0 col8\" >[CF2]</th>\n",
       "      <th id=\"T_16f7d_level0_col9\" class=\"col_heading level0 col9\" >[CF3]</th>\n",
       "      <th id=\"T_16f7d_level0_col10\" class=\"col_heading level0 col10\" >[OH]</th>\n",
       "      <th id=\"T_16f7d_level0_col11\" class=\"col_heading level0 col11\" >CH=CH</th>\n",
       "      <th id=\"T_16f7d_level0_col12\" class=\"col_heading level0 col12\" >CH=CH2</th>\n",
       "      <th id=\"T_16f7d_level0_col13\" class=\"col_heading level0 col13\" >[Im13]</th>\n",
       "      <th id=\"T_16f7d_level0_col14\" class=\"col_heading level0 col14\" >[MIm]</th>\n",
       "      <th id=\"T_16f7d_level0_col15\" class=\"col_heading level0 col15\" >[MMIM]</th>\n",
       "      <th id=\"T_16f7d_level0_col16\" class=\"col_heading level0 col16\" >[Py]</th>\n",
       "      <th id=\"T_16f7d_level0_col17\" class=\"col_heading level0 col17\" >[MPy]</th>\n",
       "      <th id=\"T_16f7d_level0_col18\" class=\"col_heading level0 col18\" >[MPyrro]</th>\n",
       "      <th id=\"T_16f7d_level0_col19\" class=\"col_heading level0 col19\" >[MPip]</th>\n",
       "      <th id=\"T_16f7d_level0_col20\" class=\"col_heading level0 col20\" >[NH3]</th>\n",
       "      <th id=\"T_16f7d_level0_col21\" class=\"col_heading level0 col21\" >[NH2]</th>\n",
       "      <th id=\"T_16f7d_level0_col22\" class=\"col_heading level0 col22\" >[NH]</th>\n",
       "      <th id=\"T_16f7d_level0_col23\" class=\"col_heading level0 col23\" >[N]</th>\n",
       "      <th id=\"T_16f7d_level0_col24\" class=\"col_heading level0 col24\" >[P]</th>\n",
       "      <th id=\"T_16f7d_level0_col25\" class=\"col_heading level0 col25\" >[S]</th>\n",
       "      <th id=\"T_16f7d_level0_col26\" class=\"col_heading level0 col26\" >[BF4]</th>\n",
       "      <th id=\"T_16f7d_level0_col27\" class=\"col_heading level0 col27\" >[Cl]</th>\n",
       "      <th id=\"T_16f7d_level0_col28\" class=\"col_heading level0 col28\" >[DCA]</th>\n",
       "      <th id=\"T_16f7d_level0_col29\" class=\"col_heading level0 col29\" >[NO3]</th>\n",
       "      <th id=\"T_16f7d_level0_col30\" class=\"col_heading level0 col30\" >[PF6]</th>\n",
       "      <th id=\"T_16f7d_level0_col31\" class=\"col_heading level0 col31\" >[SCN]</th>\n",
       "      <th id=\"T_16f7d_level0_col32\" class=\"col_heading level0 col32\" >[TCB]</th>\n",
       "      <th id=\"T_16f7d_level0_col33\" class=\"col_heading level0 col33\" >[C(CN)3]</th>\n",
       "      <th id=\"T_16f7d_level0_col34\" class=\"col_heading level0 col34\" >[HSO4]</th>\n",
       "      <th id=\"T_16f7d_level0_col35\" class=\"col_heading level0 col35\" >[FSA]</th>\n",
       "      <th id=\"T_16f7d_level0_col36\" class=\"col_heading level0 col36\" >[Tf2N]</th>\n",
       "      <th id=\"T_16f7d_level0_col37\" class=\"col_heading level0 col37\" >[BETA]</th>\n",
       "      <th id=\"T_16f7d_level0_col38\" class=\"col_heading level0 col38\" >[FOR]</th>\n",
       "      <th id=\"T_16f7d_level0_col39\" class=\"col_heading level0 col39\" >[TFA]</th>\n",
       "      <th id=\"T_16f7d_level0_col40\" class=\"col_heading level0 col40\" >[C3F7CO2]</th>\n",
       "      <th id=\"T_16f7d_level0_col41\" class=\"col_heading level0 col41\" >[MeSO4]</th>\n",
       "      <th id=\"T_16f7d_level0_col42\" class=\"col_heading level0 col42\" >[EtSO4]</th>\n",
       "      <th id=\"T_16f7d_level0_col43\" class=\"col_heading level0 col43\" >[MDEGSO4]</th>\n",
       "      <th id=\"T_16f7d_level0_col44\" class=\"col_heading level0 col44\" >[MeSO3]</th>\n",
       "      <th id=\"T_16f7d_level0_col45\" class=\"col_heading level0 col45\" >[TfO]</th>\n",
       "      <th id=\"T_16f7d_level0_col46\" class=\"col_heading level0 col46\" >[NfO]</th>\n",
       "      <th id=\"T_16f7d_level0_col47\" class=\"col_heading level0 col47\" >[TDfO]</th>\n",
       "      <th id=\"T_16f7d_level0_col48\" class=\"col_heading level0 col48\" >[TOS]</th>\n",
       "      <th id=\"T_16f7d_level0_col49\" class=\"col_heading level0 col49\" >[C12PhSO3]</th>\n",
       "      <th id=\"T_16f7d_level0_col50\" class=\"col_heading level0 col50\" >[DMPO4]</th>\n",
       "      <th id=\"T_16f7d_level0_col51\" class=\"col_heading level0 col51\" >[DEPO4]</th>\n",
       "      <th id=\"T_16f7d_level0_col52\" class=\"col_heading level0 col52\" >[DBPO4]</th>\n",
       "      <th id=\"T_16f7d_level0_col53\" class=\"col_heading level0 col53\" >[methide]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_16f7d_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_16f7d_row0_col0\" class=\"data row0 col0\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col1\" class=\"data row0 col1\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col2\" class=\"data row0 col2\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col3\" class=\"data row0 col3\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col4\" class=\"data row0 col4\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col5\" class=\"data row0 col5\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col6\" class=\"data row0 col6\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col7\" class=\"data row0 col7\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col8\" class=\"data row0 col8\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col9\" class=\"data row0 col9\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col10\" class=\"data row0 col10\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col11\" class=\"data row0 col11\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col12\" class=\"data row0 col12\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col13\" class=\"data row0 col13\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col14\" class=\"data row0 col14\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col15\" class=\"data row0 col15\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col16\" class=\"data row0 col16\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col17\" class=\"data row0 col17\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col18\" class=\"data row0 col18\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col19\" class=\"data row0 col19\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col20\" class=\"data row0 col20\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col21\" class=\"data row0 col21\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col22\" class=\"data row0 col22\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col23\" class=\"data row0 col23\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col24\" class=\"data row0 col24\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col25\" class=\"data row0 col25\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col26\" class=\"data row0 col26\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col27\" class=\"data row0 col27\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col28\" class=\"data row0 col28\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col29\" class=\"data row0 col29\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col30\" class=\"data row0 col30\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col31\" class=\"data row0 col31\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col32\" class=\"data row0 col32\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col33\" class=\"data row0 col33\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col34\" class=\"data row0 col34\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col35\" class=\"data row0 col35\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col36\" class=\"data row0 col36\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col37\" class=\"data row0 col37\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col38\" class=\"data row0 col38\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col39\" class=\"data row0 col39\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col40\" class=\"data row0 col40\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col41\" class=\"data row0 col41\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col42\" class=\"data row0 col42\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col43\" class=\"data row0 col43\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col44\" class=\"data row0 col44\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col45\" class=\"data row0 col45\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col46\" class=\"data row0 col46\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col47\" class=\"data row0 col47\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col48\" class=\"data row0 col48\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col49\" class=\"data row0 col49\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col50\" class=\"data row0 col50\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col51\" class=\"data row0 col51\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col52\" class=\"data row0 col52\" >10116.00</td>\n",
       "      <td id=\"T_16f7d_row0_col53\" class=\"data row0 col53\" >10116.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16f7d_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_16f7d_row1_col0\" class=\"data row1 col0\" >0.33</td>\n",
       "      <td id=\"T_16f7d_row1_col1\" class=\"data row1 col1\" >325.27</td>\n",
       "      <td id=\"T_16f7d_row1_col2\" class=\"data row1 col2\" >54.21</td>\n",
       "      <td id=\"T_16f7d_row1_col3\" class=\"data row1 col3\" >1.18</td>\n",
       "      <td id=\"T_16f7d_row1_col4\" class=\"data row1 col4\" >4.72</td>\n",
       "      <td id=\"T_16f7d_row1_col5\" class=\"data row1 col5\" >0.02</td>\n",
       "      <td id=\"T_16f7d_row1_col6\" class=\"data row1 col6\" >0.02</td>\n",
       "      <td id=\"T_16f7d_row1_col7\" class=\"data row1 col7\" >0.04</td>\n",
       "      <td id=\"T_16f7d_row1_col8\" class=\"data row1 col8\" >0.04</td>\n",
       "      <td id=\"T_16f7d_row1_col9\" class=\"data row1 col9\" >0.01</td>\n",
       "      <td id=\"T_16f7d_row1_col10\" class=\"data row1 col10\" >0.06</td>\n",
       "      <td id=\"T_16f7d_row1_col11\" class=\"data row1 col11\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row1_col12\" class=\"data row1 col12\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row1_col13\" class=\"data row1 col13\" >0.01</td>\n",
       "      <td id=\"T_16f7d_row1_col14\" class=\"data row1 col14\" >0.77</td>\n",
       "      <td id=\"T_16f7d_row1_col15\" class=\"data row1 col15\" >0.01</td>\n",
       "      <td id=\"T_16f7d_row1_col16\" class=\"data row1 col16\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row1_col17\" class=\"data row1 col17\" >0.01</td>\n",
       "      <td id=\"T_16f7d_row1_col18\" class=\"data row1 col18\" >0.09</td>\n",
       "      <td id=\"T_16f7d_row1_col19\" class=\"data row1 col19\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row1_col20\" class=\"data row1 col20\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row1_col21\" class=\"data row1 col21\" >0.01</td>\n",
       "      <td id=\"T_16f7d_row1_col22\" class=\"data row1 col22\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row1_col23\" class=\"data row1 col23\" >0.02</td>\n",
       "      <td id=\"T_16f7d_row1_col24\" class=\"data row1 col24\" >0.05</td>\n",
       "      <td id=\"T_16f7d_row1_col25\" class=\"data row1 col25\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row1_col26\" class=\"data row1 col26\" >0.11</td>\n",
       "      <td id=\"T_16f7d_row1_col27\" class=\"data row1 col27\" >0.02</td>\n",
       "      <td id=\"T_16f7d_row1_col28\" class=\"data row1 col28\" >0.02</td>\n",
       "      <td id=\"T_16f7d_row1_col29\" class=\"data row1 col29\" >0.02</td>\n",
       "      <td id=\"T_16f7d_row1_col30\" class=\"data row1 col30\" >0.11</td>\n",
       "      <td id=\"T_16f7d_row1_col31\" class=\"data row1 col31\" >0.02</td>\n",
       "      <td id=\"T_16f7d_row1_col32\" class=\"data row1 col32\" >0.01</td>\n",
       "      <td id=\"T_16f7d_row1_col33\" class=\"data row1 col33\" >0.07</td>\n",
       "      <td id=\"T_16f7d_row1_col34\" class=\"data row1 col34\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row1_col35\" class=\"data row1 col35\" >0.01</td>\n",
       "      <td id=\"T_16f7d_row1_col36\" class=\"data row1 col36\" >0.43</td>\n",
       "      <td id=\"T_16f7d_row1_col37\" class=\"data row1 col37\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row1_col38\" class=\"data row1 col38\" >0.01</td>\n",
       "      <td id=\"T_16f7d_row1_col39\" class=\"data row1 col39\" >0.01</td>\n",
       "      <td id=\"T_16f7d_row1_col40\" class=\"data row1 col40\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row1_col41\" class=\"data row1 col41\" >0.02</td>\n",
       "      <td id=\"T_16f7d_row1_col42\" class=\"data row1 col42\" >0.01</td>\n",
       "      <td id=\"T_16f7d_row1_col43\" class=\"data row1 col43\" >0.01</td>\n",
       "      <td id=\"T_16f7d_row1_col44\" class=\"data row1 col44\" >0.02</td>\n",
       "      <td id=\"T_16f7d_row1_col45\" class=\"data row1 col45\" >0.05</td>\n",
       "      <td id=\"T_16f7d_row1_col46\" class=\"data row1 col46\" >0.01</td>\n",
       "      <td id=\"T_16f7d_row1_col47\" class=\"data row1 col47\" >0.01</td>\n",
       "      <td id=\"T_16f7d_row1_col48\" class=\"data row1 col48\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row1_col49\" class=\"data row1 col49\" >0.01</td>\n",
       "      <td id=\"T_16f7d_row1_col50\" class=\"data row1 col50\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row1_col51\" class=\"data row1 col51\" >0.01</td>\n",
       "      <td id=\"T_16f7d_row1_col52\" class=\"data row1 col52\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row1_col53\" class=\"data row1 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16f7d_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_16f7d_row2_col0\" class=\"data row2 col0\" >0.24</td>\n",
       "      <td id=\"T_16f7d_row2_col1\" class=\"data row2 col1\" >25.24</td>\n",
       "      <td id=\"T_16f7d_row2_col2\" class=\"data row2 col2\" >76.66</td>\n",
       "      <td id=\"T_16f7d_row2_col3\" class=\"data row2 col3\" >0.96</td>\n",
       "      <td id=\"T_16f7d_row2_col4\" class=\"data row2 col4\" >5.48</td>\n",
       "      <td id=\"T_16f7d_row2_col5\" class=\"data row2 col5\" >0.25</td>\n",
       "      <td id=\"T_16f7d_row2_col6\" class=\"data row2 col6\" >0.16</td>\n",
       "      <td id=\"T_16f7d_row2_col7\" class=\"data row2 col7\" >0.20</td>\n",
       "      <td id=\"T_16f7d_row2_col8\" class=\"data row2 col8\" >0.39</td>\n",
       "      <td id=\"T_16f7d_row2_col9\" class=\"data row2 col9\" >0.10</td>\n",
       "      <td id=\"T_16f7d_row2_col10\" class=\"data row2 col10\" >0.28</td>\n",
       "      <td id=\"T_16f7d_row2_col11\" class=\"data row2 col11\" >0.06</td>\n",
       "      <td id=\"T_16f7d_row2_col12\" class=\"data row2 col12\" >0.06</td>\n",
       "      <td id=\"T_16f7d_row2_col13\" class=\"data row2 col13\" >0.10</td>\n",
       "      <td id=\"T_16f7d_row2_col14\" class=\"data row2 col14\" >0.42</td>\n",
       "      <td id=\"T_16f7d_row2_col15\" class=\"data row2 col15\" >0.08</td>\n",
       "      <td id=\"T_16f7d_row2_col16\" class=\"data row2 col16\" >0.07</td>\n",
       "      <td id=\"T_16f7d_row2_col17\" class=\"data row2 col17\" >0.11</td>\n",
       "      <td id=\"T_16f7d_row2_col18\" class=\"data row2 col18\" >0.29</td>\n",
       "      <td id=\"T_16f7d_row2_col19\" class=\"data row2 col19\" >0.06</td>\n",
       "      <td id=\"T_16f7d_row2_col20\" class=\"data row2 col20\" >0.07</td>\n",
       "      <td id=\"T_16f7d_row2_col21\" class=\"data row2 col21\" >0.09</td>\n",
       "      <td id=\"T_16f7d_row2_col22\" class=\"data row2 col22\" >0.06</td>\n",
       "      <td id=\"T_16f7d_row2_col23\" class=\"data row2 col23\" >0.16</td>\n",
       "      <td id=\"T_16f7d_row2_col24\" class=\"data row2 col24\" >0.23</td>\n",
       "      <td id=\"T_16f7d_row2_col25\" class=\"data row2 col25\" >0.06</td>\n",
       "      <td id=\"T_16f7d_row2_col26\" class=\"data row2 col26\" >0.31</td>\n",
       "      <td id=\"T_16f7d_row2_col27\" class=\"data row2 col27\" >0.12</td>\n",
       "      <td id=\"T_16f7d_row2_col28\" class=\"data row2 col28\" >0.15</td>\n",
       "      <td id=\"T_16f7d_row2_col29\" class=\"data row2 col29\" >0.12</td>\n",
       "      <td id=\"T_16f7d_row2_col30\" class=\"data row2 col30\" >0.31</td>\n",
       "      <td id=\"T_16f7d_row2_col31\" class=\"data row2 col31\" >0.14</td>\n",
       "      <td id=\"T_16f7d_row2_col32\" class=\"data row2 col32\" >0.08</td>\n",
       "      <td id=\"T_16f7d_row2_col33\" class=\"data row2 col33\" >0.26</td>\n",
       "      <td id=\"T_16f7d_row2_col34\" class=\"data row2 col34\" >0.04</td>\n",
       "      <td id=\"T_16f7d_row2_col35\" class=\"data row2 col35\" >0.11</td>\n",
       "      <td id=\"T_16f7d_row2_col36\" class=\"data row2 col36\" >0.49</td>\n",
       "      <td id=\"T_16f7d_row2_col37\" class=\"data row2 col37\" >0.03</td>\n",
       "      <td id=\"T_16f7d_row2_col38\" class=\"data row2 col38\" >0.11</td>\n",
       "      <td id=\"T_16f7d_row2_col39\" class=\"data row2 col39\" >0.11</td>\n",
       "      <td id=\"T_16f7d_row2_col40\" class=\"data row2 col40\" >0.05</td>\n",
       "      <td id=\"T_16f7d_row2_col41\" class=\"data row2 col41\" >0.13</td>\n",
       "      <td id=\"T_16f7d_row2_col42\" class=\"data row2 col42\" >0.11</td>\n",
       "      <td id=\"T_16f7d_row2_col43\" class=\"data row2 col43\" >0.10</td>\n",
       "      <td id=\"T_16f7d_row2_col44\" class=\"data row2 col44\" >0.15</td>\n",
       "      <td id=\"T_16f7d_row2_col45\" class=\"data row2 col45\" >0.23</td>\n",
       "      <td id=\"T_16f7d_row2_col46\" class=\"data row2 col46\" >0.09</td>\n",
       "      <td id=\"T_16f7d_row2_col47\" class=\"data row2 col47\" >0.08</td>\n",
       "      <td id=\"T_16f7d_row2_col48\" class=\"data row2 col48\" >0.06</td>\n",
       "      <td id=\"T_16f7d_row2_col49\" class=\"data row2 col49\" >0.10</td>\n",
       "      <td id=\"T_16f7d_row2_col50\" class=\"data row2 col50\" >0.03</td>\n",
       "      <td id=\"T_16f7d_row2_col51\" class=\"data row2 col51\" >0.07</td>\n",
       "      <td id=\"T_16f7d_row2_col52\" class=\"data row2 col52\" >0.04</td>\n",
       "      <td id=\"T_16f7d_row2_col53\" class=\"data row2 col53\" >0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16f7d_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_16f7d_row3_col0\" class=\"data row3 col0\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col1\" class=\"data row3 col1\" >243.20</td>\n",
       "      <td id=\"T_16f7d_row3_col2\" class=\"data row3 col2\" >0.01</td>\n",
       "      <td id=\"T_16f7d_row3_col3\" class=\"data row3 col3\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col4\" class=\"data row3 col4\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col5\" class=\"data row3 col5\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col6\" class=\"data row3 col6\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col7\" class=\"data row3 col7\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col8\" class=\"data row3 col8\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col9\" class=\"data row3 col9\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col10\" class=\"data row3 col10\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col11\" class=\"data row3 col11\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col12\" class=\"data row3 col12\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col13\" class=\"data row3 col13\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col14\" class=\"data row3 col14\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col15\" class=\"data row3 col15\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col16\" class=\"data row3 col16\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col17\" class=\"data row3 col17\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col18\" class=\"data row3 col18\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col19\" class=\"data row3 col19\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col20\" class=\"data row3 col20\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col21\" class=\"data row3 col21\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col22\" class=\"data row3 col22\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col23\" class=\"data row3 col23\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col24\" class=\"data row3 col24\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col25\" class=\"data row3 col25\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col26\" class=\"data row3 col26\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col27\" class=\"data row3 col27\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col28\" class=\"data row3 col28\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col29\" class=\"data row3 col29\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col30\" class=\"data row3 col30\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col31\" class=\"data row3 col31\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col32\" class=\"data row3 col32\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col33\" class=\"data row3 col33\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col34\" class=\"data row3 col34\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col35\" class=\"data row3 col35\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col36\" class=\"data row3 col36\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col37\" class=\"data row3 col37\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col38\" class=\"data row3 col38\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col39\" class=\"data row3 col39\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col40\" class=\"data row3 col40\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col41\" class=\"data row3 col41\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col42\" class=\"data row3 col42\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col43\" class=\"data row3 col43\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col44\" class=\"data row3 col44\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col45\" class=\"data row3 col45\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col46\" class=\"data row3 col46\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col47\" class=\"data row3 col47\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col48\" class=\"data row3 col48\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col49\" class=\"data row3 col49\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col50\" class=\"data row3 col50\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col51\" class=\"data row3 col51\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col52\" class=\"data row3 col52\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row3_col53\" class=\"data row3 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16f7d_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_16f7d_row4_col0\" class=\"data row4 col0\" >0.14</td>\n",
       "      <td id=\"T_16f7d_row4_col1\" class=\"data row4 col1\" >308.15</td>\n",
       "      <td id=\"T_16f7d_row4_col2\" class=\"data row4 col2\" >10.00</td>\n",
       "      <td id=\"T_16f7d_row4_col3\" class=\"data row4 col3\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row4_col4\" class=\"data row4 col4\" >3.00</td>\n",
       "      <td id=\"T_16f7d_row4_col5\" class=\"data row4 col5\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col6\" class=\"data row4 col6\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col7\" class=\"data row4 col7\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col8\" class=\"data row4 col8\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col9\" class=\"data row4 col9\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col10\" class=\"data row4 col10\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col11\" class=\"data row4 col11\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col12\" class=\"data row4 col12\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col13\" class=\"data row4 col13\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col14\" class=\"data row4 col14\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row4_col15\" class=\"data row4 col15\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col16\" class=\"data row4 col16\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col17\" class=\"data row4 col17\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col18\" class=\"data row4 col18\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col19\" class=\"data row4 col19\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col20\" class=\"data row4 col20\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col21\" class=\"data row4 col21\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col22\" class=\"data row4 col22\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col23\" class=\"data row4 col23\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col24\" class=\"data row4 col24\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col25\" class=\"data row4 col25\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col26\" class=\"data row4 col26\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col27\" class=\"data row4 col27\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col28\" class=\"data row4 col28\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col29\" class=\"data row4 col29\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col30\" class=\"data row4 col30\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col31\" class=\"data row4 col31\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col32\" class=\"data row4 col32\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col33\" class=\"data row4 col33\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col34\" class=\"data row4 col34\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col35\" class=\"data row4 col35\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col36\" class=\"data row4 col36\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col37\" class=\"data row4 col37\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col38\" class=\"data row4 col38\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col39\" class=\"data row4 col39\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col40\" class=\"data row4 col40\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col41\" class=\"data row4 col41\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col42\" class=\"data row4 col42\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col43\" class=\"data row4 col43\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col44\" class=\"data row4 col44\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col45\" class=\"data row4 col45\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col46\" class=\"data row4 col46\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col47\" class=\"data row4 col47\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col48\" class=\"data row4 col48\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col49\" class=\"data row4 col49\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col50\" class=\"data row4 col50\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col51\" class=\"data row4 col51\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col52\" class=\"data row4 col52\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row4_col53\" class=\"data row4 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16f7d_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_16f7d_row5_col0\" class=\"data row5 col0\" >0.30</td>\n",
       "      <td id=\"T_16f7d_row5_col1\" class=\"data row5 col1\" >323.15</td>\n",
       "      <td id=\"T_16f7d_row5_col2\" class=\"data row5 col2\" >26.80</td>\n",
       "      <td id=\"T_16f7d_row5_col3\" class=\"data row5 col3\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row5_col4\" class=\"data row5 col4\" >3.00</td>\n",
       "      <td id=\"T_16f7d_row5_col5\" class=\"data row5 col5\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col6\" class=\"data row5 col6\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col7\" class=\"data row5 col7\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col8\" class=\"data row5 col8\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col9\" class=\"data row5 col9\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col10\" class=\"data row5 col10\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col11\" class=\"data row5 col11\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col12\" class=\"data row5 col12\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col13\" class=\"data row5 col13\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col14\" class=\"data row5 col14\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row5_col15\" class=\"data row5 col15\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col16\" class=\"data row5 col16\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col17\" class=\"data row5 col17\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col18\" class=\"data row5 col18\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col19\" class=\"data row5 col19\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col20\" class=\"data row5 col20\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col21\" class=\"data row5 col21\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col22\" class=\"data row5 col22\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col23\" class=\"data row5 col23\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col24\" class=\"data row5 col24\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col25\" class=\"data row5 col25\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col26\" class=\"data row5 col26\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col27\" class=\"data row5 col27\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col28\" class=\"data row5 col28\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col29\" class=\"data row5 col29\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col30\" class=\"data row5 col30\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col31\" class=\"data row5 col31\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col32\" class=\"data row5 col32\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col33\" class=\"data row5 col33\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col34\" class=\"data row5 col34\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col35\" class=\"data row5 col35\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col36\" class=\"data row5 col36\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col37\" class=\"data row5 col37\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col38\" class=\"data row5 col38\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col39\" class=\"data row5 col39\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col40\" class=\"data row5 col40\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col41\" class=\"data row5 col41\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col42\" class=\"data row5 col42\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col43\" class=\"data row5 col43\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col44\" class=\"data row5 col44\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col45\" class=\"data row5 col45\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col46\" class=\"data row5 col46\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col47\" class=\"data row5 col47\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col48\" class=\"data row5 col48\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col49\" class=\"data row5 col49\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col50\" class=\"data row5 col50\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col51\" class=\"data row5 col51\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col52\" class=\"data row5 col52\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row5_col53\" class=\"data row5 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16f7d_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_16f7d_row6_col0\" class=\"data row6 col0\" >0.51</td>\n",
       "      <td id=\"T_16f7d_row6_col1\" class=\"data row6 col1\" >342.59</td>\n",
       "      <td id=\"T_16f7d_row6_col2\" class=\"data row6 col2\" >64.76</td>\n",
       "      <td id=\"T_16f7d_row6_col3\" class=\"data row6 col3\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row6_col4\" class=\"data row6 col4\" >5.00</td>\n",
       "      <td id=\"T_16f7d_row6_col5\" class=\"data row6 col5\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col6\" class=\"data row6 col6\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col7\" class=\"data row6 col7\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col8\" class=\"data row6 col8\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col9\" class=\"data row6 col9\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col10\" class=\"data row6 col10\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col11\" class=\"data row6 col11\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col12\" class=\"data row6 col12\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col13\" class=\"data row6 col13\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col14\" class=\"data row6 col14\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row6_col15\" class=\"data row6 col15\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col16\" class=\"data row6 col16\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col17\" class=\"data row6 col17\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col18\" class=\"data row6 col18\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col19\" class=\"data row6 col19\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col20\" class=\"data row6 col20\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col21\" class=\"data row6 col21\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col22\" class=\"data row6 col22\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col23\" class=\"data row6 col23\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col24\" class=\"data row6 col24\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col25\" class=\"data row6 col25\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col26\" class=\"data row6 col26\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col27\" class=\"data row6 col27\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col28\" class=\"data row6 col28\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col29\" class=\"data row6 col29\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col30\" class=\"data row6 col30\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col31\" class=\"data row6 col31\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col32\" class=\"data row6 col32\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col33\" class=\"data row6 col33\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col34\" class=\"data row6 col34\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col35\" class=\"data row6 col35\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col36\" class=\"data row6 col36\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row6_col37\" class=\"data row6 col37\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col38\" class=\"data row6 col38\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col39\" class=\"data row6 col39\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col40\" class=\"data row6 col40\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col41\" class=\"data row6 col41\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col42\" class=\"data row6 col42\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col43\" class=\"data row6 col43\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col44\" class=\"data row6 col44\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col45\" class=\"data row6 col45\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col46\" class=\"data row6 col46\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col47\" class=\"data row6 col47\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col48\" class=\"data row6 col48\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col49\" class=\"data row6 col49\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col50\" class=\"data row6 col50\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col51\" class=\"data row6 col51\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col52\" class=\"data row6 col52\" >0.00</td>\n",
       "      <td id=\"T_16f7d_row6_col53\" class=\"data row6 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_16f7d_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_16f7d_row7_col0\" class=\"data row7 col0\" >0.95</td>\n",
       "      <td id=\"T_16f7d_row7_col1\" class=\"data row7 col1\" >453.15</td>\n",
       "      <td id=\"T_16f7d_row7_col2\" class=\"data row7 col2\" >499.90</td>\n",
       "      <td id=\"T_16f7d_row7_col3\" class=\"data row7 col3\" >7.00</td>\n",
       "      <td id=\"T_16f7d_row7_col4\" class=\"data row7 col4\" >28.00</td>\n",
       "      <td id=\"T_16f7d_row7_col5\" class=\"data row7 col5\" >3.00</td>\n",
       "      <td id=\"T_16f7d_row7_col6\" class=\"data row7 col6\" >2.00</td>\n",
       "      <td id=\"T_16f7d_row7_col7\" class=\"data row7 col7\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col8\" class=\"data row7 col8\" >5.00</td>\n",
       "      <td id=\"T_16f7d_row7_col9\" class=\"data row7 col9\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col10\" class=\"data row7 col10\" >3.00</td>\n",
       "      <td id=\"T_16f7d_row7_col11\" class=\"data row7 col11\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col12\" class=\"data row7 col12\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col13\" class=\"data row7 col13\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col14\" class=\"data row7 col14\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col15\" class=\"data row7 col15\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col16\" class=\"data row7 col16\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col17\" class=\"data row7 col17\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col18\" class=\"data row7 col18\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col19\" class=\"data row7 col19\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col20\" class=\"data row7 col20\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col21\" class=\"data row7 col21\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col22\" class=\"data row7 col22\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col23\" class=\"data row7 col23\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col24\" class=\"data row7 col24\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col25\" class=\"data row7 col25\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col26\" class=\"data row7 col26\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col27\" class=\"data row7 col27\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col28\" class=\"data row7 col28\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col29\" class=\"data row7 col29\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col30\" class=\"data row7 col30\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col31\" class=\"data row7 col31\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col32\" class=\"data row7 col32\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col33\" class=\"data row7 col33\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col34\" class=\"data row7 col34\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col35\" class=\"data row7 col35\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col36\" class=\"data row7 col36\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col37\" class=\"data row7 col37\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col38\" class=\"data row7 col38\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col39\" class=\"data row7 col39\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col40\" class=\"data row7 col40\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col41\" class=\"data row7 col41\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col42\" class=\"data row7 col42\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col43\" class=\"data row7 col43\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col44\" class=\"data row7 col44\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col45\" class=\"data row7 col45\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col46\" class=\"data row7 col46\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col47\" class=\"data row7 col47\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col48\" class=\"data row7 col48\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col49\" class=\"data row7 col49\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col50\" class=\"data row7 col50\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col51\" class=\"data row7 col51\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col52\" class=\"data row7 col52\" >1.00</td>\n",
       "      <td id=\"T_16f7d_row7_col53\" class=\"data row7 col53\" >1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb877f8ecb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataCO2f='CO2-data'+'/'+'dataCO2.csv'\n",
    "dataCO2=pd.read_csv(dataCO2f,sep=\";\",header=0)\n",
    "display(dataCO2)\n",
    "# describe() generates descriptive statistics\n",
    "display(dataCO2.describe().style.format(\"{0:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf1e66-0756-47f2-928d-d0d578613bba",
   "metadata": {},
   "source": [
    "## 2. Assessment of the stability of the original ML algorithm of Song *et al*. by *K*-fold cross validation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b3ba1f-2f90-441f-b931-28bc9e64748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# separation of the data set into two subsets: (1) training of the ANN & (2) test of the ANN\n",
    "# library used: pandas\n",
    "xdata = dataCO2.drop(['IL','cation','anion','x_CO2'],axis=1)\n",
    "ydata = dataCO2['x_CO2']\n",
    "\n",
    "#######################################################################################\n",
    "# ANN: 1 input layer (53 neurons) / 2 hidden layers (20 and 7 neurons) / 1 output layer (1 neuron) \n",
    "# library used: keras\n",
    "\n",
    "def defANN(shape,acthL):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name='iLayer'))\n",
    "    model.add(keras.layers.Dense(7, activation=acthL, name='hLayer'))\n",
    "    model.add(keras.layers.Dense(1, name='oLayer'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "acthL='tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a3138b7-5dc7-4804-93cb-caab9887319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[91mFold 0\u001b[0m\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 12:30:15.851266: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-30 12:30:15.851299: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-30 12:30:15.851321: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (localhost.localdomain): /proc/driver/nvidia/version does not exist\n",
      "2022-06-30 12:30:15.851594: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 2s 3ms/step - loss: 0.0699 - mae: 0.2155 - mse: 0.0699 - val_loss: 0.0572 - val_mae: 0.2029 - val_mse: 0.0572\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0556 - mae: 0.2000 - mse: 0.0556 - val_loss: 0.0571 - val_mae: 0.2043 - val_mse: 0.0571\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0556 - mae: 0.2003 - mse: 0.0556 - val_loss: 0.0571 - val_mae: 0.2032 - val_mse: 0.0571\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0556 - mae: 0.1998 - mse: 0.0556 - val_loss: 0.0572 - val_mae: 0.2050 - val_mse: 0.0572\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0557 - mae: 0.2002 - mse: 0.0557 - val_loss: 0.0571 - val_mae: 0.2043 - val_mse: 0.0571\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0556 - mae: 0.2000 - mse: 0.0556 - val_loss: 0.0571 - val_mae: 0.2037 - val_mse: 0.0571\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0556 - mae: 0.2000 - mse: 0.0556 - val_loss: 0.0577 - val_mae: 0.2075 - val_mse: 0.0577\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0556 - mae: 0.2001 - mse: 0.0556 - val_loss: 0.0572 - val_mae: 0.2055 - val_mse: 0.0572\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0556 - mae: 0.2002 - mse: 0.0556 - val_loss: 0.0573 - val_mae: 0.2056 - val_mse: 0.0573\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0556 - mae: 0.2001 - mse: 0.0556 - val_loss: 0.0571 - val_mae: 0.2044 - val_mse: 0.0571\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.1999 - mse: 0.0555 - val_loss: 0.0571 - val_mae: 0.2033 - val_mse: 0.0571\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.2000 - mse: 0.0555 - val_loss: 0.0573 - val_mae: 0.2024 - val_mse: 0.0573\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0557 - mae: 0.2002 - mse: 0.0557 - val_loss: 0.0571 - val_mae: 0.2043 - val_mse: 0.0571\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0557 - mae: 0.2001 - mse: 0.0557 - val_loss: 0.0573 - val_mae: 0.2025 - val_mse: 0.0573\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0556 - mae: 0.2001 - mse: 0.0556 - val_loss: 0.0571 - val_mae: 0.2041 - val_mse: 0.0571\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0556 - mae: 0.2000 - mse: 0.0556 - val_loss: 0.0573 - val_mae: 0.2057 - val_mse: 0.0573\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0556 - mae: 0.2001 - mse: 0.0556 - val_loss: 0.0573 - val_mae: 0.2060 - val_mse: 0.0573\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0555 - mae: 0.1999 - mse: 0.0555 - val_loss: 0.0572 - val_mae: 0.2049 - val_mse: 0.0572\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0556 - mae: 0.2003 - mse: 0.0556 - val_loss: 0.0572 - val_mae: 0.2054 - val_mse: 0.0572\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0556 - mae: 0.1998 - mse: 0.0556 - val_loss: 0.0571 - val_mae: 0.2043 - val_mse: 0.0571\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0556 - mae: 0.2001 - mse: 0.0556 - val_loss: 0.0580 - val_mae: 0.2014 - val_mse: 0.0580\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0556 - mae: 0.2001 - mse: 0.0556 - val_loss: 0.0571 - val_mae: 0.2045 - val_mse: 0.0571\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0556 - mae: 0.2001 - mse: 0.0556 - val_loss: 0.0572 - val_mae: 0.2054 - val_mse: 0.0572\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0556 - mae: 0.2000 - mse: 0.0556 - val_loss: 0.0573 - val_mae: 0.2056 - val_mse: 0.0573\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0556 - mae: 0.2000 - mse: 0.0556 - val_loss: 0.0571 - val_mae: 0.2048 - val_mse: 0.0571\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0557 - mae: 0.2001 - mse: 0.0557 - val_loss: 0.0572 - val_mae: 0.2028 - val_mse: 0.0572\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0557 - mae: 0.2000 - mse: 0.0557 - val_loss: 0.0573 - val_mae: 0.2057 - val_mse: 0.0573\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0556 - mae: 0.2004 - mse: 0.0556 - val_loss: 0.0571 - val_mae: 0.2034 - val_mse: 0.0571\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0557 - mae: 0.2001 - mse: 0.0557 - val_loss: 0.0571 - val_mae: 0.2037 - val_mse: 0.0571\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0557 - mae: 0.2000 - mse: 0.0557 - val_loss: 0.0575 - val_mae: 0.2020 - val_mse: 0.0575\n",
      "Epoch 30: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 1ms/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.020068890728774425    std:  0.23545455616478417    MAE:  0.1982978520485219     R2:  -0.028548249894968382\n",
      "Test. mean:  -0.01994157081157465    std:  0.23896300240213716    MAE:  0.20200936088966698     R2:  nan\n",
      "\u001b[1m\u001b[91mFold 1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.10/site-packages/numpy/lib/function_base.py:2691: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/usr/lib64/python3.10/site-packages/numpy/lib/function_base.py:2692: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.1170 - mae: 0.2754 - mse: 0.1170 - val_loss: 0.0563 - val_mae: 0.2030 - val_mse: 0.0563\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.2001 - val_mse: 0.0554\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2011 - mse: 0.0560 - val_loss: 0.0554 - val_mae: 0.1991 - val_mse: 0.0554\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0560 - mae: 0.2011 - mse: 0.0560 - val_loss: 0.0555 - val_mae: 0.1981 - val_mse: 0.0555\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2010 - mse: 0.0560 - val_loss: 0.0560 - val_mae: 0.2027 - val_mse: 0.0560\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0560 - mae: 0.2012 - mse: 0.0560 - val_loss: 0.0554 - val_mae: 0.1989 - val_mse: 0.0554\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0555 - val_mae: 0.2007 - val_mse: 0.0555\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0560 - mae: 0.2012 - mse: 0.0560 - val_loss: 0.0554 - val_mae: 0.1997 - val_mse: 0.0554\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0556 - val_mae: 0.2013 - val_mse: 0.0556\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0565 - val_mae: 0.2045 - val_mse: 0.0565\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0556 - val_mae: 0.1978 - val_mse: 0.0556\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2010 - mse: 0.0560 - val_loss: 0.0557 - val_mae: 0.2016 - val_mse: 0.0557\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0559 - val_mae: 0.2027 - val_mse: 0.0559\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0557 - val_mae: 0.2016 - val_mse: 0.0557\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2013 - mse: 0.0560 - val_loss: 0.0555 - val_mae: 0.2009 - val_mse: 0.0555\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2012 - mse: 0.0562 - val_loss: 0.0553 - val_mae: 0.1997 - val_mse: 0.0553\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0557 - val_mae: 0.2019 - val_mse: 0.0557\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0555 - val_mae: 0.2011 - val_mse: 0.0555\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.2002 - val_mse: 0.0554\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0560 - mae: 0.2009 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.1993 - val_mse: 0.0553\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2014 - mse: 0.0561 - val_loss: 0.0559 - val_mae: 0.2027 - val_mse: 0.0559\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1991 - val_mse: 0.0553\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.2003 - val_mse: 0.0554\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2012 - mse: 0.0562 - val_loss: 0.0558 - val_mae: 0.2022 - val_mse: 0.0558\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0560 - mae: 0.2010 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.1987 - val_mse: 0.0553\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0556 - val_mae: 0.2015 - val_mse: 0.0556\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2012 - mse: 0.0560 - val_loss: 0.0554 - val_mae: 0.2003 - val_mse: 0.0554\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2012 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.1984 - val_mse: 0.0553\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2010 - mse: 0.0560 - val_loss: 0.0558 - val_mae: 0.1972 - val_mse: 0.0558\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2012 - mse: 0.0560 - val_loss: 0.0558 - val_mae: 0.1972 - val_mse: 0.0558\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1987 - val_mse: 0.0553\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2015 - mse: 0.0562 - val_loss: 0.0557 - val_mae: 0.1972 - val_mse: 0.0557\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2012 - mse: 0.0562 - val_loss: 0.0554 - val_mae: 0.1980 - val_mse: 0.0554\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0559 - mae: 0.2008 - mse: 0.0559 - val_loss: 0.0556 - val_mae: 0.2018 - val_mse: 0.0556\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0556 - val_mae: 0.1975 - val_mse: 0.0556\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2014 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1986 - val_mse: 0.0553\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2011 - mse: 0.0561 - val_loss: 0.0557 - val_mae: 0.2019 - val_mse: 0.0557\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1984 - val_mse: 0.0553\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0560 - mae: 0.2012 - mse: 0.0560 - val_loss: 0.0556 - val_mae: 0.2019 - val_mse: 0.0556\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.2003 - val_mse: 0.0553\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0560 - mae: 0.2011 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.1990 - val_mse: 0.0553\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2012 - mse: 0.0562 - val_loss: 0.0565 - val_mae: 0.2045 - val_mse: 0.0565\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.2008 - val_mse: 0.0554\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.2003 - val_mse: 0.0553\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0562 - mae: 0.2014 - mse: 0.0562 - val_loss: 0.0558 - val_mae: 0.2024 - val_mse: 0.0558\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2016 - mse: 0.0561 - val_loss: 0.0556 - val_mae: 0.1975 - val_mse: 0.0556\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1997 - val_mse: 0.0553\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0556 - val_mae: 0.1976 - val_mse: 0.0556\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1998 - val_mse: 0.0553\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0560 - mae: 0.2011 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.2000 - val_mse: 0.0553\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1995 - val_mse: 0.0553\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0560 - mae: 0.2012 - mse: 0.0560 - val_loss: 0.0554 - val_mae: 0.2008 - val_mse: 0.0554\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0562 - mae: 0.2015 - mse: 0.0562 - val_loss: 0.0553 - val_mae: 0.2000 - val_mse: 0.0553\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0557 - val_mae: 0.2023 - val_mse: 0.0557\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2013 - mse: 0.0562 - val_loss: 0.0554 - val_mae: 0.1982 - val_mse: 0.0554\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2014 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.2010 - val_mse: 0.0554\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.2005 - val_mse: 0.0554\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2011 - mse: 0.0561 - val_loss: 0.0556 - val_mae: 0.2018 - val_mse: 0.0556\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2014 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1993 - val_mse: 0.0553\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.2001 - val_mse: 0.0553\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0557 - val_mae: 0.1974 - val_mse: 0.0557\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1988 - val_mse: 0.0553\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1999 - val_mse: 0.0553\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2014 - mse: 0.0561 - val_loss: 0.0556 - val_mae: 0.1974 - val_mse: 0.0556\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2016 - mse: 0.0563 - val_loss: 0.0553 - val_mae: 0.1985 - val_mse: 0.0553\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2011 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.1993 - val_mse: 0.0553\n",
      "Epoch 67/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2011 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.2007 - val_mse: 0.0554\n",
      "Epoch 68/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1992 - val_mse: 0.0553\n",
      "Epoch 69/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0562 - mae: 0.2012 - mse: 0.0562 - val_loss: 0.0554 - val_mae: 0.2009 - val_mse: 0.0554\n",
      "Epoch 70/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2014 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.2004 - val_mse: 0.0553\n",
      "Epoch 71/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0560 - mae: 0.2010 - mse: 0.0560 - val_loss: 0.0562 - val_mae: 0.2037 - val_mse: 0.0562\n",
      "Epoch 72/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0562 - mae: 0.2015 - mse: 0.0562 - val_loss: 0.0553 - val_mae: 0.1989 - val_mse: 0.0553\n",
      "Epoch 73/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1996 - val_mse: 0.0553\n",
      "Epoch 74/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1988 - val_mse: 0.0553\n",
      "Epoch 75/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0560 - mae: 0.2012 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.1988 - val_mse: 0.0553\n",
      "Epoch 76/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2010 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.1998 - val_mse: 0.0553\n",
      "Epoch 77/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.2008 - val_mse: 0.0554\n",
      "Epoch 78/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1991 - val_mse: 0.0553\n",
      "Epoch 79/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1990 - val_mse: 0.0553\n",
      "Epoch 80/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1994 - val_mse: 0.0553\n",
      "Epoch 81/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0560 - mae: 0.2011 - mse: 0.0560 - val_loss: 0.0561 - val_mae: 0.2035 - val_mse: 0.0561\n",
      "Epoch 81: early stopping\n",
      "253/253 [==============================] - 0s 2ms/step\n",
      "64/64 [==============================] - 0s 2ms/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  0.02206890731678776    std:  0.23641055181585133    MAE:  0.20388704012645922     R2:  nan\n",
      "Test. mean:  0.028883209377735725    std:  0.23507965264680147    MAE:  0.20347816218690015     R2:  -0.05198566851614379\n",
      "\u001b[1m\u001b[91mFold 2\u001b[0m\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.10/site-packages/numpy/lib/function_base.py:2691: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/usr/lib64/python3.10/site-packages/numpy/lib/function_base.py:2692: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 2s 3ms/step - loss: 0.0232 - mae: 0.1236 - mse: 0.0232 - val_loss: 0.0215 - val_mae: 0.1207 - val_mse: 0.0215\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0214 - mae: 0.1181 - mse: 0.0214 - val_loss: 0.0196 - val_mae: 0.1135 - val_mse: 0.0196\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0200 - mae: 0.1139 - mse: 0.0200 - val_loss: 0.0192 - val_mae: 0.1112 - val_mse: 0.0192\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0171 - mae: 0.1036 - mse: 0.0171 - val_loss: 0.0133 - val_mae: 0.0920 - val_mse: 0.0133\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0128 - mae: 0.0875 - mse: 0.0128 - val_loss: 0.0111 - val_mae: 0.0812 - val_mse: 0.0111\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0114 - mae: 0.0821 - mse: 0.0114 - val_loss: 0.0097 - val_mae: 0.0762 - val_mse: 0.0097\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0106 - mae: 0.0793 - mse: 0.0106 - val_loss: 0.0091 - val_mae: 0.0736 - val_mse: 0.0091\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0100 - mae: 0.0766 - mse: 0.0100 - val_loss: 0.0089 - val_mae: 0.0726 - val_mse: 0.0089\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0099 - mae: 0.0762 - mse: 0.0099 - val_loss: 0.0088 - val_mae: 0.0716 - val_mse: 0.0088\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0097 - mae: 0.0755 - mse: 0.0097 - val_loss: 0.0088 - val_mae: 0.0723 - val_mse: 0.0088\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0095 - mae: 0.0747 - mse: 0.0095 - val_loss: 0.0086 - val_mae: 0.0706 - val_mse: 0.0086\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0095 - mae: 0.0745 - mse: 0.0095 - val_loss: 0.0084 - val_mae: 0.0695 - val_mse: 0.0084\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0093 - mae: 0.0739 - mse: 0.0093 - val_loss: 0.0095 - val_mae: 0.0779 - val_mse: 0.0095\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0093 - mae: 0.0738 - mse: 0.0093 - val_loss: 0.0096 - val_mae: 0.0782 - val_mse: 0.0096\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0736 - mse: 0.0092 - val_loss: 0.0082 - val_mae: 0.0682 - val_mse: 0.0082\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0734 - mse: 0.0092 - val_loss: 0.0083 - val_mae: 0.0682 - val_mse: 0.0083\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0093 - mae: 0.0741 - mse: 0.0093 - val_loss: 0.0082 - val_mae: 0.0692 - val_mse: 0.0082\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0726 - mse: 0.0090 - val_loss: 0.0080 - val_mae: 0.0680 - val_mse: 0.0080\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0723 - mse: 0.0090 - val_loss: 0.0089 - val_mae: 0.0712 - val_mse: 0.0089\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0090 - mae: 0.0727 - mse: 0.0090 - val_loss: 0.0085 - val_mae: 0.0689 - val_mse: 0.0085\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0089 - mae: 0.0721 - mse: 0.0089 - val_loss: 0.0086 - val_mae: 0.0687 - val_mse: 0.0086\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0728 - mse: 0.0091 - val_loss: 0.0083 - val_mae: 0.0715 - val_mse: 0.0083\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0711 - mse: 0.0087 - val_loss: 0.0091 - val_mae: 0.0757 - val_mse: 0.0091\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0723 - mse: 0.0089 - val_loss: 0.0085 - val_mae: 0.0692 - val_mse: 0.0085\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0741 - mse: 0.0092 - val_loss: 0.0092 - val_mae: 0.0763 - val_mse: 0.0092\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0728 - mse: 0.0090 - val_loss: 0.0083 - val_mae: 0.0711 - val_mse: 0.0083\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0088 - mae: 0.0717 - mse: 0.0088 - val_loss: 0.0080 - val_mae: 0.0673 - val_mse: 0.0080\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0726 - mse: 0.0090 - val_loss: 0.0090 - val_mae: 0.0711 - val_mse: 0.0090\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0719 - mse: 0.0088 - val_loss: 0.0078 - val_mae: 0.0665 - val_mse: 0.0078\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0725 - mse: 0.0090 - val_loss: 0.0085 - val_mae: 0.0681 - val_mse: 0.0085\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0718 - mse: 0.0088 - val_loss: 0.0079 - val_mae: 0.0665 - val_mse: 0.0079\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0705 - mse: 0.0085 - val_loss: 0.0092 - val_mae: 0.0729 - val_mse: 0.0092\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0728 - mse: 0.0089 - val_loss: 0.0079 - val_mae: 0.0660 - val_mse: 0.0079\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0087 - mae: 0.0714 - mse: 0.0087 - val_loss: 0.0079 - val_mae: 0.0688 - val_mse: 0.0079\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0708 - mse: 0.0086 - val_loss: 0.0077 - val_mae: 0.0666 - val_mse: 0.0077\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0724 - mse: 0.0089 - val_loss: 0.0081 - val_mae: 0.0663 - val_mse: 0.0081\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0715 - mse: 0.0087 - val_loss: 0.0081 - val_mae: 0.0707 - val_mse: 0.0081\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0715 - mse: 0.0087 - val_loss: 0.0080 - val_mae: 0.0699 - val_mse: 0.0080\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0087 - mae: 0.0714 - mse: 0.0087 - val_loss: 0.0080 - val_mae: 0.0697 - val_mse: 0.0080\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0706 - mse: 0.0085 - val_loss: 0.0077 - val_mae: 0.0669 - val_mse: 0.0077\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0715 - mse: 0.0087 - val_loss: 0.0082 - val_mae: 0.0716 - val_mse: 0.0082\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0734 - mse: 0.0090 - val_loss: 0.0089 - val_mae: 0.0759 - val_mse: 0.0089\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0722 - mse: 0.0088 - val_loss: 0.0078 - val_mae: 0.0658 - val_mse: 0.0078\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0718 - mse: 0.0087 - val_loss: 0.0077 - val_mae: 0.0668 - val_mse: 0.0077\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0708 - mse: 0.0085 - val_loss: 0.0078 - val_mae: 0.0658 - val_mse: 0.0078\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0711 - mse: 0.0086 - val_loss: 0.0082 - val_mae: 0.0718 - val_mse: 0.0082\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0708 - mse: 0.0086 - val_loss: 0.0092 - val_mae: 0.0779 - val_mse: 0.0092\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0707 - mse: 0.0086 - val_loss: 0.0079 - val_mae: 0.0657 - val_mse: 0.0079\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0719 - mse: 0.0087 - val_loss: 0.0096 - val_mae: 0.0800 - val_mse: 0.0096\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0709 - mse: 0.0086 - val_loss: 0.0077 - val_mae: 0.0660 - val_mse: 0.0077\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0711 - mse: 0.0086 - val_loss: 0.0082 - val_mae: 0.0712 - val_mse: 0.0082\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0711 - mse: 0.0086 - val_loss: 0.0078 - val_mae: 0.0678 - val_mse: 0.0078\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0706 - mse: 0.0085 - val_loss: 0.0085 - val_mae: 0.0679 - val_mse: 0.0085\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0712 - mse: 0.0086 - val_loss: 0.0078 - val_mae: 0.0657 - val_mse: 0.0078\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0715 - mse: 0.0087 - val_loss: 0.0106 - val_mae: 0.0796 - val_mse: 0.0106\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0719 - mse: 0.0088 - val_loss: 0.0096 - val_mae: 0.0798 - val_mse: 0.0096\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0724 - mse: 0.0088 - val_loss: 0.0087 - val_mae: 0.0697 - val_mse: 0.0087\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0705 - mse: 0.0085 - val_loss: 0.0080 - val_mae: 0.0694 - val_mse: 0.0080\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0086 - mae: 0.0715 - mse: 0.0086 - val_loss: 0.0079 - val_mae: 0.0659 - val_mse: 0.0079\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0720 - mse: 0.0087 - val_loss: 0.0080 - val_mae: 0.0696 - val_mse: 0.0080\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0087 - mae: 0.0717 - mse: 0.0087 - val_loss: 0.0079 - val_mae: 0.0690 - val_mse: 0.0079\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0086 - mae: 0.0712 - mse: 0.0086 - val_loss: 0.0079 - val_mae: 0.0658 - val_mse: 0.0079\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0085 - mae: 0.0707 - mse: 0.0085 - val_loss: 0.0083 - val_mae: 0.0723 - val_mse: 0.0083\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0086 - mae: 0.0711 - mse: 0.0086 - val_loss: 0.0089 - val_mae: 0.0761 - val_mse: 0.0089\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0087 - mae: 0.0716 - mse: 0.0087 - val_loss: 0.0078 - val_mae: 0.0658 - val_mse: 0.0078\n",
      "Epoch 65: early stopping\n",
      "253/253 [==============================] - 0s 2ms/step\n",
      "64/64 [==============================] - 0s 2ms/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  0.00805638905410305    std:  0.09059226695318794    MAE:  0.06818905664162807     R2:  0.9240469870248417\n",
      "Test. mean:  0.008421536500807129    std:  0.0881740837389064    MAE:  0.06584487981256891     R2:  0.9255337633600431\n",
      "\u001b[1m\u001b[91mFold 3\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 2s 3ms/step - loss: 0.0974 - mae: 0.2451 - mse: 0.0974 - val_loss: 0.0561 - val_mae: 0.1962 - val_mse: 0.0561\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0475 - mae: 0.1779 - mse: 0.0475 - val_loss: 0.0441 - val_mae: 0.1729 - val_mse: 0.0441\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0396 - mae: 0.1631 - mse: 0.0396 - val_loss: 0.0424 - val_mae: 0.1695 - val_mse: 0.0424\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0393 - mae: 0.1629 - mse: 0.0393 - val_loss: 0.0389 - val_mae: 0.1651 - val_mse: 0.0389\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0304 - mae: 0.1413 - mse: 0.0304 - val_loss: 0.0230 - val_mae: 0.1239 - val_mse: 0.0230\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0238 - mae: 0.1214 - mse: 0.0238 - val_loss: 0.0210 - val_mae: 0.1186 - val_mse: 0.0210\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0204 - mae: 0.1147 - mse: 0.0204 - val_loss: 0.0201 - val_mae: 0.1150 - val_mse: 0.0201\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0180 - mae: 0.1079 - mse: 0.0180 - val_loss: 0.0176 - val_mae: 0.1066 - val_mse: 0.0176\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0182 - mae: 0.1074 - mse: 0.0182 - val_loss: 0.0204 - val_mae: 0.1137 - val_mse: 0.0204\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0172 - mae: 0.1047 - mse: 0.0172 - val_loss: 0.0167 - val_mae: 0.1029 - val_mse: 0.0167\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0166 - mae: 0.1021 - mse: 0.0166 - val_loss: 0.0163 - val_mae: 0.1004 - val_mse: 0.0163\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0158 - mae: 0.0990 - mse: 0.0158 - val_loss: 0.0169 - val_mae: 0.1017 - val_mse: 0.0169\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0154 - mae: 0.0963 - mse: 0.0154 - val_loss: 0.0156 - val_mae: 0.0950 - val_mse: 0.0156\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0148 - mae: 0.0937 - mse: 0.0148 - val_loss: 0.0149 - val_mae: 0.0945 - val_mse: 0.0149\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0142 - mae: 0.0914 - mse: 0.0142 - val_loss: 0.0137 - val_mae: 0.0895 - val_mse: 0.0137\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0127 - mae: 0.0854 - mse: 0.0127 - val_loss: 0.0134 - val_mae: 0.0889 - val_mse: 0.0134\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0116 - mae: 0.0816 - mse: 0.0116 - val_loss: 0.0137 - val_mae: 0.0874 - val_mse: 0.0137\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0108 - mae: 0.0778 - mse: 0.0108 - val_loss: 0.0106 - val_mae: 0.0770 - val_mse: 0.0106\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0720 - mse: 0.0092 - val_loss: 0.0091 - val_mae: 0.0710 - val_mse: 0.0091\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0084 - mae: 0.0687 - mse: 0.0084 - val_loss: 0.0083 - val_mae: 0.0685 - val_mse: 0.0083\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0083 - mae: 0.0687 - mse: 0.0083 - val_loss: 0.0087 - val_mae: 0.0697 - val_mse: 0.0087\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0081 - mae: 0.0679 - mse: 0.0081 - val_loss: 0.0083 - val_mae: 0.0698 - val_mse: 0.0083\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0690 - mse: 0.0083 - val_loss: 0.0089 - val_mae: 0.0737 - val_mse: 0.0089\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0081 - mae: 0.0683 - mse: 0.0081 - val_loss: 0.0090 - val_mae: 0.0699 - val_mse: 0.0090\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0079 - mae: 0.0668 - mse: 0.0079 - val_loss: 0.0084 - val_mae: 0.0684 - val_mse: 0.0084\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0080 - mae: 0.0676 - mse: 0.0080 - val_loss: 0.0077 - val_mae: 0.0655 - val_mse: 0.0077\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0077 - mae: 0.0663 - mse: 0.0077 - val_loss: 0.0077 - val_mae: 0.0669 - val_mse: 0.0077\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0079 - mae: 0.0673 - mse: 0.0079 - val_loss: 0.0078 - val_mae: 0.0658 - val_mse: 0.0078\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0075 - mae: 0.0652 - mse: 0.0075 - val_loss: 0.0076 - val_mae: 0.0646 - val_mse: 0.0076\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0077 - mae: 0.0664 - mse: 0.0077 - val_loss: 0.0084 - val_mae: 0.0717 - val_mse: 0.0084\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0076 - mae: 0.0660 - mse: 0.0076 - val_loss: 0.0077 - val_mae: 0.0674 - val_mse: 0.0077\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0073 - mae: 0.0646 - mse: 0.0073 - val_loss: 0.0073 - val_mae: 0.0638 - val_mse: 0.0073\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0075 - mae: 0.0656 - mse: 0.0075 - val_loss: 0.0090 - val_mae: 0.0710 - val_mse: 0.0090\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0074 - mae: 0.0653 - mse: 0.0074 - val_loss: 0.0073 - val_mae: 0.0627 - val_mse: 0.0073\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0073 - mae: 0.0644 - mse: 0.0073 - val_loss: 0.0094 - val_mae: 0.0747 - val_mse: 0.0094\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0073 - mae: 0.0640 - mse: 0.0073 - val_loss: 0.0071 - val_mae: 0.0628 - val_mse: 0.0071\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0073 - mae: 0.0647 - mse: 0.0073 - val_loss: 0.0071 - val_mae: 0.0646 - val_mse: 0.0071\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0071 - mae: 0.0635 - mse: 0.0071 - val_loss: 0.0097 - val_mae: 0.0804 - val_mse: 0.0097\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0069 - mae: 0.0623 - mse: 0.0069 - val_loss: 0.0073 - val_mae: 0.0649 - val_mse: 0.0073\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0073 - mae: 0.0650 - mse: 0.0073 - val_loss: 0.0088 - val_mae: 0.0711 - val_mse: 0.0088\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0070 - mae: 0.0633 - mse: 0.0070 - val_loss: 0.0073 - val_mae: 0.0671 - val_mse: 0.0073\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0622 - mse: 0.0068 - val_loss: 0.0074 - val_mae: 0.0677 - val_mse: 0.0074\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0066 - mae: 0.0613 - mse: 0.0066 - val_loss: 0.0065 - val_mae: 0.0609 - val_mse: 0.0065\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0066 - mae: 0.0616 - mse: 0.0066 - val_loss: 0.0081 - val_mae: 0.0688 - val_mse: 0.0081\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0623 - mse: 0.0067 - val_loss: 0.0081 - val_mae: 0.0723 - val_mse: 0.0081\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0597 - mse: 0.0063 - val_loss: 0.0064 - val_mae: 0.0597 - val_mse: 0.0064\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0619 - mse: 0.0067 - val_loss: 0.0065 - val_mae: 0.0612 - val_mse: 0.0065\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0614 - mse: 0.0066 - val_loss: 0.0066 - val_mae: 0.0590 - val_mse: 0.0066\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0618 - mse: 0.0066 - val_loss: 0.0068 - val_mae: 0.0639 - val_mse: 0.0068\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0592 - mse: 0.0062 - val_loss: 0.0064 - val_mae: 0.0612 - val_mse: 0.0064\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0606 - mse: 0.0064 - val_loss: 0.0067 - val_mae: 0.0602 - val_mse: 0.0067\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0611 - mse: 0.0065 - val_loss: 0.0061 - val_mae: 0.0585 - val_mse: 0.0061\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0612 - mse: 0.0065 - val_loss: 0.0074 - val_mae: 0.0677 - val_mse: 0.0074\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0604 - mse: 0.0064 - val_loss: 0.0064 - val_mae: 0.0613 - val_mse: 0.0064\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0619 - mse: 0.0067 - val_loss: 0.0062 - val_mae: 0.0588 - val_mse: 0.0062\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0600 - mse: 0.0063 - val_loss: 0.0081 - val_mae: 0.0690 - val_mse: 0.0081\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0615 - mse: 0.0066 - val_loss: 0.0065 - val_mae: 0.0588 - val_mse: 0.0065\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0603 - mse: 0.0064 - val_loss: 0.0062 - val_mae: 0.0598 - val_mse: 0.0062\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0591 - mse: 0.0062 - val_loss: 0.0061 - val_mae: 0.0574 - val_mse: 0.0061\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0601 - mse: 0.0063 - val_loss: 0.0065 - val_mae: 0.0609 - val_mse: 0.0065\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0601 - mse: 0.0063 - val_loss: 0.0062 - val_mae: 0.0599 - val_mse: 0.0062\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0587 - mse: 0.0061 - val_loss: 0.0062 - val_mae: 0.0583 - val_mse: 0.0062\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0063 - mae: 0.0601 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0573 - val_mse: 0.0061\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0064 - mae: 0.0608 - mse: 0.0064 - val_loss: 0.0064 - val_mae: 0.0615 - val_mse: 0.0064\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0062 - mae: 0.0595 - mse: 0.0062 - val_loss: 0.0070 - val_mae: 0.0623 - val_mse: 0.0070\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0063 - mae: 0.0599 - mse: 0.0063 - val_loss: 0.0065 - val_mae: 0.0617 - val_mse: 0.0065\n",
      "Epoch 67/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0063 - mae: 0.0601 - mse: 0.0063 - val_loss: 0.0062 - val_mae: 0.0599 - val_mse: 0.0062\n",
      "Epoch 68/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0062 - mae: 0.0596 - mse: 0.0062 - val_loss: 0.0074 - val_mae: 0.0687 - val_mse: 0.0074\n",
      "Epoch 69/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0064 - mae: 0.0608 - mse: 0.0064 - val_loss: 0.0067 - val_mae: 0.0604 - val_mse: 0.0067\n",
      "Epoch 70/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0065 - mae: 0.0612 - mse: 0.0065 - val_loss: 0.0063 - val_mae: 0.0583 - val_mse: 0.0063\n",
      "Epoch 71/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0063 - mae: 0.0595 - mse: 0.0063 - val_loss: 0.0060 - val_mae: 0.0577 - val_mse: 0.0060\n",
      "Epoch 72/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0062 - mae: 0.0592 - mse: 0.0062 - val_loss: 0.0061 - val_mae: 0.0572 - val_mse: 0.0061\n",
      "Epoch 73/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0063 - mae: 0.0600 - mse: 0.0063 - val_loss: 0.0069 - val_mae: 0.0616 - val_mse: 0.0069\n",
      "Epoch 74/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0062 - mae: 0.0596 - mse: 0.0062 - val_loss: 0.0061 - val_mae: 0.0583 - val_mse: 0.0061\n",
      "Epoch 75/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0064 - mae: 0.0603 - mse: 0.0064 - val_loss: 0.0061 - val_mae: 0.0572 - val_mse: 0.0061\n",
      "Epoch 76/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0062 - mae: 0.0592 - mse: 0.0062 - val_loss: 0.0062 - val_mae: 0.0592 - val_mse: 0.0062\n",
      "Epoch 77/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0062 - mae: 0.0596 - mse: 0.0062 - val_loss: 0.0063 - val_mae: 0.0606 - val_mse: 0.0063\n",
      "Epoch 78/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0062 - mae: 0.0598 - mse: 0.0062 - val_loss: 0.0060 - val_mae: 0.0570 - val_mse: 0.0060\n",
      "Epoch 79/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0062 - mae: 0.0597 - mse: 0.0062 - val_loss: 0.0061 - val_mae: 0.0576 - val_mse: 0.0061\n",
      "Epoch 80/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0062 - mae: 0.0600 - mse: 0.0062 - val_loss: 0.0071 - val_mae: 0.0672 - val_mse: 0.0071\n",
      "Epoch 81/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0063 - mae: 0.0602 - mse: 0.0063 - val_loss: 0.0060 - val_mae: 0.0584 - val_mse: 0.0060\n",
      "Epoch 82/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0060 - mae: 0.0582 - mse: 0.0060 - val_loss: 0.0063 - val_mae: 0.0591 - val_mse: 0.0063\n",
      "Epoch 83/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0061 - mae: 0.0591 - mse: 0.0061 - val_loss: 0.0063 - val_mae: 0.0607 - val_mse: 0.0063\n",
      "Epoch 84/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0606 - mse: 0.0064 - val_loss: 0.0060 - val_mae: 0.0584 - val_mse: 0.0060\n",
      "Epoch 85/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0597 - mse: 0.0062 - val_loss: 0.0061 - val_mae: 0.0571 - val_mse: 0.0061\n",
      "Epoch 86/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0594 - mse: 0.0062 - val_loss: 0.0064 - val_mae: 0.0613 - val_mse: 0.0064\n",
      "Epoch 86: early stopping\n",
      "253/253 [==============================] - 0s 2ms/step\n",
      "64/64 [==============================] - 0s 1ms/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.018425020120686135    std:  0.07550353521415791    MAE:  0.05993061701450329     R2:  0.9471301098351315\n",
      "Test. mean:  -0.016546222117541164    std:  0.07804014886975015    MAE:  0.0613224121012679     R2:  0.9457547669496666\n",
      "\u001b[1m\u001b[91mFold 4\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 2s 3ms/step - loss: 0.1964 - mae: 0.2662 - mse: 0.1964 - val_loss: 0.0216 - val_mae: 0.1175 - val_mse: 0.0216\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0165 - mae: 0.1010 - mse: 0.0165 - val_loss: 0.0139 - val_mae: 0.0899 - val_mse: 0.0139\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0130 - mae: 0.0859 - mse: 0.0130 - val_loss: 0.0114 - val_mae: 0.0789 - val_mse: 0.0114\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0106 - mae: 0.0768 - mse: 0.0106 - val_loss: 0.0093 - val_mae: 0.0699 - val_mse: 0.0093\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0090 - mae: 0.0699 - mse: 0.0090 - val_loss: 0.0087 - val_mae: 0.0674 - val_mse: 0.0087\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0658 - mse: 0.0082 - val_loss: 0.0071 - val_mae: 0.0603 - val_mse: 0.0071\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0618 - mse: 0.0072 - val_loss: 0.0064 - val_mae: 0.0566 - val_mse: 0.0064\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0597 - mse: 0.0067 - val_loss: 0.0061 - val_mae: 0.0575 - val_mse: 0.0061\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0581 - mse: 0.0062 - val_loss: 0.0056 - val_mae: 0.0541 - val_mse: 0.0056\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0058 - mae: 0.0561 - mse: 0.0058 - val_loss: 0.0064 - val_mae: 0.0621 - val_mse: 0.0064\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0057 - mae: 0.0561 - mse: 0.0057 - val_loss: 0.0054 - val_mae: 0.0543 - val_mse: 0.0054\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0051 - mae: 0.0530 - mse: 0.0051 - val_loss: 0.0049 - val_mae: 0.0514 - val_mse: 0.0049\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0052 - mae: 0.0541 - mse: 0.0052 - val_loss: 0.0062 - val_mae: 0.0635 - val_mse: 0.0062\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0049 - mae: 0.0521 - mse: 0.0049 - val_loss: 0.0055 - val_mae: 0.0556 - val_mse: 0.0055\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0048 - mae: 0.0520 - mse: 0.0048 - val_loss: 0.0045 - val_mae: 0.0485 - val_mse: 0.0045\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0046 - mae: 0.0512 - mse: 0.0046 - val_loss: 0.0042 - val_mae: 0.0477 - val_mse: 0.0042\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0043 - mae: 0.0495 - mse: 0.0043 - val_loss: 0.0044 - val_mae: 0.0488 - val_mse: 0.0044\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0043 - mae: 0.0495 - mse: 0.0043 - val_loss: 0.0046 - val_mae: 0.0536 - val_mse: 0.0046\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0043 - mae: 0.0496 - mse: 0.0043 - val_loss: 0.0038 - val_mae: 0.0460 - val_mse: 0.0038\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0043 - mae: 0.0495 - mse: 0.0043 - val_loss: 0.0039 - val_mae: 0.0455 - val_mse: 0.0039\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0040 - mae: 0.0480 - mse: 0.0040 - val_loss: 0.0036 - val_mae: 0.0445 - val_mse: 0.0036\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0039 - mae: 0.0474 - mse: 0.0039 - val_loss: 0.0038 - val_mae: 0.0464 - val_mse: 0.0038\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0041 - mae: 0.0487 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0434 - val_mse: 0.0035\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0039 - mae: 0.0471 - mse: 0.0039 - val_loss: 0.0035 - val_mae: 0.0445 - val_mse: 0.0035\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0468 - mse: 0.0038 - val_loss: 0.0038 - val_mae: 0.0478 - val_mse: 0.0038\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0463 - mse: 0.0037 - val_loss: 0.0036 - val_mae: 0.0449 - val_mse: 0.0036\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0466 - mse: 0.0038 - val_loss: 0.0034 - val_mae: 0.0434 - val_mse: 0.0034\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0473 - mse: 0.0038 - val_loss: 0.0034 - val_mae: 0.0432 - val_mse: 0.0034\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0460 - mse: 0.0037 - val_loss: 0.0034 - val_mae: 0.0440 - val_mse: 0.0034\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0466 - mse: 0.0037 - val_loss: 0.0045 - val_mae: 0.0521 - val_mse: 0.0045\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0466 - mse: 0.0037 - val_loss: 0.0038 - val_mae: 0.0467 - val_mse: 0.0038\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0462 - mse: 0.0037 - val_loss: 0.0042 - val_mae: 0.0510 - val_mse: 0.0042\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0459 - mse: 0.0036 - val_loss: 0.0042 - val_mae: 0.0476 - val_mse: 0.0042\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0037 - mae: 0.0459 - mse: 0.0037 - val_loss: 0.0032 - val_mae: 0.0423 - val_mse: 0.0032\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0034 - mae: 0.0441 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0439 - val_mse: 0.0034\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0457 - mse: 0.0036 - val_loss: 0.0035 - val_mae: 0.0436 - val_mse: 0.0035\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0449 - mse: 0.0035 - val_loss: 0.0039 - val_mae: 0.0491 - val_mse: 0.0039\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0450 - mse: 0.0035 - val_loss: 0.0030 - val_mae: 0.0403 - val_mse: 0.0030\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0448 - mse: 0.0035 - val_loss: 0.0036 - val_mae: 0.0459 - val_mse: 0.0036\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0442 - mse: 0.0034 - val_loss: 0.0030 - val_mae: 0.0410 - val_mse: 0.0030\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0431 - mse: 0.0032 - val_loss: 0.0037 - val_mae: 0.0484 - val_mse: 0.0037\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0448 - mse: 0.0035 - val_loss: 0.0029 - val_mae: 0.0402 - val_mse: 0.0029\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0434 - mse: 0.0033 - val_loss: 0.0050 - val_mae: 0.0565 - val_mse: 0.0050\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0428 - mse: 0.0031 - val_loss: 0.0030 - val_mae: 0.0406 - val_mse: 0.0030\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0438 - mse: 0.0033 - val_loss: 0.0043 - val_mae: 0.0511 - val_mse: 0.0043\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0431 - mse: 0.0032 - val_loss: 0.0031 - val_mae: 0.0417 - val_mse: 0.0031\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0434 - mse: 0.0033 - val_loss: 0.0030 - val_mae: 0.0398 - val_mse: 0.0030\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0425 - mse: 0.0031 - val_loss: 0.0027 - val_mae: 0.0394 - val_mse: 0.0027\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0421 - mse: 0.0031 - val_loss: 0.0039 - val_mae: 0.0494 - val_mse: 0.0039\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0433 - mse: 0.0032 - val_loss: 0.0056 - val_mae: 0.0625 - val_mse: 0.0056\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0425 - mse: 0.0031 - val_loss: 0.0027 - val_mae: 0.0390 - val_mse: 0.0027\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0417 - mse: 0.0030 - val_loss: 0.0029 - val_mae: 0.0398 - val_mse: 0.0029\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0418 - mse: 0.0030 - val_loss: 0.0027 - val_mae: 0.0381 - val_mse: 0.0027\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0423 - mse: 0.0031 - val_loss: 0.0029 - val_mae: 0.0418 - val_mse: 0.0029\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0427 - mse: 0.0031 - val_loss: 0.0029 - val_mae: 0.0402 - val_mse: 0.0029\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0413 - mse: 0.0030 - val_loss: 0.0032 - val_mae: 0.0437 - val_mse: 0.0032\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0414 - mse: 0.0030 - val_loss: 0.0035 - val_mae: 0.0446 - val_mse: 0.0035\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0427 - mse: 0.0031 - val_loss: 0.0028 - val_mae: 0.0387 - val_mse: 0.0028\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0423 - mse: 0.0031 - val_loss: 0.0035 - val_mae: 0.0456 - val_mse: 0.0035\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0416 - mse: 0.0030 - val_loss: 0.0027 - val_mae: 0.0384 - val_mse: 0.0027\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0423 - mse: 0.0031 - val_loss: 0.0029 - val_mae: 0.0411 - val_mse: 0.0029\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0411 - mse: 0.0029 - val_loss: 0.0039 - val_mae: 0.0505 - val_mse: 0.0039\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0419 - mse: 0.0030 - val_loss: 0.0031 - val_mae: 0.0426 - val_mse: 0.0031\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0423 - mse: 0.0030 - val_loss: 0.0028 - val_mae: 0.0391 - val_mse: 0.0028\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0423 - mse: 0.0031 - val_loss: 0.0044 - val_mae: 0.0525 - val_mse: 0.0044\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0411 - mse: 0.0029 - val_loss: 0.0030 - val_mae: 0.0405 - val_mse: 0.0030\n",
      "Epoch 66: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 1ms/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.006186951224535291    std:  0.05346225939582465    MAE:  0.040428475035020964     R2:  0.9754436261384721\n",
      "Test. mean:  -0.006544468932392797    std:  0.05394700989191795    MAE:  0.0405469616126524     R2:  0.9743281430117672\n",
      "\n",
      "Duration :  00:04:31 460ms\n",
      "\u001b[1maverage MAE of the training set:\u001b[0m   0.11 +/- 0.07\n",
      "\u001b[1maverage MAE of the validation set:\u001b[0m 0.11 +/- 0.07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#######################################################################################\n",
    "# optimization of the ANN\n",
    "# library used: keras and scikit learn for the KFold cross-validator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "VERBOSE = 1\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 25\n",
    "N_SPLIT = 5\n",
    "\n",
    "vID.chrono_start()\n",
    "\n",
    "# variables created to save at each iteration of the KFold process: the man error, the standard deviation, MAE, R2\n",
    "meantT=list()\n",
    "stdtT=list()\n",
    "MAEtT=list()\n",
    "R2tT=list()\n",
    "meanvT=list()\n",
    "stdvT=list()\n",
    "MAEvT=list()\n",
    "R2vT=list()\n",
    "\n",
    "kfold = KFold(n_splits=N_SPLIT,shuffle=True,random_state=42) # k-fold is here!\n",
    "#print(list(kfold.split(x_train,y_train)))\n",
    "\n",
    "j = 0 # Variable for keeping count of split we are executing\n",
    "# The KFold cv provides train/test indices to split data in train/test sets\n",
    "for train_idx, val_idx in list(kfold.split(xdata,ydata)):\n",
    "\n",
    "    x_train_cv = xdata.iloc[train_idx]\n",
    "    x_valid_cv = xdata.iloc[val_idx]\n",
    "    y_train_cv = ydata.iloc[train_idx]\n",
    "    y_valid_cv = ydata.iloc[val_idx]\n",
    "#    display(x_train_cv,x_valid_cv)\n",
    "# This part has been commented with respect to the original script\n",
    "    # scaler = preprocessing.StandardScaler()\n",
    "    # scaler.fit(x_train_cv.values)\n",
    "    # xt_scaled = scaler.transform(x_train_cv.values) #returns a numpy array\n",
    "    # xv_scaled = scaler.transform(x_valid_cv.values) #returns a numpy array\n",
    "    # x_train_cv = pd.DataFrame(xt_scaled, index=x_train_cv.index, columns=x_train_cv.columns)\n",
    "    # x_valid_cv = pd.DataFrame(xv_scaled, index=x_valid_cv.index, columns=x_valid_cv.columns)\n",
    "    # del xt_scaled, xv_scaled\n",
    "##############\n",
    "#    display(x_train_cv.describe().style.format(\"{0:.2f}\").set_caption(\"Training set after normalization (with scikit-learn):\"))\n",
    "#    display(x_valid_cv.describe().style.format(\"{0:.2f}\").set_caption(\"Validation set after normalization (with scikit-learn):\"))\n",
    "    print(f\"{color.BOLD}{color.RED}Fold {j}{color.OFF}\")\n",
    "    j+=1\n",
    "    ANNmodel=defANN( (53,), acthL )\n",
    "    ANNhistory = ANNmodel.fit(x_train_cv,\n",
    "                        y_train_cv,\n",
    "                        epochs          = EPOCHS,\n",
    "                        batch_size      = BATCH_SIZE,\n",
    "                        verbose         = VERBOSE,\n",
    "                        validation_data = (x_valid_cv, y_valid_cv),\n",
    "                        callbacks=[es])\n",
    "    ytrain_hat=ANNmodel.predict(x_train_cv)\n",
    "    yvalid_hat=ANNmodel.predict(x_valid_cv)\n",
    "    diffyt = ytrain_hat.ravel() - y_train_cv.ravel()\n",
    "    diffyv = yvalid_hat.ravel() - y_valid_cv.ravel()\n",
    "\n",
    "    print()\n",
    "    print(\"xCO2(predicted) - xCO2(actual)\")\n",
    "    print(\n",
    "          \"Train.\",\"mean: \", np.mean(diffyt),\n",
    "          \"   std: \", np.std(diffyt),\n",
    "          \"   MAE: \", np.average(abs(diffyt)),\n",
    "          \"    R2: \", np.corrcoef(y_train_cv.ravel(),ytrain_hat.ravel())[0,1]\n",
    "         )\n",
    "    print(\n",
    "          \"Test.\",\"mean: \", np.mean(diffyv),\n",
    "          \"   std: \", np.std(diffyv),\n",
    "          \"   MAE: \", np.average(abs(diffyv)),\n",
    "          \"    R2: \", np.corrcoef(y_valid_cv.ravel(),yvalid_hat.ravel())[0,1]\n",
    "         )\n",
    "    meantT.append(np.mean(diffyt))\n",
    "    meanvT.append(np.mean(diffyv))\n",
    "    stdtT.append(np.std(diffyt))\n",
    "    stdvT.append(np.std(diffyv))\n",
    "    MAEtT.append(np.average(abs(diffyt)))\n",
    "    MAEvT.append(np.average(abs(diffyv)))\n",
    "    R2tT.append(np.corrcoef(y_train_cv.ravel(),ytrain_hat.ravel())[0,1])\n",
    "    R2vT.append(np.corrcoef(y_valid_cv.ravel(),yvalid_hat.ravel())[0,1])\n",
    "    \n",
    "vID.chrono_show()\n",
    "\n",
    "#######################################################################################\n",
    "# accuracy of the ANN?\n",
    "# library used: numpy\n",
    "print(f\"{color.BOLD}average MAE of the training set:{color.OFF}   {np.mean(MAEtT):.2f} +/- {np.std(MAEtT):.2f}\")\n",
    "print(f\"{color.BOLD}average MAE of the validation set:{color.OFF} {np.mean(MAEvT):.2f} +/- {np.std(MAEvT):.2f}\")\n",
    "\n",
    "figCV, axCV = plt.subplots(1, 1)\n",
    "figCV.set_size_inches(12,5)\n",
    "axCV.errorbar(x=np.arange(len(meantT)), y=meantT, yerr=MAEtT, label='training sets', fmt='o-', capsize=10)\n",
    "axCV.errorbar(x=np.arange(len(meanvT))+0.1, y=meanvT, yerr=MAEvT, label='validation sets', fmt='o-', capsize=10)\n",
    "axCV.legend(loc='lower left', shadow=True, fontsize='14')\n",
    "axCV.set_xlabel('cross-validation k-values ',fontdict={'fontsize':16})\n",
    "axCV.set_ylabel('$\\hat{y}-y_{\\mathrm{actual}}$',fontdict={'fontsize':16})\n",
    "axCV.tick_params(labelsize = 14)\n",
    "plt.savefig('./CO2-images/KFold-cv-AppliedToSong_etal.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d639724-2e98-43dd-b6e5-7f0767cfb27d",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "You have probably found results close to those reported in the following error plot:\n",
    "<p style=\"text-align: center\"><img width=\"650px\" src=\"./CO2-images/KFold-cv-AppliedToSong_etalK.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_ResultsSong\"></p>\n",
    "    <b>This error plot shows a bad performance of the original ML algorithm of Song <i>et al</i>. (<i>i.e.</i> without standardization of the data), with a strong variation of error bars.</b>\n",
    "    \n",
    "Either the authors did actually apply a standardization preprocessing and they forgot to mention it in the article, or they ran several optimization algorithms of the ANN until they found a seemingly performant one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1998a632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**End at:** Thursday 30 June 2022, 12:35:06  \n",
       "**Duration:** 00:05:04 872ms"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"./svg/logoEnd.svg\" style=\"margin-left:auto; margin-right:auto\"/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vID.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb92c04-23d1-4c52-885d-fa8e5eee43c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
