{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10024ca9-7e94-4f5c-a982-b4537b919d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: bold;\n",
       "}\n",
       "body {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: 200;\n",
       "}\n",
       "h1 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 30px ;\n",
       "  color: white;\n",
       "  background: #b11d01;\n",
       "  text-align: center;\n",
       "}\n",
       "h2 {\n",
       "  border: 3px solid #333;\n",
       "  padding: 18px ;\n",
       "  color: #b11d01;\n",
       "  background: #ffffff;\n",
       "  text-align: center;\n",
       "}\n",
       "h3 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 12px ;\n",
       "  color: #000000;\n",
       "  background: #c1c1c1;\n",
       "  text-align: left;\n",
       "}\n",
       "h4 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #d9fffc;\n",
       "  text-align: left;\n",
       "}\n",
       "h5 {\n",
       "  border: 1px solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #ffffff;\n",
       "  text-align: left;\n",
       "}\n",
       ".warn {    \n",
       "    background-color: #fcf2f2;\n",
       "    border-color: #dFb5b4;\n",
       "    border-left: 5px solid #dfb5b4;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       ".rq {    \n",
       "    background-color: #e2e2e2;\n",
       "    border-color: #969696;\n",
       "    border-left: 5px solid #969696;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Start at:** Friday 24 June 2022, 09:20:16  \n",
       "**Hostname:** localhost.localdomain (Linux)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"./svg/logoPytChem.svg\" style=\"margin-left:auto; margin-right:auto\"/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import visualID_Eng as vID\n",
    "vID.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e4b9b",
   "metadata": {},
   "source": [
    "# Prediction by an artificial neural network of the solubility of CO<sub>2</sub> in ionic liquids\n",
    "\n",
    "<div class=\"rq\">\n",
    "<b>Reference</b>: \n",
    "Z. Song, H. Shi, X. Zhang & T. Zhou (**2020**), Prediction of CO<sub>2</sub> solubility in ionic liquids using machine learning methods, [<i>Chem. Eng. Sci.</i> <b>223</b>: 115752](https://www.doi.org/10.1016/j.ces.2020.115752) \n",
    "<br>\n",
    "<p style=\"text-align: center\"><img width=\"650px\" src=\"./CO2-images/AbstractANNCO2-SongEtal.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_AbstractSong\"></p>\n",
    "<br>\n",
    "The main results are graphically reported below.\n",
    "<br>\n",
    "<p style=\"text-align: center\"><img width=\"900px\" src=\"./CO2-images/ANNCO2-SongEtal-Results.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_ResultsSong\"></p>\n",
    "<br>\n",
    "Yet, it seems sthat no standardization process of the data has been applied. \n",
    "    \n",
    "<span style=\"color:red\">Moreover, a spurious separation of the data between training and test sets has been applied: \"<i>Instead of performing random selection, we employ a hybrid artificial-random strategy to decompose the dataset. Specifically, the data points consisting of the least frequently used groups are equally divided into five folders\"</i></span> \n",
    "<br><br>\n",
    "<b>It raises doubts about the stability of the algorithm developped in this paper (*unless the authors forgot to mention that data were standardized*).</b>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84860c4-3fcf-45f5-80a3-607ead68da80",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "<span style=\"font-weight:bold\">The goal of this exercise is to apply the <i>K</i>-fold cross-validation the ANN part of this article, <i>i.e.</i> without standardized data. </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af39c092-f4b7-460b-b698-32e0f2c55461",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os,sys\n",
    "from IPython.display import display\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   OFF = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89a89d-fb12-4ba1-8382-1609a0c4f848",
   "metadata": {},
   "source": [
    "<a id=\"data-read\"></a>\n",
    "## **1.** Database reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7d2251b-1f22-42f5-b930-e6032fe55170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IL</th>\n",
       "      <th>cation</th>\n",
       "      <th>anion</th>\n",
       "      <th>x_CO2</th>\n",
       "      <th>T (K)</th>\n",
       "      <th>P (bar)</th>\n",
       "      <th>[CH3]</th>\n",
       "      <th>[CH2]</th>\n",
       "      <th>[CH]</th>\n",
       "      <th>[OCH2]</th>\n",
       "      <th>...</th>\n",
       "      <th>[MeSO3]</th>\n",
       "      <th>[TfO]</th>\n",
       "      <th>[NfO]</th>\n",
       "      <th>[TDfO]</th>\n",
       "      <th>[TOS]</th>\n",
       "      <th>[C12PhSO3]</th>\n",
       "      <th>[DMPO4]</th>\n",
       "      <th>[DEPO4]</th>\n",
       "      <th>[DBPO4]</th>\n",
       "      <th>[methide]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>363.15</td>\n",
       "      <td>246.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.500</td>\n",
       "      <td>383.15</td>\n",
       "      <td>235.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>353.15</td>\n",
       "      <td>223.30</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.500</td>\n",
       "      <td>373.15</td>\n",
       "      <td>198.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>343.15</td>\n",
       "      <td>188.50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10111</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.592</td>\n",
       "      <td>298.15</td>\n",
       "      <td>35.86</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10112</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.239</td>\n",
       "      <td>343.15</td>\n",
       "      <td>27.54</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10113</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.396</td>\n",
       "      <td>298.15</td>\n",
       "      <td>20.15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10114</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.140</td>\n",
       "      <td>343.15</td>\n",
       "      <td>17.93</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10115</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.139</td>\n",
       "      <td>323.15</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10116 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 IL  cation   anion  x_CO2   T (K)  P (bar)  [CH3]  [CH2]  \\\n",
       "0       [BMIM][BF4]  [BMIM]   [BF4]  0.610  363.15   246.00      1      3   \n",
       "1       [BMIM][BF4]  [BMIM]   [BF4]  0.500  383.15   235.00      1      3   \n",
       "2       [BMIM][BF4]  [BMIM]   [BF4]  0.610  353.15   223.30      1      3   \n",
       "3       [BMIM][BF4]  [BMIM]   [BF4]  0.500  373.15   198.00      1      3   \n",
       "4       [BMIM][BF4]  [BMIM]   [BF4]  0.610  343.15   188.50      1      3   \n",
       "...             ...     ...     ...    ...     ...      ...    ...    ...   \n",
       "10111  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.592  298.15    35.86      1      5   \n",
       "10112  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.239  343.15    27.54      1      5   \n",
       "10113  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.396  298.15    20.15      1      5   \n",
       "10114  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.140  343.15    17.93      1      5   \n",
       "10115  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.139  323.15     8.00      1      5   \n",
       "\n",
       "       [CH]  [OCH2]  ...  [MeSO3]  [TfO]  [NfO]  [TDfO]  [TOS]  [C12PhSO3]  \\\n",
       "0         0       0  ...        0      0      0       0      0           0   \n",
       "1         0       0  ...        0      0      0       0      0           0   \n",
       "2         0       0  ...        0      0      0       0      0           0   \n",
       "3         0       0  ...        0      0      0       0      0           0   \n",
       "4         0       0  ...        0      0      0       0      0           0   \n",
       "...     ...     ...  ...      ...    ...    ...     ...    ...         ...   \n",
       "10111     0       0  ...        0      0      0       0      0           0   \n",
       "10112     0       0  ...        0      0      0       0      0           0   \n",
       "10113     0       0  ...        0      0      0       0      0           0   \n",
       "10114     0       0  ...        0      0      0       0      0           0   \n",
       "10115     0       0  ...        0      0      0       0      0           0   \n",
       "\n",
       "       [DMPO4]  [DEPO4]  [DBPO4]  [methide]  \n",
       "0            0        0        0          0  \n",
       "1            0        0        0          0  \n",
       "2            0        0        0          0  \n",
       "3            0        0        0          0  \n",
       "4            0        0        0          0  \n",
       "...        ...      ...      ...        ...  \n",
       "10111        0        0        0          0  \n",
       "10112        0        0        0          0  \n",
       "10113        0        0        0          0  \n",
       "10114        0        0        0          0  \n",
       "10115        0        0        0          0  \n",
       "\n",
       "[10116 rows x 57 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e10d6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e10d6_level0_col0\" class=\"col_heading level0 col0\" >x_CO2</th>\n",
       "      <th id=\"T_e10d6_level0_col1\" class=\"col_heading level0 col1\" >T (K)</th>\n",
       "      <th id=\"T_e10d6_level0_col2\" class=\"col_heading level0 col2\" >P (bar)</th>\n",
       "      <th id=\"T_e10d6_level0_col3\" class=\"col_heading level0 col3\" >[CH3]</th>\n",
       "      <th id=\"T_e10d6_level0_col4\" class=\"col_heading level0 col4\" >[CH2]</th>\n",
       "      <th id=\"T_e10d6_level0_col5\" class=\"col_heading level0 col5\" >[CH]</th>\n",
       "      <th id=\"T_e10d6_level0_col6\" class=\"col_heading level0 col6\" >[OCH2]</th>\n",
       "      <th id=\"T_e10d6_level0_col7\" class=\"col_heading level0 col7\" >[OCH3]</th>\n",
       "      <th id=\"T_e10d6_level0_col8\" class=\"col_heading level0 col8\" >[CF2]</th>\n",
       "      <th id=\"T_e10d6_level0_col9\" class=\"col_heading level0 col9\" >[CF3]</th>\n",
       "      <th id=\"T_e10d6_level0_col10\" class=\"col_heading level0 col10\" >[OH]</th>\n",
       "      <th id=\"T_e10d6_level0_col11\" class=\"col_heading level0 col11\" >CH=CH</th>\n",
       "      <th id=\"T_e10d6_level0_col12\" class=\"col_heading level0 col12\" >CH=CH2</th>\n",
       "      <th id=\"T_e10d6_level0_col13\" class=\"col_heading level0 col13\" >[Im13]</th>\n",
       "      <th id=\"T_e10d6_level0_col14\" class=\"col_heading level0 col14\" >[MIm]</th>\n",
       "      <th id=\"T_e10d6_level0_col15\" class=\"col_heading level0 col15\" >[MMIM]</th>\n",
       "      <th id=\"T_e10d6_level0_col16\" class=\"col_heading level0 col16\" >[Py]</th>\n",
       "      <th id=\"T_e10d6_level0_col17\" class=\"col_heading level0 col17\" >[MPy]</th>\n",
       "      <th id=\"T_e10d6_level0_col18\" class=\"col_heading level0 col18\" >[MPyrro]</th>\n",
       "      <th id=\"T_e10d6_level0_col19\" class=\"col_heading level0 col19\" >[MPip]</th>\n",
       "      <th id=\"T_e10d6_level0_col20\" class=\"col_heading level0 col20\" >[NH3]</th>\n",
       "      <th id=\"T_e10d6_level0_col21\" class=\"col_heading level0 col21\" >[NH2]</th>\n",
       "      <th id=\"T_e10d6_level0_col22\" class=\"col_heading level0 col22\" >[NH]</th>\n",
       "      <th id=\"T_e10d6_level0_col23\" class=\"col_heading level0 col23\" >[N]</th>\n",
       "      <th id=\"T_e10d6_level0_col24\" class=\"col_heading level0 col24\" >[P]</th>\n",
       "      <th id=\"T_e10d6_level0_col25\" class=\"col_heading level0 col25\" >[S]</th>\n",
       "      <th id=\"T_e10d6_level0_col26\" class=\"col_heading level0 col26\" >[BF4]</th>\n",
       "      <th id=\"T_e10d6_level0_col27\" class=\"col_heading level0 col27\" >[Cl]</th>\n",
       "      <th id=\"T_e10d6_level0_col28\" class=\"col_heading level0 col28\" >[DCA]</th>\n",
       "      <th id=\"T_e10d6_level0_col29\" class=\"col_heading level0 col29\" >[NO3]</th>\n",
       "      <th id=\"T_e10d6_level0_col30\" class=\"col_heading level0 col30\" >[PF6]</th>\n",
       "      <th id=\"T_e10d6_level0_col31\" class=\"col_heading level0 col31\" >[SCN]</th>\n",
       "      <th id=\"T_e10d6_level0_col32\" class=\"col_heading level0 col32\" >[TCB]</th>\n",
       "      <th id=\"T_e10d6_level0_col33\" class=\"col_heading level0 col33\" >[C(CN)3]</th>\n",
       "      <th id=\"T_e10d6_level0_col34\" class=\"col_heading level0 col34\" >[HSO4]</th>\n",
       "      <th id=\"T_e10d6_level0_col35\" class=\"col_heading level0 col35\" >[FSA]</th>\n",
       "      <th id=\"T_e10d6_level0_col36\" class=\"col_heading level0 col36\" >[Tf2N]</th>\n",
       "      <th id=\"T_e10d6_level0_col37\" class=\"col_heading level0 col37\" >[BETA]</th>\n",
       "      <th id=\"T_e10d6_level0_col38\" class=\"col_heading level0 col38\" >[FOR]</th>\n",
       "      <th id=\"T_e10d6_level0_col39\" class=\"col_heading level0 col39\" >[TFA]</th>\n",
       "      <th id=\"T_e10d6_level0_col40\" class=\"col_heading level0 col40\" >[C3F7CO2]</th>\n",
       "      <th id=\"T_e10d6_level0_col41\" class=\"col_heading level0 col41\" >[MeSO4]</th>\n",
       "      <th id=\"T_e10d6_level0_col42\" class=\"col_heading level0 col42\" >[EtSO4]</th>\n",
       "      <th id=\"T_e10d6_level0_col43\" class=\"col_heading level0 col43\" >[MDEGSO4]</th>\n",
       "      <th id=\"T_e10d6_level0_col44\" class=\"col_heading level0 col44\" >[MeSO3]</th>\n",
       "      <th id=\"T_e10d6_level0_col45\" class=\"col_heading level0 col45\" >[TfO]</th>\n",
       "      <th id=\"T_e10d6_level0_col46\" class=\"col_heading level0 col46\" >[NfO]</th>\n",
       "      <th id=\"T_e10d6_level0_col47\" class=\"col_heading level0 col47\" >[TDfO]</th>\n",
       "      <th id=\"T_e10d6_level0_col48\" class=\"col_heading level0 col48\" >[TOS]</th>\n",
       "      <th id=\"T_e10d6_level0_col49\" class=\"col_heading level0 col49\" >[C12PhSO3]</th>\n",
       "      <th id=\"T_e10d6_level0_col50\" class=\"col_heading level0 col50\" >[DMPO4]</th>\n",
       "      <th id=\"T_e10d6_level0_col51\" class=\"col_heading level0 col51\" >[DEPO4]</th>\n",
       "      <th id=\"T_e10d6_level0_col52\" class=\"col_heading level0 col52\" >[DBPO4]</th>\n",
       "      <th id=\"T_e10d6_level0_col53\" class=\"col_heading level0 col53\" >[methide]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e10d6_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_e10d6_row0_col0\" class=\"data row0 col0\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col1\" class=\"data row0 col1\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col2\" class=\"data row0 col2\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col3\" class=\"data row0 col3\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col4\" class=\"data row0 col4\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col5\" class=\"data row0 col5\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col6\" class=\"data row0 col6\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col7\" class=\"data row0 col7\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col8\" class=\"data row0 col8\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col9\" class=\"data row0 col9\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col10\" class=\"data row0 col10\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col11\" class=\"data row0 col11\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col12\" class=\"data row0 col12\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col13\" class=\"data row0 col13\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col14\" class=\"data row0 col14\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col15\" class=\"data row0 col15\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col16\" class=\"data row0 col16\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col17\" class=\"data row0 col17\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col18\" class=\"data row0 col18\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col19\" class=\"data row0 col19\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col20\" class=\"data row0 col20\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col21\" class=\"data row0 col21\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col22\" class=\"data row0 col22\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col23\" class=\"data row0 col23\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col24\" class=\"data row0 col24\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col25\" class=\"data row0 col25\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col26\" class=\"data row0 col26\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col27\" class=\"data row0 col27\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col28\" class=\"data row0 col28\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col29\" class=\"data row0 col29\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col30\" class=\"data row0 col30\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col31\" class=\"data row0 col31\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col32\" class=\"data row0 col32\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col33\" class=\"data row0 col33\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col34\" class=\"data row0 col34\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col35\" class=\"data row0 col35\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col36\" class=\"data row0 col36\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col37\" class=\"data row0 col37\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col38\" class=\"data row0 col38\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col39\" class=\"data row0 col39\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col40\" class=\"data row0 col40\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col41\" class=\"data row0 col41\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col42\" class=\"data row0 col42\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col43\" class=\"data row0 col43\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col44\" class=\"data row0 col44\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col45\" class=\"data row0 col45\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col46\" class=\"data row0 col46\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col47\" class=\"data row0 col47\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col48\" class=\"data row0 col48\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col49\" class=\"data row0 col49\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col50\" class=\"data row0 col50\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col51\" class=\"data row0 col51\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col52\" class=\"data row0 col52\" >10116.00</td>\n",
       "      <td id=\"T_e10d6_row0_col53\" class=\"data row0 col53\" >10116.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e10d6_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_e10d6_row1_col0\" class=\"data row1 col0\" >0.33</td>\n",
       "      <td id=\"T_e10d6_row1_col1\" class=\"data row1 col1\" >325.27</td>\n",
       "      <td id=\"T_e10d6_row1_col2\" class=\"data row1 col2\" >54.21</td>\n",
       "      <td id=\"T_e10d6_row1_col3\" class=\"data row1 col3\" >1.18</td>\n",
       "      <td id=\"T_e10d6_row1_col4\" class=\"data row1 col4\" >4.72</td>\n",
       "      <td id=\"T_e10d6_row1_col5\" class=\"data row1 col5\" >0.02</td>\n",
       "      <td id=\"T_e10d6_row1_col6\" class=\"data row1 col6\" >0.02</td>\n",
       "      <td id=\"T_e10d6_row1_col7\" class=\"data row1 col7\" >0.04</td>\n",
       "      <td id=\"T_e10d6_row1_col8\" class=\"data row1 col8\" >0.04</td>\n",
       "      <td id=\"T_e10d6_row1_col9\" class=\"data row1 col9\" >0.01</td>\n",
       "      <td id=\"T_e10d6_row1_col10\" class=\"data row1 col10\" >0.06</td>\n",
       "      <td id=\"T_e10d6_row1_col11\" class=\"data row1 col11\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row1_col12\" class=\"data row1 col12\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row1_col13\" class=\"data row1 col13\" >0.01</td>\n",
       "      <td id=\"T_e10d6_row1_col14\" class=\"data row1 col14\" >0.77</td>\n",
       "      <td id=\"T_e10d6_row1_col15\" class=\"data row1 col15\" >0.01</td>\n",
       "      <td id=\"T_e10d6_row1_col16\" class=\"data row1 col16\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row1_col17\" class=\"data row1 col17\" >0.01</td>\n",
       "      <td id=\"T_e10d6_row1_col18\" class=\"data row1 col18\" >0.09</td>\n",
       "      <td id=\"T_e10d6_row1_col19\" class=\"data row1 col19\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row1_col20\" class=\"data row1 col20\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row1_col21\" class=\"data row1 col21\" >0.01</td>\n",
       "      <td id=\"T_e10d6_row1_col22\" class=\"data row1 col22\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row1_col23\" class=\"data row1 col23\" >0.02</td>\n",
       "      <td id=\"T_e10d6_row1_col24\" class=\"data row1 col24\" >0.05</td>\n",
       "      <td id=\"T_e10d6_row1_col25\" class=\"data row1 col25\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row1_col26\" class=\"data row1 col26\" >0.11</td>\n",
       "      <td id=\"T_e10d6_row1_col27\" class=\"data row1 col27\" >0.02</td>\n",
       "      <td id=\"T_e10d6_row1_col28\" class=\"data row1 col28\" >0.02</td>\n",
       "      <td id=\"T_e10d6_row1_col29\" class=\"data row1 col29\" >0.02</td>\n",
       "      <td id=\"T_e10d6_row1_col30\" class=\"data row1 col30\" >0.11</td>\n",
       "      <td id=\"T_e10d6_row1_col31\" class=\"data row1 col31\" >0.02</td>\n",
       "      <td id=\"T_e10d6_row1_col32\" class=\"data row1 col32\" >0.01</td>\n",
       "      <td id=\"T_e10d6_row1_col33\" class=\"data row1 col33\" >0.07</td>\n",
       "      <td id=\"T_e10d6_row1_col34\" class=\"data row1 col34\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row1_col35\" class=\"data row1 col35\" >0.01</td>\n",
       "      <td id=\"T_e10d6_row1_col36\" class=\"data row1 col36\" >0.43</td>\n",
       "      <td id=\"T_e10d6_row1_col37\" class=\"data row1 col37\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row1_col38\" class=\"data row1 col38\" >0.01</td>\n",
       "      <td id=\"T_e10d6_row1_col39\" class=\"data row1 col39\" >0.01</td>\n",
       "      <td id=\"T_e10d6_row1_col40\" class=\"data row1 col40\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row1_col41\" class=\"data row1 col41\" >0.02</td>\n",
       "      <td id=\"T_e10d6_row1_col42\" class=\"data row1 col42\" >0.01</td>\n",
       "      <td id=\"T_e10d6_row1_col43\" class=\"data row1 col43\" >0.01</td>\n",
       "      <td id=\"T_e10d6_row1_col44\" class=\"data row1 col44\" >0.02</td>\n",
       "      <td id=\"T_e10d6_row1_col45\" class=\"data row1 col45\" >0.05</td>\n",
       "      <td id=\"T_e10d6_row1_col46\" class=\"data row1 col46\" >0.01</td>\n",
       "      <td id=\"T_e10d6_row1_col47\" class=\"data row1 col47\" >0.01</td>\n",
       "      <td id=\"T_e10d6_row1_col48\" class=\"data row1 col48\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row1_col49\" class=\"data row1 col49\" >0.01</td>\n",
       "      <td id=\"T_e10d6_row1_col50\" class=\"data row1 col50\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row1_col51\" class=\"data row1 col51\" >0.01</td>\n",
       "      <td id=\"T_e10d6_row1_col52\" class=\"data row1 col52\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row1_col53\" class=\"data row1 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e10d6_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_e10d6_row2_col0\" class=\"data row2 col0\" >0.24</td>\n",
       "      <td id=\"T_e10d6_row2_col1\" class=\"data row2 col1\" >25.24</td>\n",
       "      <td id=\"T_e10d6_row2_col2\" class=\"data row2 col2\" >76.66</td>\n",
       "      <td id=\"T_e10d6_row2_col3\" class=\"data row2 col3\" >0.96</td>\n",
       "      <td id=\"T_e10d6_row2_col4\" class=\"data row2 col4\" >5.48</td>\n",
       "      <td id=\"T_e10d6_row2_col5\" class=\"data row2 col5\" >0.25</td>\n",
       "      <td id=\"T_e10d6_row2_col6\" class=\"data row2 col6\" >0.16</td>\n",
       "      <td id=\"T_e10d6_row2_col7\" class=\"data row2 col7\" >0.20</td>\n",
       "      <td id=\"T_e10d6_row2_col8\" class=\"data row2 col8\" >0.39</td>\n",
       "      <td id=\"T_e10d6_row2_col9\" class=\"data row2 col9\" >0.10</td>\n",
       "      <td id=\"T_e10d6_row2_col10\" class=\"data row2 col10\" >0.28</td>\n",
       "      <td id=\"T_e10d6_row2_col11\" class=\"data row2 col11\" >0.06</td>\n",
       "      <td id=\"T_e10d6_row2_col12\" class=\"data row2 col12\" >0.06</td>\n",
       "      <td id=\"T_e10d6_row2_col13\" class=\"data row2 col13\" >0.10</td>\n",
       "      <td id=\"T_e10d6_row2_col14\" class=\"data row2 col14\" >0.42</td>\n",
       "      <td id=\"T_e10d6_row2_col15\" class=\"data row2 col15\" >0.08</td>\n",
       "      <td id=\"T_e10d6_row2_col16\" class=\"data row2 col16\" >0.07</td>\n",
       "      <td id=\"T_e10d6_row2_col17\" class=\"data row2 col17\" >0.11</td>\n",
       "      <td id=\"T_e10d6_row2_col18\" class=\"data row2 col18\" >0.29</td>\n",
       "      <td id=\"T_e10d6_row2_col19\" class=\"data row2 col19\" >0.06</td>\n",
       "      <td id=\"T_e10d6_row2_col20\" class=\"data row2 col20\" >0.07</td>\n",
       "      <td id=\"T_e10d6_row2_col21\" class=\"data row2 col21\" >0.09</td>\n",
       "      <td id=\"T_e10d6_row2_col22\" class=\"data row2 col22\" >0.06</td>\n",
       "      <td id=\"T_e10d6_row2_col23\" class=\"data row2 col23\" >0.16</td>\n",
       "      <td id=\"T_e10d6_row2_col24\" class=\"data row2 col24\" >0.23</td>\n",
       "      <td id=\"T_e10d6_row2_col25\" class=\"data row2 col25\" >0.06</td>\n",
       "      <td id=\"T_e10d6_row2_col26\" class=\"data row2 col26\" >0.31</td>\n",
       "      <td id=\"T_e10d6_row2_col27\" class=\"data row2 col27\" >0.12</td>\n",
       "      <td id=\"T_e10d6_row2_col28\" class=\"data row2 col28\" >0.15</td>\n",
       "      <td id=\"T_e10d6_row2_col29\" class=\"data row2 col29\" >0.12</td>\n",
       "      <td id=\"T_e10d6_row2_col30\" class=\"data row2 col30\" >0.31</td>\n",
       "      <td id=\"T_e10d6_row2_col31\" class=\"data row2 col31\" >0.14</td>\n",
       "      <td id=\"T_e10d6_row2_col32\" class=\"data row2 col32\" >0.08</td>\n",
       "      <td id=\"T_e10d6_row2_col33\" class=\"data row2 col33\" >0.26</td>\n",
       "      <td id=\"T_e10d6_row2_col34\" class=\"data row2 col34\" >0.04</td>\n",
       "      <td id=\"T_e10d6_row2_col35\" class=\"data row2 col35\" >0.11</td>\n",
       "      <td id=\"T_e10d6_row2_col36\" class=\"data row2 col36\" >0.49</td>\n",
       "      <td id=\"T_e10d6_row2_col37\" class=\"data row2 col37\" >0.03</td>\n",
       "      <td id=\"T_e10d6_row2_col38\" class=\"data row2 col38\" >0.11</td>\n",
       "      <td id=\"T_e10d6_row2_col39\" class=\"data row2 col39\" >0.11</td>\n",
       "      <td id=\"T_e10d6_row2_col40\" class=\"data row2 col40\" >0.05</td>\n",
       "      <td id=\"T_e10d6_row2_col41\" class=\"data row2 col41\" >0.13</td>\n",
       "      <td id=\"T_e10d6_row2_col42\" class=\"data row2 col42\" >0.11</td>\n",
       "      <td id=\"T_e10d6_row2_col43\" class=\"data row2 col43\" >0.10</td>\n",
       "      <td id=\"T_e10d6_row2_col44\" class=\"data row2 col44\" >0.15</td>\n",
       "      <td id=\"T_e10d6_row2_col45\" class=\"data row2 col45\" >0.23</td>\n",
       "      <td id=\"T_e10d6_row2_col46\" class=\"data row2 col46\" >0.09</td>\n",
       "      <td id=\"T_e10d6_row2_col47\" class=\"data row2 col47\" >0.08</td>\n",
       "      <td id=\"T_e10d6_row2_col48\" class=\"data row2 col48\" >0.06</td>\n",
       "      <td id=\"T_e10d6_row2_col49\" class=\"data row2 col49\" >0.10</td>\n",
       "      <td id=\"T_e10d6_row2_col50\" class=\"data row2 col50\" >0.03</td>\n",
       "      <td id=\"T_e10d6_row2_col51\" class=\"data row2 col51\" >0.07</td>\n",
       "      <td id=\"T_e10d6_row2_col52\" class=\"data row2 col52\" >0.04</td>\n",
       "      <td id=\"T_e10d6_row2_col53\" class=\"data row2 col53\" >0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e10d6_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_e10d6_row3_col0\" class=\"data row3 col0\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col1\" class=\"data row3 col1\" >243.20</td>\n",
       "      <td id=\"T_e10d6_row3_col2\" class=\"data row3 col2\" >0.01</td>\n",
       "      <td id=\"T_e10d6_row3_col3\" class=\"data row3 col3\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col4\" class=\"data row3 col4\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col5\" class=\"data row3 col5\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col6\" class=\"data row3 col6\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col7\" class=\"data row3 col7\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col8\" class=\"data row3 col8\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col9\" class=\"data row3 col9\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col10\" class=\"data row3 col10\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col11\" class=\"data row3 col11\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col12\" class=\"data row3 col12\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col13\" class=\"data row3 col13\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col14\" class=\"data row3 col14\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col15\" class=\"data row3 col15\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col16\" class=\"data row3 col16\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col17\" class=\"data row3 col17\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col18\" class=\"data row3 col18\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col19\" class=\"data row3 col19\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col20\" class=\"data row3 col20\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col21\" class=\"data row3 col21\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col22\" class=\"data row3 col22\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col23\" class=\"data row3 col23\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col24\" class=\"data row3 col24\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col25\" class=\"data row3 col25\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col26\" class=\"data row3 col26\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col27\" class=\"data row3 col27\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col28\" class=\"data row3 col28\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col29\" class=\"data row3 col29\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col30\" class=\"data row3 col30\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col31\" class=\"data row3 col31\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col32\" class=\"data row3 col32\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col33\" class=\"data row3 col33\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col34\" class=\"data row3 col34\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col35\" class=\"data row3 col35\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col36\" class=\"data row3 col36\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col37\" class=\"data row3 col37\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col38\" class=\"data row3 col38\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col39\" class=\"data row3 col39\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col40\" class=\"data row3 col40\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col41\" class=\"data row3 col41\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col42\" class=\"data row3 col42\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col43\" class=\"data row3 col43\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col44\" class=\"data row3 col44\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col45\" class=\"data row3 col45\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col46\" class=\"data row3 col46\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col47\" class=\"data row3 col47\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col48\" class=\"data row3 col48\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col49\" class=\"data row3 col49\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col50\" class=\"data row3 col50\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col51\" class=\"data row3 col51\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col52\" class=\"data row3 col52\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row3_col53\" class=\"data row3 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e10d6_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_e10d6_row4_col0\" class=\"data row4 col0\" >0.14</td>\n",
       "      <td id=\"T_e10d6_row4_col1\" class=\"data row4 col1\" >308.15</td>\n",
       "      <td id=\"T_e10d6_row4_col2\" class=\"data row4 col2\" >10.00</td>\n",
       "      <td id=\"T_e10d6_row4_col3\" class=\"data row4 col3\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row4_col4\" class=\"data row4 col4\" >3.00</td>\n",
       "      <td id=\"T_e10d6_row4_col5\" class=\"data row4 col5\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col6\" class=\"data row4 col6\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col7\" class=\"data row4 col7\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col8\" class=\"data row4 col8\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col9\" class=\"data row4 col9\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col10\" class=\"data row4 col10\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col11\" class=\"data row4 col11\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col12\" class=\"data row4 col12\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col13\" class=\"data row4 col13\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col14\" class=\"data row4 col14\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row4_col15\" class=\"data row4 col15\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col16\" class=\"data row4 col16\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col17\" class=\"data row4 col17\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col18\" class=\"data row4 col18\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col19\" class=\"data row4 col19\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col20\" class=\"data row4 col20\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col21\" class=\"data row4 col21\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col22\" class=\"data row4 col22\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col23\" class=\"data row4 col23\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col24\" class=\"data row4 col24\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col25\" class=\"data row4 col25\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col26\" class=\"data row4 col26\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col27\" class=\"data row4 col27\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col28\" class=\"data row4 col28\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col29\" class=\"data row4 col29\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col30\" class=\"data row4 col30\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col31\" class=\"data row4 col31\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col32\" class=\"data row4 col32\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col33\" class=\"data row4 col33\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col34\" class=\"data row4 col34\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col35\" class=\"data row4 col35\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col36\" class=\"data row4 col36\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col37\" class=\"data row4 col37\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col38\" class=\"data row4 col38\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col39\" class=\"data row4 col39\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col40\" class=\"data row4 col40\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col41\" class=\"data row4 col41\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col42\" class=\"data row4 col42\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col43\" class=\"data row4 col43\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col44\" class=\"data row4 col44\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col45\" class=\"data row4 col45\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col46\" class=\"data row4 col46\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col47\" class=\"data row4 col47\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col48\" class=\"data row4 col48\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col49\" class=\"data row4 col49\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col50\" class=\"data row4 col50\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col51\" class=\"data row4 col51\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col52\" class=\"data row4 col52\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row4_col53\" class=\"data row4 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e10d6_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_e10d6_row5_col0\" class=\"data row5 col0\" >0.30</td>\n",
       "      <td id=\"T_e10d6_row5_col1\" class=\"data row5 col1\" >323.15</td>\n",
       "      <td id=\"T_e10d6_row5_col2\" class=\"data row5 col2\" >26.80</td>\n",
       "      <td id=\"T_e10d6_row5_col3\" class=\"data row5 col3\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row5_col4\" class=\"data row5 col4\" >3.00</td>\n",
       "      <td id=\"T_e10d6_row5_col5\" class=\"data row5 col5\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col6\" class=\"data row5 col6\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col7\" class=\"data row5 col7\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col8\" class=\"data row5 col8\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col9\" class=\"data row5 col9\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col10\" class=\"data row5 col10\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col11\" class=\"data row5 col11\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col12\" class=\"data row5 col12\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col13\" class=\"data row5 col13\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col14\" class=\"data row5 col14\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row5_col15\" class=\"data row5 col15\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col16\" class=\"data row5 col16\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col17\" class=\"data row5 col17\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col18\" class=\"data row5 col18\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col19\" class=\"data row5 col19\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col20\" class=\"data row5 col20\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col21\" class=\"data row5 col21\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col22\" class=\"data row5 col22\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col23\" class=\"data row5 col23\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col24\" class=\"data row5 col24\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col25\" class=\"data row5 col25\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col26\" class=\"data row5 col26\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col27\" class=\"data row5 col27\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col28\" class=\"data row5 col28\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col29\" class=\"data row5 col29\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col30\" class=\"data row5 col30\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col31\" class=\"data row5 col31\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col32\" class=\"data row5 col32\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col33\" class=\"data row5 col33\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col34\" class=\"data row5 col34\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col35\" class=\"data row5 col35\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col36\" class=\"data row5 col36\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col37\" class=\"data row5 col37\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col38\" class=\"data row5 col38\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col39\" class=\"data row5 col39\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col40\" class=\"data row5 col40\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col41\" class=\"data row5 col41\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col42\" class=\"data row5 col42\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col43\" class=\"data row5 col43\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col44\" class=\"data row5 col44\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col45\" class=\"data row5 col45\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col46\" class=\"data row5 col46\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col47\" class=\"data row5 col47\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col48\" class=\"data row5 col48\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col49\" class=\"data row5 col49\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col50\" class=\"data row5 col50\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col51\" class=\"data row5 col51\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col52\" class=\"data row5 col52\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row5_col53\" class=\"data row5 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e10d6_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_e10d6_row6_col0\" class=\"data row6 col0\" >0.51</td>\n",
       "      <td id=\"T_e10d6_row6_col1\" class=\"data row6 col1\" >342.59</td>\n",
       "      <td id=\"T_e10d6_row6_col2\" class=\"data row6 col2\" >64.76</td>\n",
       "      <td id=\"T_e10d6_row6_col3\" class=\"data row6 col3\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row6_col4\" class=\"data row6 col4\" >5.00</td>\n",
       "      <td id=\"T_e10d6_row6_col5\" class=\"data row6 col5\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col6\" class=\"data row6 col6\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col7\" class=\"data row6 col7\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col8\" class=\"data row6 col8\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col9\" class=\"data row6 col9\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col10\" class=\"data row6 col10\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col11\" class=\"data row6 col11\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col12\" class=\"data row6 col12\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col13\" class=\"data row6 col13\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col14\" class=\"data row6 col14\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row6_col15\" class=\"data row6 col15\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col16\" class=\"data row6 col16\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col17\" class=\"data row6 col17\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col18\" class=\"data row6 col18\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col19\" class=\"data row6 col19\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col20\" class=\"data row6 col20\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col21\" class=\"data row6 col21\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col22\" class=\"data row6 col22\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col23\" class=\"data row6 col23\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col24\" class=\"data row6 col24\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col25\" class=\"data row6 col25\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col26\" class=\"data row6 col26\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col27\" class=\"data row6 col27\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col28\" class=\"data row6 col28\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col29\" class=\"data row6 col29\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col30\" class=\"data row6 col30\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col31\" class=\"data row6 col31\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col32\" class=\"data row6 col32\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col33\" class=\"data row6 col33\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col34\" class=\"data row6 col34\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col35\" class=\"data row6 col35\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col36\" class=\"data row6 col36\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row6_col37\" class=\"data row6 col37\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col38\" class=\"data row6 col38\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col39\" class=\"data row6 col39\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col40\" class=\"data row6 col40\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col41\" class=\"data row6 col41\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col42\" class=\"data row6 col42\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col43\" class=\"data row6 col43\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col44\" class=\"data row6 col44\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col45\" class=\"data row6 col45\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col46\" class=\"data row6 col46\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col47\" class=\"data row6 col47\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col48\" class=\"data row6 col48\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col49\" class=\"data row6 col49\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col50\" class=\"data row6 col50\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col51\" class=\"data row6 col51\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col52\" class=\"data row6 col52\" >0.00</td>\n",
       "      <td id=\"T_e10d6_row6_col53\" class=\"data row6 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e10d6_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_e10d6_row7_col0\" class=\"data row7 col0\" >0.95</td>\n",
       "      <td id=\"T_e10d6_row7_col1\" class=\"data row7 col1\" >453.15</td>\n",
       "      <td id=\"T_e10d6_row7_col2\" class=\"data row7 col2\" >499.90</td>\n",
       "      <td id=\"T_e10d6_row7_col3\" class=\"data row7 col3\" >7.00</td>\n",
       "      <td id=\"T_e10d6_row7_col4\" class=\"data row7 col4\" >28.00</td>\n",
       "      <td id=\"T_e10d6_row7_col5\" class=\"data row7 col5\" >3.00</td>\n",
       "      <td id=\"T_e10d6_row7_col6\" class=\"data row7 col6\" >2.00</td>\n",
       "      <td id=\"T_e10d6_row7_col7\" class=\"data row7 col7\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col8\" class=\"data row7 col8\" >5.00</td>\n",
       "      <td id=\"T_e10d6_row7_col9\" class=\"data row7 col9\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col10\" class=\"data row7 col10\" >3.00</td>\n",
       "      <td id=\"T_e10d6_row7_col11\" class=\"data row7 col11\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col12\" class=\"data row7 col12\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col13\" class=\"data row7 col13\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col14\" class=\"data row7 col14\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col15\" class=\"data row7 col15\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col16\" class=\"data row7 col16\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col17\" class=\"data row7 col17\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col18\" class=\"data row7 col18\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col19\" class=\"data row7 col19\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col20\" class=\"data row7 col20\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col21\" class=\"data row7 col21\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col22\" class=\"data row7 col22\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col23\" class=\"data row7 col23\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col24\" class=\"data row7 col24\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col25\" class=\"data row7 col25\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col26\" class=\"data row7 col26\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col27\" class=\"data row7 col27\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col28\" class=\"data row7 col28\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col29\" class=\"data row7 col29\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col30\" class=\"data row7 col30\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col31\" class=\"data row7 col31\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col32\" class=\"data row7 col32\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col33\" class=\"data row7 col33\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col34\" class=\"data row7 col34\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col35\" class=\"data row7 col35\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col36\" class=\"data row7 col36\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col37\" class=\"data row7 col37\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col38\" class=\"data row7 col38\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col39\" class=\"data row7 col39\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col40\" class=\"data row7 col40\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col41\" class=\"data row7 col41\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col42\" class=\"data row7 col42\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col43\" class=\"data row7 col43\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col44\" class=\"data row7 col44\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col45\" class=\"data row7 col45\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col46\" class=\"data row7 col46\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col47\" class=\"data row7 col47\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col48\" class=\"data row7 col48\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col49\" class=\"data row7 col49\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col50\" class=\"data row7 col50\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col51\" class=\"data row7 col51\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col52\" class=\"data row7 col52\" >1.00</td>\n",
       "      <td id=\"T_e10d6_row7_col53\" class=\"data row7 col53\" >1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa3d8662670>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataCO2f='CO2-data'+'/'+'dataCO2.csv'\n",
    "dataCO2=pd.read_csv(dataCO2f,sep=\";\",header=0)\n",
    "display(dataCO2)\n",
    "# describe() generates descriptive statistics\n",
    "display(dataCO2.describe().style.format(\"{0:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf1e66-0756-47f2-928d-d0d578613bba",
   "metadata": {},
   "source": [
    "## 2. Assessment of the stability of the original ML algorithm of Song *et al*. by *K*-fold cross validation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b3ba1f-2f90-441f-b931-28bc9e64748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# separation of the data set into two subsets: (1) training of the ANN & (2) test of the ANN\n",
    "# library used: pandas\n",
    "xdata = dataCO2.drop(['IL','cation','anion','x_CO2'],axis=1)\n",
    "ydata = dataCO2['x_CO2']\n",
    "\n",
    "#######################################################################################\n",
    "# ANN: 1 input layer (53 neurons) / 2 hidden layers (20 and 7 neurons) / 1 output layer (1 neuron) \n",
    "# library used: keras\n",
    "\n",
    "def defANN(shape,acthL):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name='iLayer'))\n",
    "    model.add(keras.layers.Dense(7, activation=acthL, name='hLayer'))\n",
    "    model.add(keras.layers.Dense(1, name='oLayer'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "acthL='tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a3138b7-5dc7-4804-93cb-caab9887319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[91mFold 0\u001b[0m\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 09:20:23.857190: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-06-24 09:20:23.857560: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-24 09:20:23.858098: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\n",
      "2022-06-24 09:20:23.959569: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-06-24 09:20:23.960023: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2099940000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 2s 4ms/step - loss: 0.3648 - mae: 0.4700 - mse: 0.3648 - val_loss: 0.0304 - val_mae: 0.1436 - val_mse: 0.0304\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0284 - mae: 0.1381 - mse: 0.0284 - val_loss: 0.0280 - val_mae: 0.1377 - val_mse: 0.0280\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0261 - mae: 0.1311 - mse: 0.0261 - val_loss: 0.0247 - val_mae: 0.1249 - val_mse: 0.0247\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 0s 1ms/step - loss: 0.0233 - mae: 0.1208 - mse: 0.0233 - val_loss: 0.0205 - val_mae: 0.1141 - val_mse: 0.0205\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0199 - mae: 0.1114 - mse: 0.0199 - val_loss: 0.0178 - val_mae: 0.1067 - val_mse: 0.0178\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0188 - mae: 0.1092 - mse: 0.0188 - val_loss: 0.0175 - val_mae: 0.1053 - val_mse: 0.0175\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 0s 2ms/step - loss: 0.0156 - mae: 0.0993 - mse: 0.0156 - val_loss: 0.0130 - val_mae: 0.0901 - val_mse: 0.0130\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0119 - mae: 0.0853 - mse: 0.0119 - val_loss: 0.0113 - val_mae: 0.0823 - val_mse: 0.0113\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 0s 2ms/step - loss: 0.0111 - mae: 0.0818 - mse: 0.0111 - val_loss: 0.0106 - val_mae: 0.0803 - val_mse: 0.0106\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 0s 2ms/step - loss: 0.0105 - mae: 0.0789 - mse: 0.0105 - val_loss: 0.0107 - val_mae: 0.0817 - val_mse: 0.0107\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 0s 2ms/step - loss: 0.0097 - mae: 0.0758 - mse: 0.0097 - val_loss: 0.0077 - val_mae: 0.0655 - val_mse: 0.0077\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 0s 2ms/step - loss: 0.0088 - mae: 0.0716 - mse: 0.0088 - val_loss: 0.0091 - val_mae: 0.0729 - val_mse: 0.0091\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0731 - mse: 0.0092 - val_loss: 0.0085 - val_mae: 0.0714 - val_mse: 0.0085\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0079 - mae: 0.0674 - mse: 0.0079 - val_loss: 0.0065 - val_mae: 0.0601 - val_mse: 0.0065\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0075 - mae: 0.0656 - mse: 0.0075 - val_loss: 0.0102 - val_mae: 0.0822 - val_mse: 0.0102\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0075 - mae: 0.0657 - mse: 0.0075 - val_loss: 0.0079 - val_mae: 0.0681 - val_mse: 0.0079\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0641 - mse: 0.0071 - val_loss: 0.0066 - val_mae: 0.0601 - val_mse: 0.0066\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0616 - mse: 0.0067 - val_loss: 0.0055 - val_mae: 0.0561 - val_mse: 0.0055\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0598 - mse: 0.0062 - val_loss: 0.0054 - val_mae: 0.0546 - val_mse: 0.0054\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0583 - mse: 0.0061 - val_loss: 0.0053 - val_mae: 0.0544 - val_mse: 0.0053\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0576 - mse: 0.0058 - val_loss: 0.0057 - val_mae: 0.0569 - val_mse: 0.0057\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0600 - mse: 0.0061 - val_loss: 0.0052 - val_mae: 0.0540 - val_mse: 0.0052\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0587 - mse: 0.0059 - val_loss: 0.0050 - val_mae: 0.0534 - val_mse: 0.0050\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0055 - mae: 0.0560 - mse: 0.0055 - val_loss: 0.0051 - val_mae: 0.0533 - val_mse: 0.0051\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0054 - mae: 0.0556 - mse: 0.0054 - val_loss: 0.0056 - val_mae: 0.0568 - val_mse: 0.0056\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0054 - mae: 0.0553 - mse: 0.0054 - val_loss: 0.0053 - val_mae: 0.0568 - val_mse: 0.0053\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0051 - mae: 0.0542 - mse: 0.0051 - val_loss: 0.0047 - val_mae: 0.0513 - val_mse: 0.0047\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0535 - mse: 0.0050 - val_loss: 0.0050 - val_mae: 0.0531 - val_mse: 0.0050\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0053 - mae: 0.0548 - mse: 0.0053 - val_loss: 0.0066 - val_mae: 0.0652 - val_mse: 0.0066\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 0s 2ms/step - loss: 0.0050 - mae: 0.0536 - mse: 0.0050 - val_loss: 0.0043 - val_mae: 0.0492 - val_mse: 0.0043\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0515 - mse: 0.0047 - val_loss: 0.0050 - val_mae: 0.0547 - val_mse: 0.0050\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0510 - mse: 0.0046 - val_loss: 0.0043 - val_mae: 0.0504 - val_mse: 0.0043\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0511 - mse: 0.0046 - val_loss: 0.0045 - val_mae: 0.0508 - val_mse: 0.0045\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0510 - mse: 0.0045 - val_loss: 0.0047 - val_mae: 0.0541 - val_mse: 0.0047\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0509 - mse: 0.0045 - val_loss: 0.0040 - val_mae: 0.0488 - val_mse: 0.0040\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0043 - mae: 0.0498 - mse: 0.0043 - val_loss: 0.0040 - val_mae: 0.0480 - val_mse: 0.0040\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0044 - mae: 0.0516 - mse: 0.0044 - val_loss: 0.0043 - val_mae: 0.0489 - val_mse: 0.0043\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0042 - mae: 0.0492 - mse: 0.0042 - val_loss: 0.0043 - val_mae: 0.0496 - val_mse: 0.0043\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0042 - mae: 0.0494 - mse: 0.0042 - val_loss: 0.0037 - val_mae: 0.0462 - val_mse: 0.0037\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0041 - mae: 0.0484 - mse: 0.0041 - val_loss: 0.0041 - val_mae: 0.0471 - val_mse: 0.0041\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0041 - mae: 0.0487 - mse: 0.0041 - val_loss: 0.0040 - val_mae: 0.0474 - val_mse: 0.0040\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 0s 2ms/step - loss: 0.0040 - mae: 0.0480 - mse: 0.0040 - val_loss: 0.0038 - val_mae: 0.0458 - val_mse: 0.0038\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 0s 2ms/step - loss: 0.0043 - mae: 0.0499 - mse: 0.0043 - val_loss: 0.0039 - val_mae: 0.0473 - val_mse: 0.0039\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0515 - mse: 0.0045 - val_loss: 0.0040 - val_mae: 0.0479 - val_mse: 0.0040\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 0s 2ms/step - loss: 0.0039 - mae: 0.0478 - mse: 0.0039 - val_loss: 0.0036 - val_mae: 0.0444 - val_mse: 0.0036\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0039 - mae: 0.0473 - mse: 0.0039 - val_loss: 0.0037 - val_mae: 0.0467 - val_mse: 0.0037\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0039 - mae: 0.0479 - mse: 0.0039 - val_loss: 0.0048 - val_mae: 0.0536 - val_mse: 0.0048\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0040 - mae: 0.0485 - mse: 0.0040 - val_loss: 0.0065 - val_mae: 0.0667 - val_mse: 0.0065\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0039 - mae: 0.0474 - mse: 0.0039 - val_loss: 0.0062 - val_mae: 0.0625 - val_mse: 0.0062\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0041 - mae: 0.0493 - mse: 0.0041 - val_loss: 0.0036 - val_mae: 0.0451 - val_mse: 0.0036\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0467 - mse: 0.0037 - val_loss: 0.0035 - val_mae: 0.0441 - val_mse: 0.0035\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0456 - mse: 0.0036 - val_loss: 0.0061 - val_mae: 0.0648 - val_mse: 0.0061\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0473 - mse: 0.0038 - val_loss: 0.0035 - val_mae: 0.0435 - val_mse: 0.0035\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0039 - mae: 0.0474 - mse: 0.0039 - val_loss: 0.0033 - val_mae: 0.0439 - val_mse: 0.0033\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0490 - mse: 0.0041 - val_loss: 0.0038 - val_mae: 0.0483 - val_mse: 0.0038\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0458 - mse: 0.0036 - val_loss: 0.0042 - val_mae: 0.0510 - val_mse: 0.0042\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0040 - mae: 0.0488 - mse: 0.0040 - val_loss: 0.0045 - val_mae: 0.0545 - val_mse: 0.0045\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0466 - mse: 0.0037 - val_loss: 0.0043 - val_mae: 0.0507 - val_mse: 0.0043\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0465 - mse: 0.0037 - val_loss: 0.0062 - val_mae: 0.0650 - val_mse: 0.0062\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 0s 2ms/step - loss: 0.0041 - mae: 0.0495 - mse: 0.0041 - val_loss: 0.0033 - val_mae: 0.0430 - val_mse: 0.0033\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 0s 2ms/step - loss: 0.0038 - mae: 0.0475 - mse: 0.0038 - val_loss: 0.0034 - val_mae: 0.0453 - val_mse: 0.0034\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0466 - mse: 0.0037 - val_loss: 0.0046 - val_mae: 0.0546 - val_mse: 0.0046\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0463 - mse: 0.0037 - val_loss: 0.0039 - val_mae: 0.0495 - val_mse: 0.0039\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0475 - mse: 0.0038 - val_loss: 0.0040 - val_mae: 0.0509 - val_mse: 0.0040\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0457 - mse: 0.0036 - val_loss: 0.0034 - val_mae: 0.0442 - val_mse: 0.0034\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0468 - mse: 0.0038 - val_loss: 0.0064 - val_mae: 0.0645 - val_mse: 0.0064\n",
      "Epoch 67/200\n",
      "324/324 [==============================] - 0s 2ms/step - loss: 0.0042 - mae: 0.0496 - mse: 0.0042 - val_loss: 0.0037 - val_mae: 0.0458 - val_mse: 0.0037\n",
      "Epoch 68/200\n",
      "324/324 [==============================] - 0s 2ms/step - loss: 0.0035 - mae: 0.0454 - mse: 0.0035 - val_loss: 0.0035 - val_mae: 0.0445 - val_mse: 0.0035\n",
      "Epoch 69/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0469 - mse: 0.0037 - val_loss: 0.0034 - val_mae: 0.0435 - val_mse: 0.0034\n",
      "Epoch 00069: early stopping\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.00972425350168398    std:  0.055749352795296615    MAE:  0.04214741731482149     R2:  0.9726544134245958\n",
      "Test. mean:  -0.012141064095183013    std:  0.05671443230463075    MAE:  0.043459038417186936     R2:  0.9726617711964917\n",
      "\u001b[1m\u001b[91mFold 1\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.1464 - mae: 0.2957 - mse: 0.1464 - val_loss: 0.0315 - val_mae: 0.1338 - val_mse: 0.0315\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0241 - mae: 0.1181 - mse: 0.0241 - val_loss: 0.0172 - val_mae: 0.0958 - val_mse: 0.0172\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0190 - mae: 0.0965 - mse: 0.0190 - val_loss: 0.0139 - val_mae: 0.0860 - val_mse: 0.0139\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0149 - mae: 0.0849 - mse: 0.0149 - val_loss: 0.0119 - val_mae: 0.0783 - val_mse: 0.0119\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0121 - mae: 0.0801 - mse: 0.0121 - val_loss: 0.0146 - val_mae: 0.0796 - val_mse: 0.0146\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0120 - mae: 0.0792 - mse: 0.0120 - val_loss: 0.0214 - val_mae: 0.0845 - val_mse: 0.0214\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0131 - mae: 0.0798 - mse: 0.0131 - val_loss: 0.0138 - val_mae: 0.0927 - val_mse: 0.0138\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0119 - mae: 0.0778 - mse: 0.0119 - val_loss: 0.0190 - val_mae: 0.0834 - val_mse: 0.0190\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0119 - mae: 0.0766 - mse: 0.0119 - val_loss: 0.0095 - val_mae: 0.0722 - val_mse: 0.0095\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0116 - mae: 0.0777 - mse: 0.0116 - val_loss: 0.0090 - val_mae: 0.0727 - val_mse: 0.0090\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0118 - mae: 0.0758 - mse: 0.0118 - val_loss: 0.0097 - val_mae: 0.0749 - val_mse: 0.0097\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0102 - mae: 0.0755 - mse: 0.0102 - val_loss: 0.0097 - val_mae: 0.0751 - val_mse: 0.0097\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0094 - mae: 0.0725 - mse: 0.0094 - val_loss: 0.0113 - val_mae: 0.0819 - val_mse: 0.0113\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0115 - mae: 0.0736 - mse: 0.0115 - val_loss: 0.0365 - val_mae: 0.0989 - val_mse: 0.0365\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0148 - mae: 0.0787 - mse: 0.0148 - val_loss: 0.0084 - val_mae: 0.0684 - val_mse: 0.0084\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0093 - mae: 0.0707 - mse: 0.0093 - val_loss: 0.0098 - val_mae: 0.0700 - val_mse: 0.0098\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0100 - mae: 0.0728 - mse: 0.0100 - val_loss: 0.0103 - val_mae: 0.0768 - val_mse: 0.0103\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0705 - mse: 0.0090 - val_loss: 0.0103 - val_mae: 0.0797 - val_mse: 0.0103\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0097 - mae: 0.0735 - mse: 0.0097 - val_loss: 0.0084 - val_mae: 0.0686 - val_mse: 0.0084\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0704 - mse: 0.0090 - val_loss: 0.0086 - val_mae: 0.0693 - val_mse: 0.0086\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0698 - mse: 0.0089 - val_loss: 0.0108 - val_mae: 0.0765 - val_mse: 0.0108\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0719 - mse: 0.0091 - val_loss: 0.0096 - val_mae: 0.0714 - val_mse: 0.0096\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0111 - mae: 0.0731 - mse: 0.0111 - val_loss: 0.0084 - val_mae: 0.0707 - val_mse: 0.0084\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0702 - mse: 0.0092 - val_loss: 0.0089 - val_mae: 0.0685 - val_mse: 0.0089\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0703 - mse: 0.0089 - val_loss: 0.0082 - val_mae: 0.0695 - val_mse: 0.0082\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0689 - mse: 0.0088 - val_loss: 0.0121 - val_mae: 0.0898 - val_mse: 0.0121\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0701 - mse: 0.0085 - val_loss: 0.0091 - val_mae: 0.0689 - val_mse: 0.0091\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0691 - mse: 0.0085 - val_loss: 0.0101 - val_mae: 0.0702 - val_mse: 0.0101\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0684 - mse: 0.0084 - val_loss: 0.0083 - val_mae: 0.0721 - val_mse: 0.0083\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0693 - mse: 0.0086 - val_loss: 0.0085 - val_mae: 0.0697 - val_mse: 0.0085\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0683 - mse: 0.0084 - val_loss: 0.0092 - val_mae: 0.0759 - val_mse: 0.0092\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0699 - mse: 0.0087 - val_loss: 0.0083 - val_mae: 0.0670 - val_mse: 0.0083\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0693 - mse: 0.0088 - val_loss: 0.0092 - val_mae: 0.0701 - val_mse: 0.0092\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0688 - mse: 0.0084 - val_loss: 0.0077 - val_mae: 0.0661 - val_mse: 0.0077\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0683 - mse: 0.0084 - val_loss: 0.0109 - val_mae: 0.0720 - val_mse: 0.0109\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0094 - mae: 0.0701 - mse: 0.0094 - val_loss: 0.0082 - val_mae: 0.0688 - val_mse: 0.0082\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0692 - mse: 0.0082 - val_loss: 0.0083 - val_mae: 0.0692 - val_mse: 0.0083\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0081 - mae: 0.0671 - mse: 0.0081 - val_loss: 0.0077 - val_mae: 0.0665 - val_mse: 0.0077\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0080 - mae: 0.0678 - mse: 0.0080 - val_loss: 0.0077 - val_mae: 0.0673 - val_mse: 0.0077\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0680 - mse: 0.0082 - val_loss: 0.0078 - val_mae: 0.0680 - val_mse: 0.0078\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0676 - mse: 0.0083 - val_loss: 0.0095 - val_mae: 0.0786 - val_mse: 0.0095\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0690 - mse: 0.0084 - val_loss: 0.0100 - val_mae: 0.0706 - val_mse: 0.0100\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0672 - mse: 0.0082 - val_loss: 0.0078 - val_mae: 0.0657 - val_mse: 0.0078\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0081 - mae: 0.0666 - mse: 0.0081 - val_loss: 0.0105 - val_mae: 0.0700 - val_mse: 0.0105\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0698 - mse: 0.0088 - val_loss: 0.0077 - val_mae: 0.0668 - val_mse: 0.0077\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0670 - mse: 0.0082 - val_loss: 0.0075 - val_mae: 0.0657 - val_mse: 0.0075\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0081 - mae: 0.0681 - mse: 0.0081 - val_loss: 0.0079 - val_mae: 0.0704 - val_mse: 0.0079\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0685 - mse: 0.0082 - val_loss: 0.0085 - val_mae: 0.0733 - val_mse: 0.0085\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0686 - mse: 0.0084 - val_loss: 0.0079 - val_mae: 0.0675 - val_mse: 0.0079\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0078 - mae: 0.0664 - mse: 0.0078 - val_loss: 0.0078 - val_mae: 0.0688 - val_mse: 0.0078\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0703 - mse: 0.0088 - val_loss: 0.0079 - val_mae: 0.0669 - val_mse: 0.0079\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0081 - mae: 0.0681 - mse: 0.0081 - val_loss: 0.0076 - val_mae: 0.0656 - val_mse: 0.0076\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0688 - mse: 0.0082 - val_loss: 0.0077 - val_mae: 0.0658 - val_mse: 0.0077\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0079 - mae: 0.0679 - mse: 0.0079 - val_loss: 0.0078 - val_mae: 0.0683 - val_mse: 0.0078\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0081 - mae: 0.0689 - mse: 0.0081 - val_loss: 0.0078 - val_mae: 0.0685 - val_mse: 0.0078\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0081 - mae: 0.0675 - mse: 0.0081 - val_loss: 0.0071 - val_mae: 0.0638 - val_mse: 0.0071\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0077 - mae: 0.0661 - mse: 0.0077 - val_loss: 0.0089 - val_mae: 0.0691 - val_mse: 0.0089\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0619 - mse: 0.0069 - val_loss: 0.0073 - val_mae: 0.0653 - val_mse: 0.0073\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0073 - mae: 0.0636 - mse: 0.0073 - val_loss: 0.0071 - val_mae: 0.0648 - val_mse: 0.0071\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0617 - mse: 0.0069 - val_loss: 0.0069 - val_mae: 0.0620 - val_mse: 0.0069\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0612 - mse: 0.0068 - val_loss: 0.0070 - val_mae: 0.0632 - val_mse: 0.0070\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0612 - mse: 0.0066 - val_loss: 0.0067 - val_mae: 0.0622 - val_mse: 0.0067\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0070 - mae: 0.0619 - mse: 0.0070 - val_loss: 0.0069 - val_mae: 0.0634 - val_mse: 0.0069\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0598 - mse: 0.0065 - val_loss: 0.0065 - val_mae: 0.0600 - val_mse: 0.0065\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0597 - mse: 0.0065 - val_loss: 0.0075 - val_mae: 0.0669 - val_mse: 0.0075\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0605 - mse: 0.0068 - val_loss: 0.0066 - val_mae: 0.0622 - val_mse: 0.0066\n",
      "Epoch 67/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0616 - mse: 0.0069 - val_loss: 0.0080 - val_mae: 0.0705 - val_mse: 0.0080\n",
      "Epoch 68/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0611 - mse: 0.0068 - val_loss: 0.0065 - val_mae: 0.0603 - val_mse: 0.0065\n",
      "Epoch 69/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0591 - mse: 0.0062 - val_loss: 0.0065 - val_mae: 0.0594 - val_mse: 0.0065\n",
      "Epoch 70/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0607 - mse: 0.0068 - val_loss: 0.0066 - val_mae: 0.0603 - val_mse: 0.0066\n",
      "Epoch 71/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0599 - mse: 0.0065 - val_loss: 0.0069 - val_mae: 0.0643 - val_mse: 0.0069\n",
      "Epoch 72/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0591 - mse: 0.0065 - val_loss: 0.0069 - val_mae: 0.0602 - val_mse: 0.0069\n",
      "Epoch 73/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0599 - mse: 0.0066 - val_loss: 0.0065 - val_mae: 0.0603 - val_mse: 0.0065\n",
      "Epoch 74/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0587 - mse: 0.0063 - val_loss: 0.0062 - val_mae: 0.0578 - val_mse: 0.0062\n",
      "Epoch 75/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0597 - mse: 0.0066 - val_loss: 0.0066 - val_mae: 0.0598 - val_mse: 0.0066\n",
      "Epoch 76/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0614 - mse: 0.0069 - val_loss: 0.0068 - val_mae: 0.0582 - val_mse: 0.0068\n",
      "Epoch 77/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0589 - mse: 0.0066 - val_loss: 0.0064 - val_mae: 0.0590 - val_mse: 0.0064\n",
      "Epoch 78/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0589 - mse: 0.0064 - val_loss: 0.0064 - val_mae: 0.0601 - val_mse: 0.0064\n",
      "Epoch 79/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0597 - mse: 0.0064 - val_loss: 0.0063 - val_mae: 0.0573 - val_mse: 0.0063\n",
      "Epoch 80/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0581 - mse: 0.0063 - val_loss: 0.0063 - val_mae: 0.0590 - val_mse: 0.0063\n",
      "Epoch 81/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0586 - mse: 0.0063 - val_loss: 0.0064 - val_mae: 0.0583 - val_mse: 0.0064\n",
      "Epoch 82/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0595 - mse: 0.0065 - val_loss: 0.0111 - val_mae: 0.0830 - val_mse: 0.0111\n",
      "Epoch 83/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0590 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0574 - val_mse: 0.0061\n",
      "Epoch 84/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0571 - mse: 0.0062 - val_loss: 0.0062 - val_mae: 0.0569 - val_mse: 0.0062\n",
      "Epoch 85/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0567 - mse: 0.0060 - val_loss: 0.0085 - val_mae: 0.0733 - val_mse: 0.0085\n",
      "Epoch 86/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0599 - mse: 0.0066 - val_loss: 0.0066 - val_mae: 0.0622 - val_mse: 0.0066\n",
      "Epoch 87/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0578 - mse: 0.0061 - val_loss: 0.0061 - val_mae: 0.0570 - val_mse: 0.0061\n",
      "Epoch 88/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0568 - mse: 0.0060 - val_loss: 0.0061 - val_mae: 0.0563 - val_mse: 0.0061\n",
      "Epoch 89/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0571 - mse: 0.0061 - val_loss: 0.0062 - val_mae: 0.0572 - val_mse: 0.0062\n",
      "Epoch 90/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0582 - mse: 0.0062 - val_loss: 0.0062 - val_mae: 0.0575 - val_mse: 0.0062\n",
      "Epoch 91/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0565 - mse: 0.0059 - val_loss: 0.0059 - val_mae: 0.0557 - val_mse: 0.0059\n",
      "Epoch 92/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0575 - mse: 0.0059 - val_loss: 0.0061 - val_mae: 0.0571 - val_mse: 0.0061\n",
      "Epoch 93/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0573 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0564 - val_mse: 0.0059\n",
      "Epoch 94/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0572 - mse: 0.0060 - val_loss: 0.0062 - val_mae: 0.0574 - val_mse: 0.0062\n",
      "Epoch 95/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0572 - mse: 0.0059 - val_loss: 0.0059 - val_mae: 0.0562 - val_mse: 0.0059\n",
      "Epoch 96/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0559 - mse: 0.0057 - val_loss: 0.0057 - val_mae: 0.0557 - val_mse: 0.0057\n",
      "Epoch 97/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0580 - mse: 0.0060 - val_loss: 0.0060 - val_mae: 0.0580 - val_mse: 0.0060\n",
      "Epoch 98/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0559 - mse: 0.0057 - val_loss: 0.0059 - val_mae: 0.0551 - val_mse: 0.0059\n",
      "Epoch 99/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0054 - mae: 0.0552 - mse: 0.0054 - val_loss: 0.0065 - val_mae: 0.0592 - val_mse: 0.0065\n",
      "Epoch 100/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0558 - mse: 0.0057 - val_loss: 0.0057 - val_mae: 0.0549 - val_mse: 0.0057\n",
      "Epoch 101/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0055 - mae: 0.0554 - mse: 0.0055 - val_loss: 0.0059 - val_mae: 0.0572 - val_mse: 0.0059\n",
      "Epoch 102/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0576 - mse: 0.0059 - val_loss: 0.0059 - val_mae: 0.0575 - val_mse: 0.0059\n",
      "Epoch 103/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0563 - mse: 0.0058 - val_loss: 0.0059 - val_mae: 0.0556 - val_mse: 0.0059\n",
      "Epoch 104/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0576 - mse: 0.0058 - val_loss: 0.0056 - val_mae: 0.0538 - val_mse: 0.0056\n",
      "Epoch 105/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0565 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0562 - val_mse: 0.0058\n",
      "Epoch 106/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0054 - mae: 0.0546 - mse: 0.0054 - val_loss: 0.0056 - val_mae: 0.0546 - val_mse: 0.0056\n",
      "Epoch 107/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0054 - mae: 0.0551 - mse: 0.0054 - val_loss: 0.0057 - val_mae: 0.0553 - val_mse: 0.0057\n",
      "Epoch 108/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0054 - mae: 0.0547 - mse: 0.0054 - val_loss: 0.0052 - val_mae: 0.0532 - val_mse: 0.0052\n",
      "Epoch 109/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0054 - mae: 0.0549 - mse: 0.0054 - val_loss: 0.0054 - val_mae: 0.0554 - val_mse: 0.0054\n",
      "Epoch 110/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0054 - mae: 0.0550 - mse: 0.0054 - val_loss: 0.0056 - val_mae: 0.0554 - val_mse: 0.0056\n",
      "Epoch 111/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0558 - mse: 0.0056 - val_loss: 0.0051 - val_mae: 0.0526 - val_mse: 0.0051\n",
      "Epoch 112/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0557 - mse: 0.0056 - val_loss: 0.0055 - val_mae: 0.0569 - val_mse: 0.0055\n",
      "Epoch 113/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0053 - mae: 0.0552 - mse: 0.0053 - val_loss: 0.0053 - val_mae: 0.0538 - val_mse: 0.0053\n",
      "Epoch 114/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0052 - mae: 0.0542 - mse: 0.0052 - val_loss: 0.0055 - val_mae: 0.0565 - val_mse: 0.0055\n",
      "Epoch 115/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0532 - mse: 0.0050 - val_loss: 0.0051 - val_mae: 0.0532 - val_mse: 0.0051\n",
      "Epoch 116/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0054 - mae: 0.0551 - mse: 0.0054 - val_loss: 0.0054 - val_mae: 0.0543 - val_mse: 0.0054\n",
      "Epoch 117/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0052 - mae: 0.0546 - mse: 0.0052 - val_loss: 0.0051 - val_mae: 0.0533 - val_mse: 0.0051\n",
      "Epoch 118/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0053 - mae: 0.0547 - mse: 0.0053 - val_loss: 0.0051 - val_mae: 0.0522 - val_mse: 0.0051\n",
      "Epoch 119/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0054 - mae: 0.0547 - mse: 0.0054 - val_loss: 0.0052 - val_mae: 0.0527 - val_mse: 0.0052\n",
      "Epoch 120/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0054 - mae: 0.0545 - mse: 0.0054 - val_loss: 0.0053 - val_mae: 0.0547 - val_mse: 0.0053\n",
      "Epoch 121/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0527 - mse: 0.0050 - val_loss: 0.0053 - val_mae: 0.0539 - val_mse: 0.0053\n",
      "Epoch 122/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0527 - mse: 0.0050 - val_loss: 0.0056 - val_mae: 0.0549 - val_mse: 0.0056\n",
      "Epoch 123/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0525 - mse: 0.0050 - val_loss: 0.0052 - val_mae: 0.0532 - val_mse: 0.0052\n",
      "Epoch 124/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0052 - mae: 0.0536 - mse: 0.0052 - val_loss: 0.0050 - val_mae: 0.0515 - val_mse: 0.0050\n",
      "Epoch 125/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0053 - mae: 0.0545 - mse: 0.0053 - val_loss: 0.0054 - val_mae: 0.0547 - val_mse: 0.0054\n",
      "Epoch 126/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0053 - mae: 0.0543 - mse: 0.0053 - val_loss: 0.0055 - val_mae: 0.0570 - val_mse: 0.0055\n",
      "Epoch 127/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0053 - mae: 0.0543 - mse: 0.0053 - val_loss: 0.0058 - val_mae: 0.0572 - val_mse: 0.0058\n",
      "Epoch 128/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0053 - mae: 0.0547 - mse: 0.0053 - val_loss: 0.0051 - val_mae: 0.0526 - val_mse: 0.0051\n",
      "Epoch 129/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0053 - mae: 0.0542 - mse: 0.0053 - val_loss: 0.0049 - val_mae: 0.0515 - val_mse: 0.0049\n",
      "Epoch 130/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0052 - mae: 0.0542 - mse: 0.0052 - val_loss: 0.0051 - val_mae: 0.0538 - val_mse: 0.0051\n",
      "Epoch 131/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0051 - mae: 0.0533 - mse: 0.0051 - val_loss: 0.0050 - val_mae: 0.0522 - val_mse: 0.0050\n",
      "Epoch 132/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0049 - mae: 0.0527 - mse: 0.0049 - val_loss: 0.0053 - val_mae: 0.0531 - val_mse: 0.0053\n",
      "Epoch 133/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0536 - mse: 0.0050 - val_loss: 0.0056 - val_mae: 0.0553 - val_mse: 0.0056\n",
      "Epoch 134/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0051 - mae: 0.0534 - mse: 0.0051 - val_loss: 0.0054 - val_mae: 0.0564 - val_mse: 0.0054\n",
      "Epoch 135/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0529 - mse: 0.0050 - val_loss: 0.0050 - val_mae: 0.0522 - val_mse: 0.0050\n",
      "Epoch 136/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0053 - mae: 0.0542 - mse: 0.0053 - val_loss: 0.0058 - val_mae: 0.0584 - val_mse: 0.0058\n",
      "Epoch 137/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0053 - mae: 0.0548 - mse: 0.0053 - val_loss: 0.0052 - val_mae: 0.0550 - val_mse: 0.0052\n",
      "Epoch 138/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0055 - mae: 0.0564 - mse: 0.0055 - val_loss: 0.0048 - val_mae: 0.0514 - val_mse: 0.0048\n",
      "Epoch 139/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0048 - mae: 0.0526 - mse: 0.0048 - val_loss: 0.0049 - val_mae: 0.0521 - val_mse: 0.0049\n",
      "Epoch 140/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0053 - mae: 0.0544 - mse: 0.0053 - val_loss: 0.0049 - val_mae: 0.0511 - val_mse: 0.0049\n",
      "Epoch 141/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0050 - mae: 0.0529 - mse: 0.0050 - val_loss: 0.0050 - val_mae: 0.0515 - val_mse: 0.0050\n",
      "Epoch 142/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0051 - mae: 0.0537 - mse: 0.0051 - val_loss: 0.0050 - val_mae: 0.0516 - val_mse: 0.0050\n",
      "Epoch 143/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0049 - mae: 0.0530 - mse: 0.0049 - val_loss: 0.0051 - val_mae: 0.0532 - val_mse: 0.0051\n",
      "Epoch 144/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0051 - mae: 0.0535 - mse: 0.0051 - val_loss: 0.0050 - val_mae: 0.0530 - val_mse: 0.0050\n",
      "Epoch 145/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0051 - mae: 0.0533 - mse: 0.0051 - val_loss: 0.0049 - val_mae: 0.0522 - val_mse: 0.0049\n",
      "Epoch 146/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0051 - mae: 0.0536 - mse: 0.0051 - val_loss: 0.0050 - val_mae: 0.0538 - val_mse: 0.0050\n",
      "Epoch 147/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0051 - mae: 0.0535 - mse: 0.0051 - val_loss: 0.0048 - val_mae: 0.0509 - val_mse: 0.0048\n",
      "Epoch 148/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0048 - mae: 0.0518 - mse: 0.0048 - val_loss: 0.0052 - val_mae: 0.0542 - val_mse: 0.0052\n",
      "Epoch 149/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0048 - mae: 0.0517 - mse: 0.0048 - val_loss: 0.0048 - val_mae: 0.0508 - val_mse: 0.0048\n",
      "Epoch 150/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0529 - mse: 0.0050 - val_loss: 0.0052 - val_mae: 0.0540 - val_mse: 0.0052\n",
      "Epoch 151/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0515 - mse: 0.0047 - val_loss: 0.0061 - val_mae: 0.0614 - val_mse: 0.0061\n",
      "Epoch 152/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0510 - mse: 0.0047 - val_loss: 0.0055 - val_mae: 0.0550 - val_mse: 0.0055\n",
      "Epoch 153/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0513 - mse: 0.0047 - val_loss: 0.0048 - val_mae: 0.0510 - val_mse: 0.0048\n",
      "Epoch 154/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0048 - mae: 0.0518 - mse: 0.0048 - val_loss: 0.0050 - val_mae: 0.0523 - val_mse: 0.0050\n",
      "Epoch 155/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0048 - mae: 0.0523 - mse: 0.0048 - val_loss: 0.0048 - val_mae: 0.0512 - val_mse: 0.0048\n",
      "Epoch 156/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0048 - mae: 0.0521 - mse: 0.0048 - val_loss: 0.0067 - val_mae: 0.0647 - val_mse: 0.0067\n",
      "Epoch 157/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0533 - mse: 0.0050 - val_loss: 0.0054 - val_mae: 0.0566 - val_mse: 0.0054\n",
      "Epoch 158/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0531 - mse: 0.0050 - val_loss: 0.0048 - val_mae: 0.0511 - val_mse: 0.0048\n",
      "Epoch 159/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0504 - mse: 0.0046 - val_loss: 0.0060 - val_mae: 0.0594 - val_mse: 0.0060\n",
      "Epoch 160/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0512 - mse: 0.0047 - val_loss: 0.0050 - val_mae: 0.0534 - val_mse: 0.0050\n",
      "Epoch 161/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0048 - mae: 0.0523 - mse: 0.0048 - val_loss: 0.0048 - val_mae: 0.0511 - val_mse: 0.0048\n",
      "Epoch 162/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0511 - mse: 0.0047 - val_loss: 0.0048 - val_mae: 0.0506 - val_mse: 0.0048\n",
      "Epoch 163/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0049 - mae: 0.0531 - mse: 0.0049 - val_loss: 0.0049 - val_mae: 0.0533 - val_mse: 0.0049\n",
      "Epoch 164/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0509 - mse: 0.0046 - val_loss: 0.0051 - val_mae: 0.0540 - val_mse: 0.0051\n",
      "Epoch 00164: early stopping\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  0.013051585121361583    std:  0.06822385468383906    MAE:  0.05213872956095786     R2:  0.9583308972030735\n",
      "Test. mean:  0.015039830995076733    std:  0.07003121125552252    MAE:  0.05398728786786572     R2:  0.9557482430858326\n",
      "\u001b[1m\u001b[91mFold 2\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.2843 - mae: 0.4136 - mse: 0.2843 - val_loss: 0.0266 - val_mae: 0.1220 - val_mse: 0.0266\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0223 - mae: 0.1146 - mse: 0.0223 - val_loss: 0.0159 - val_mae: 0.1000 - val_mse: 0.0159\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0171 - mae: 0.1019 - mse: 0.0171 - val_loss: 0.0144 - val_mae: 0.0946 - val_mse: 0.0144\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0138 - mae: 0.0899 - mse: 0.0138 - val_loss: 0.0098 - val_mae: 0.0761 - val_mse: 0.0098\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0107 - mae: 0.0795 - mse: 0.0107 - val_loss: 0.0104 - val_mae: 0.0783 - val_mse: 0.0104\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0713 - mse: 0.0090 - val_loss: 0.0073 - val_mae: 0.0632 - val_mse: 0.0073\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0683 - mse: 0.0084 - val_loss: 0.0069 - val_mae: 0.0624 - val_mse: 0.0069\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0076 - mae: 0.0642 - mse: 0.0076 - val_loss: 0.0065 - val_mae: 0.0580 - val_mse: 0.0065\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0619 - mse: 0.0072 - val_loss: 0.0062 - val_mae: 0.0558 - val_mse: 0.0062\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0591 - mse: 0.0065 - val_loss: 0.0066 - val_mae: 0.0602 - val_mse: 0.0066\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0569 - mse: 0.0062 - val_loss: 0.0049 - val_mae: 0.0495 - val_mse: 0.0049\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0552 - mse: 0.0057 - val_loss: 0.0044 - val_mae: 0.0489 - val_mse: 0.0044\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0531 - mse: 0.0050 - val_loss: 0.0042 - val_mae: 0.0485 - val_mse: 0.0042\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0520 - mse: 0.0047 - val_loss: 0.0047 - val_mae: 0.0557 - val_mse: 0.0047\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0044 - mae: 0.0505 - mse: 0.0044 - val_loss: 0.0031 - val_mae: 0.0407 - val_mse: 0.0031\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0040 - mae: 0.0482 - mse: 0.0040 - val_loss: 0.0047 - val_mae: 0.0523 - val_mse: 0.0047\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0041 - mae: 0.0485 - mse: 0.0041 - val_loss: 0.0032 - val_mae: 0.0422 - val_mse: 0.0032\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0470 - mse: 0.0038 - val_loss: 0.0039 - val_mae: 0.0514 - val_mse: 0.0039\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0450 - mse: 0.0036 - val_loss: 0.0026 - val_mae: 0.0391 - val_mse: 0.0026\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0453 - mse: 0.0036 - val_loss: 0.0041 - val_mae: 0.0513 - val_mse: 0.0041\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0453 - mse: 0.0036 - val_loss: 0.0051 - val_mae: 0.0572 - val_mse: 0.0051\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0446 - mse: 0.0035 - val_loss: 0.0027 - val_mae: 0.0402 - val_mse: 0.0027\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0460 - mse: 0.0036 - val_loss: 0.0028 - val_mae: 0.0397 - val_mse: 0.0028\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0455 - mse: 0.0035 - val_loss: 0.0033 - val_mae: 0.0465 - val_mse: 0.0033\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0451 - mse: 0.0035 - val_loss: 0.0031 - val_mae: 0.0435 - val_mse: 0.0031\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0432 - mse: 0.0032 - val_loss: 0.0028 - val_mae: 0.0405 - val_mse: 0.0028\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0445 - mse: 0.0035 - val_loss: 0.0032 - val_mae: 0.0464 - val_mse: 0.0032\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0445 - mse: 0.0034 - val_loss: 0.0025 - val_mae: 0.0371 - val_mse: 0.0025\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0463 - mse: 0.0038 - val_loss: 0.0028 - val_mae: 0.0418 - val_mse: 0.0028\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0426 - mse: 0.0032 - val_loss: 0.0073 - val_mae: 0.0723 - val_mse: 0.0073\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0449 - mse: 0.0034 - val_loss: 0.0030 - val_mae: 0.0412 - val_mse: 0.0030\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0428 - mse: 0.0032 - val_loss: 0.0031 - val_mae: 0.0432 - val_mse: 0.0031\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0431 - mse: 0.0033 - val_loss: 0.0028 - val_mae: 0.0420 - val_mse: 0.0028\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0428 - mse: 0.0031 - val_loss: 0.0031 - val_mae: 0.0448 - val_mse: 0.0031\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0441 - mse: 0.0034 - val_loss: 0.0030 - val_mae: 0.0434 - val_mse: 0.0030\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0436 - mse: 0.0034 - val_loss: 0.0029 - val_mae: 0.0421 - val_mse: 0.0029\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0460 - mse: 0.0037 - val_loss: 0.0027 - val_mae: 0.0392 - val_mse: 0.0027\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0411 - mse: 0.0030 - val_loss: 0.0026 - val_mae: 0.0408 - val_mse: 0.0026\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0473 - mse: 0.0038 - val_loss: 0.0036 - val_mae: 0.0450 - val_mse: 0.0036\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0461 - mse: 0.0036 - val_loss: 0.0045 - val_mae: 0.0556 - val_mse: 0.0045\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0437 - mse: 0.0033 - val_loss: 0.0025 - val_mae: 0.0367 - val_mse: 0.0025\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0420 - mse: 0.0032 - val_loss: 0.0023 - val_mae: 0.0362 - val_mse: 0.0023\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0427 - mse: 0.0032 - val_loss: 0.0027 - val_mae: 0.0408 - val_mse: 0.0027\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0435 - mse: 0.0033 - val_loss: 0.0024 - val_mae: 0.0361 - val_mse: 0.0024\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0436 - mse: 0.0033 - val_loss: 0.0043 - val_mae: 0.0529 - val_mse: 0.0043\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0444 - mse: 0.0034 - val_loss: 0.0024 - val_mae: 0.0375 - val_mse: 0.0024\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0430 - mse: 0.0032 - val_loss: 0.0024 - val_mae: 0.0372 - val_mse: 0.0024\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0427 - mse: 0.0031 - val_loss: 0.0031 - val_mae: 0.0454 - val_mse: 0.0031\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0465 - mse: 0.0037 - val_loss: 0.0025 - val_mae: 0.0377 - val_mse: 0.0025\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0456 - mse: 0.0035 - val_loss: 0.0035 - val_mae: 0.0457 - val_mse: 0.0035\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0439 - mse: 0.0034 - val_loss: 0.0047 - val_mae: 0.0565 - val_mse: 0.0047\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0417 - mse: 0.0031 - val_loss: 0.0030 - val_mae: 0.0412 - val_mse: 0.0030\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0436 - mse: 0.0034 - val_loss: 0.0027 - val_mae: 0.0411 - val_mse: 0.0027\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0443 - mse: 0.0034 - val_loss: 0.0035 - val_mae: 0.0453 - val_mse: 0.0035\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0423 - mse: 0.0031 - val_loss: 0.0052 - val_mae: 0.0584 - val_mse: 0.0052\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0447 - mse: 0.0035 - val_loss: 0.0027 - val_mae: 0.0394 - val_mse: 0.0027\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0432 - mse: 0.0033 - val_loss: 0.0026 - val_mae: 0.0390 - val_mse: 0.0026\n",
      "Epoch 00057: early stopping\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.014235361933999345    std:  0.05263681582586076    MAE:  0.04035019864970605     R2:  0.9759841361434113\n",
      "Test. mean:  -0.013229358187385703    std:  0.04967396956102751    MAE:  0.0390400333463889     R2:  0.977900837361067\n",
      "\u001b[1m\u001b[91mFold 3\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 2s 2ms/step - loss: 0.4685 - mae: 0.5975 - mse: 0.4685 - val_loss: 0.0862 - val_mae: 0.2322 - val_mse: 0.0862\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0747 - mae: 0.2163 - mse: 0.0747 - val_loss: 0.0581 - val_mae: 0.2054 - val_mse: 0.0581\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0558 - mae: 0.2003 - mse: 0.0558 - val_loss: 0.0576 - val_mae: 0.2048 - val_mse: 0.0576\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0566 - mae: 0.2030 - mse: 0.0566 - val_loss: 0.0577 - val_mae: 0.2055 - val_mse: 0.0577\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0550 - mae: 0.1988 - mse: 0.0550 - val_loss: 0.0577 - val_mae: 0.2055 - val_mse: 0.0577\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.2002 - mse: 0.0554 - val_loss: 0.0576 - val_mae: 0.2052 - val_mse: 0.0576\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0567 - mae: 0.2027 - mse: 0.0567 - val_loss: 0.0577 - val_mae: 0.2041 - val_mse: 0.0577\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0547 - mae: 0.1984 - mse: 0.0547 - val_loss: 0.0576 - val_mae: 0.2046 - val_mse: 0.0576\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1991 - mse: 0.0554 - val_loss: 0.0580 - val_mae: 0.2035 - val_mse: 0.0580\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0544 - mae: 0.1962 - mse: 0.0544 - val_loss: 0.0577 - val_mae: 0.2057 - val_mse: 0.0577\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2025 - mse: 0.0563 - val_loss: 0.0578 - val_mae: 0.2039 - val_mse: 0.0578\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.2000 - mse: 0.0555 - val_loss: 0.0581 - val_mae: 0.2035 - val_mse: 0.0581\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0553 - mae: 0.1994 - mse: 0.0553 - val_loss: 0.0577 - val_mae: 0.2058 - val_mse: 0.0577\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0553 - mae: 0.1993 - mse: 0.0553 - val_loss: 0.0579 - val_mae: 0.2066 - val_mse: 0.0579\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0564 - mae: 0.2022 - mse: 0.0564 - val_loss: 0.0577 - val_mae: 0.2059 - val_mse: 0.0577\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1991 - mse: 0.0554 - val_loss: 0.0581 - val_mae: 0.2034 - val_mse: 0.0581\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0545 - mae: 0.1975 - mse: 0.0545 - val_loss: 0.0578 - val_mae: 0.2039 - val_mse: 0.0578\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0558 - mae: 0.2000 - mse: 0.0558 - val_loss: 0.0576 - val_mae: 0.2050 - val_mse: 0.0576\n",
      "Epoch 00018: early stopping\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  0.0032126546092539968    std:  0.23518808566421381    MAE:  0.19997206661790962     R2:  -0.025829758682606226\n",
      "Test. mean:  0.0018354027881082452    std:  0.24000916983311973    MAE:  0.20498441871934323     R2:  -0.03177913727212167\n",
      "\u001b[1m\u001b[91mFold 4\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 2s 3ms/step - loss: 0.3616 - mae: 0.4071 - mse: 0.3616 - val_loss: 0.0251 - val_mae: 0.1197 - val_mse: 0.0251\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0171 - mae: 0.0995 - mse: 0.0171 - val_loss: 0.0127 - val_mae: 0.0869 - val_mse: 0.0127\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0129 - mae: 0.0872 - mse: 0.0129 - val_loss: 0.0108 - val_mae: 0.0790 - val_mse: 0.0108\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0108 - mae: 0.0790 - mse: 0.0108 - val_loss: 0.0104 - val_mae: 0.0766 - val_mse: 0.0104\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0096 - mae: 0.0746 - mse: 0.0096 - val_loss: 0.0114 - val_mae: 0.0811 - val_mse: 0.0114\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0720 - mse: 0.0092 - val_loss: 0.0101 - val_mae: 0.0756 - val_mse: 0.0101\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0690 - mse: 0.0084 - val_loss: 0.0090 - val_mae: 0.0697 - val_mse: 0.0090\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0707 - mse: 0.0088 - val_loss: 0.0089 - val_mae: 0.0699 - val_mse: 0.0089\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0079 - mae: 0.0662 - mse: 0.0079 - val_loss: 0.0089 - val_mae: 0.0704 - val_mse: 0.0089\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0682 - mse: 0.0083 - val_loss: 0.0074 - val_mae: 0.0634 - val_mse: 0.0074\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0075 - mae: 0.0641 - mse: 0.0075 - val_loss: 0.0073 - val_mae: 0.0625 - val_mse: 0.0073\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0074 - mae: 0.0644 - mse: 0.0074 - val_loss: 0.0071 - val_mae: 0.0625 - val_mse: 0.0071\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0077 - mae: 0.0657 - mse: 0.0077 - val_loss: 0.0071 - val_mae: 0.0602 - val_mse: 0.0071\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0617 - mse: 0.0069 - val_loss: 0.0085 - val_mae: 0.0681 - val_mse: 0.0085\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0629 - mse: 0.0071 - val_loss: 0.0067 - val_mae: 0.0600 - val_mse: 0.0067\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0621 - mse: 0.0068 - val_loss: 0.0067 - val_mae: 0.0614 - val_mse: 0.0067\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0617 - mse: 0.0068 - val_loss: 0.0079 - val_mae: 0.0665 - val_mse: 0.0079\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0078 - mae: 0.0674 - mse: 0.0078 - val_loss: 0.0067 - val_mae: 0.0596 - val_mse: 0.0067\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0552 - mse: 0.0060 - val_loss: 0.0041 - val_mae: 0.0438 - val_mse: 0.0041\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0049 - mae: 0.0509 - mse: 0.0049 - val_loss: 0.0039 - val_mae: 0.0450 - val_mse: 0.0039\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0053 - mae: 0.0533 - mse: 0.0053 - val_loss: 0.0043 - val_mae: 0.0515 - val_mse: 0.0043\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0040 - mae: 0.0465 - mse: 0.0040 - val_loss: 0.0033 - val_mae: 0.0426 - val_mse: 0.0033\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0445 - mse: 0.0036 - val_loss: 0.0032 - val_mae: 0.0415 - val_mse: 0.0032\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0459 - mse: 0.0038 - val_loss: 0.0033 - val_mae: 0.0418 - val_mse: 0.0033\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0041 - mae: 0.0476 - mse: 0.0041 - val_loss: 0.0034 - val_mae: 0.0436 - val_mse: 0.0034\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0040 - mae: 0.0473 - mse: 0.0040 - val_loss: 0.0028 - val_mae: 0.0380 - val_mse: 0.0028\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0448 - mse: 0.0035 - val_loss: 0.0029 - val_mae: 0.0395 - val_mse: 0.0029\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0039 - mae: 0.0472 - mse: 0.0039 - val_loss: 0.0038 - val_mae: 0.0464 - val_mse: 0.0038\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0445 - mse: 0.0035 - val_loss: 0.0028 - val_mae: 0.0381 - val_mse: 0.0028\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0417 - mse: 0.0032 - val_loss: 0.0046 - val_mae: 0.0553 - val_mse: 0.0046\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0429 - mse: 0.0033 - val_loss: 0.0043 - val_mae: 0.0522 - val_mse: 0.0043\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0428 - mse: 0.0033 - val_loss: 0.0036 - val_mae: 0.0463 - val_mse: 0.0036\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0411 - mse: 0.0031 - val_loss: 0.0030 - val_mae: 0.0415 - val_mse: 0.0030\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0450 - mse: 0.0036 - val_loss: 0.0027 - val_mae: 0.0382 - val_mse: 0.0027\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0402 - mse: 0.0029 - val_loss: 0.0031 - val_mae: 0.0417 - val_mse: 0.0031\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0428 - mse: 0.0032 - val_loss: 0.0030 - val_mae: 0.0419 - val_mse: 0.0030\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0421 - mse: 0.0031 - val_loss: 0.0029 - val_mae: 0.0400 - val_mse: 0.0029\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0405 - mse: 0.0029 - val_loss: 0.0024 - val_mae: 0.0352 - val_mse: 0.0024\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0410 - mse: 0.0030 - val_loss: 0.0040 - val_mae: 0.0523 - val_mse: 0.0040\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0039 - mae: 0.0477 - mse: 0.0039 - val_loss: 0.0028 - val_mae: 0.0385 - val_mse: 0.0028\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0456 - mse: 0.0036 - val_loss: 0.0060 - val_mae: 0.0554 - val_mse: 0.0060\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0436 - mse: 0.0036 - val_loss: 0.0023 - val_mae: 0.0347 - val_mse: 0.0023\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0399 - mse: 0.0029 - val_loss: 0.0049 - val_mae: 0.0577 - val_mse: 0.0049\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0423 - mse: 0.0032 - val_loss: 0.0048 - val_mae: 0.0550 - val_mse: 0.0048\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0414 - mse: 0.0030 - val_loss: 0.0023 - val_mae: 0.0346 - val_mse: 0.0023\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0409 - mse: 0.0029 - val_loss: 0.0026 - val_mae: 0.0371 - val_mse: 0.0026\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0409 - mse: 0.0030 - val_loss: 0.0027 - val_mae: 0.0392 - val_mse: 0.0027\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0402 - mse: 0.0029 - val_loss: 0.0038 - val_mae: 0.0481 - val_mse: 0.0038\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0441 - mse: 0.0034 - val_loss: 0.0024 - val_mae: 0.0358 - val_mse: 0.0024\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0028 - mae: 0.0395 - mse: 0.0028 - val_loss: 0.0023 - val_mae: 0.0349 - val_mse: 0.0023\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0026 - mae: 0.0377 - mse: 0.0026 - val_loss: 0.0023 - val_mae: 0.0357 - val_mse: 0.0023\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0408 - mse: 0.0030 - val_loss: 0.0027 - val_mae: 0.0387 - val_mse: 0.0027\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0411 - mse: 0.0029 - val_loss: 0.0023 - val_mae: 0.0339 - val_mse: 0.0023\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0412 - mse: 0.0030 - val_loss: 0.0023 - val_mae: 0.0343 - val_mse: 0.0023\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0420 - mse: 0.0031 - val_loss: 0.0040 - val_mae: 0.0508 - val_mse: 0.0040\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0413 - mse: 0.0029 - val_loss: 0.0036 - val_mae: 0.0466 - val_mse: 0.0036\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0028 - mae: 0.0401 - mse: 0.0028 - val_loss: 0.0034 - val_mae: 0.0435 - val_mse: 0.0034\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0425 - mse: 0.0033 - val_loss: 0.0036 - val_mae: 0.0471 - val_mse: 0.0036\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0449 - mse: 0.0035 - val_loss: 0.0027 - val_mae: 0.0382 - val_mse: 0.0027\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0412 - mse: 0.0030 - val_loss: 0.0023 - val_mae: 0.0339 - val_mse: 0.0023\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0027 - mae: 0.0386 - mse: 0.0027 - val_loss: 0.0022 - val_mae: 0.0334 - val_mse: 0.0022\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0028 - mae: 0.0388 - mse: 0.0028 - val_loss: 0.0048 - val_mae: 0.0576 - val_mse: 0.0048\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0420 - mse: 0.0031 - val_loss: 0.0029 - val_mae: 0.0417 - val_mse: 0.0029\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0403 - mse: 0.0029 - val_loss: 0.0023 - val_mae: 0.0340 - val_mse: 0.0023\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0432 - mse: 0.0033 - val_loss: 0.0024 - val_mae: 0.0340 - val_mse: 0.0024\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0395 - mse: 0.0029 - val_loss: 0.0029 - val_mae: 0.0389 - val_mse: 0.0029\n",
      "Epoch 67/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0410 - mse: 0.0030 - val_loss: 0.0024 - val_mae: 0.0367 - val_mse: 0.0024\n",
      "Epoch 68/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0028 - mae: 0.0393 - mse: 0.0028 - val_loss: 0.0023 - val_mae: 0.0344 - val_mse: 0.0023\n",
      "Epoch 69/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0027 - mae: 0.0379 - mse: 0.0027 - val_loss: 0.0027 - val_mae: 0.0380 - val_mse: 0.0027\n",
      "Epoch 70/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0426 - mse: 0.0032 - val_loss: 0.0026 - val_mae: 0.0388 - val_mse: 0.0026\n",
      "Epoch 71/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0028 - mae: 0.0397 - mse: 0.0028 - val_loss: 0.0024 - val_mae: 0.0358 - val_mse: 0.0024\n",
      "Epoch 72/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0398 - mse: 0.0029 - val_loss: 0.0023 - val_mae: 0.0347 - val_mse: 0.0023\n",
      "Epoch 73/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0028 - mae: 0.0385 - mse: 0.0028 - val_loss: 0.0033 - val_mae: 0.0460 - val_mse: 0.0033\n",
      "Epoch 74/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0027 - mae: 0.0382 - mse: 0.0027 - val_loss: 0.0039 - val_mae: 0.0513 - val_mse: 0.0039\n",
      "Epoch 75/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0027 - mae: 0.0386 - mse: 0.0027 - val_loss: 0.0024 - val_mae: 0.0342 - val_mse: 0.0024\n",
      "Epoch 76/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0028 - mae: 0.0392 - mse: 0.0028 - val_loss: 0.0025 - val_mae: 0.0357 - val_mse: 0.0025\n",
      "Epoch 00076: early stopping\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.016611694095323908    std:  0.04659337808939698    MAE:  0.03534025148871431     R2:  0.9805160170585395\n",
      "Test. mean:  -0.016716840622803382    std:  0.0469318966616411    MAE:  0.035699700531059315     R2:  0.9796997704268829\n",
      "\n",
      "Duration :  00:03:53 216ms\n",
      "\u001b[1maverage MAE of the training set:\u001b[0m   0.07 +/- 0.06\n",
      "\u001b[1maverage MAE of the validation set:\u001b[0m 0.08 +/- 0.07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#######################################################################################\n",
    "# optimization of the ANN\n",
    "# library used: keras and scikit learn for the KFold cross-validator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "VERBOSE = 1\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 25\n",
    "N_SPLIT = 5\n",
    "\n",
    "vID.chrono_start()\n",
    "\n",
    "# variables created to save at each iteration of the KFold process: the man error, the standard deviation, MAE, R2\n",
    "meantT=list()\n",
    "stdtT=list()\n",
    "MAEtT=list()\n",
    "R2tT=list()\n",
    "meanvT=list()\n",
    "stdvT=list()\n",
    "MAEvT=list()\n",
    "R2vT=list()\n",
    "\n",
    "kfold = KFold(n_splits=N_SPLIT,shuffle=True,random_state=42) # k-fold is here!\n",
    "#print(list(kfold.split(x_train,y_train)))\n",
    "\n",
    "j = 0 # Variable for keeping count of split we are executing\n",
    "# The KFold cv provides train/test indices to split data in train/test sets\n",
    "for train_idx, val_idx in list(kfold.split(xdata,ydata)):\n",
    "\n",
    "    x_train_cv = xdata.iloc[train_idx]\n",
    "    x_valid_cv = xdata.iloc[val_idx]\n",
    "    y_train_cv = ydata.iloc[train_idx]\n",
    "    y_valid_cv = ydata.iloc[val_idx]\n",
    "#    display(x_train_cv,x_valid_cv)\n",
    "# This part has been commented with respect to the original script\n",
    "    # scaler = preprocessing.StandardScaler()\n",
    "    # scaler.fit(x_train_cv.values)\n",
    "    # xt_scaled = scaler.transform(x_train_cv.values) #returns a numpy array\n",
    "    # xv_scaled = scaler.transform(x_valid_cv.values) #returns a numpy array\n",
    "    # x_train_cv = pd.DataFrame(xt_scaled, index=x_train_cv.index, columns=x_train_cv.columns)\n",
    "    # x_valid_cv = pd.DataFrame(xv_scaled, index=x_valid_cv.index, columns=x_valid_cv.columns)\n",
    "    # del xt_scaled, xv_scaled\n",
    "##############\n",
    "#    display(x_train_cv.describe().style.format(\"{0:.2f}\").set_caption(\"Training set after normalization (with scikit-learn):\"))\n",
    "#    display(x_valid_cv.describe().style.format(\"{0:.2f}\").set_caption(\"Validation set after normalization (with scikit-learn):\"))\n",
    "    print(f\"{color.BOLD}{color.RED}Fold {j}{color.OFF}\")\n",
    "    j+=1\n",
    "    ANNmodel=defANN( (53,), acthL )\n",
    "    ANNhistory = ANNmodel.fit(x_train_cv,\n",
    "                        y_train_cv,\n",
    "                        epochs          = EPOCHS,\n",
    "                        batch_size      = BATCH_SIZE,\n",
    "                        verbose         = VERBOSE,\n",
    "                        validation_data = (x_valid_cv, y_valid_cv),\n",
    "                        callbacks=[es])\n",
    "    ytrain_hat=ANNmodel.predict(x_train_cv)\n",
    "    yvalid_hat=ANNmodel.predict(x_valid_cv)\n",
    "    diffyt = ytrain_hat.ravel() - y_train_cv.ravel()\n",
    "    diffyv = yvalid_hat.ravel() - y_valid_cv.ravel()\n",
    "\n",
    "    print()\n",
    "    print(\"xCO2(predicted) - xCO2(actual)\")\n",
    "    print(\n",
    "          \"Train.\",\"mean: \", np.mean(diffyt),\n",
    "          \"   std: \", np.std(diffyt),\n",
    "          \"   MAE: \", np.average(abs(diffyt)),\n",
    "          \"    R2: \", np.corrcoef(y_train_cv.ravel(),ytrain_hat.ravel())[0,1]\n",
    "         )\n",
    "    print(\n",
    "          \"Test.\",\"mean: \", np.mean(diffyv),\n",
    "          \"   std: \", np.std(diffyv),\n",
    "          \"   MAE: \", np.average(abs(diffyv)),\n",
    "          \"    R2: \", np.corrcoef(y_valid_cv.ravel(),yvalid_hat.ravel())[0,1]\n",
    "         )\n",
    "    meantT.append(np.mean(diffyt))\n",
    "    meanvT.append(np.mean(diffyv))\n",
    "    stdtT.append(np.std(diffyt))\n",
    "    stdvT.append(np.std(diffyv))\n",
    "    MAEtT.append(np.average(abs(diffyt)))\n",
    "    MAEvT.append(np.average(abs(diffyv)))\n",
    "    R2tT.append(np.corrcoef(y_train_cv.ravel(),ytrain_hat.ravel())[0,1])\n",
    "    R2vT.append(np.corrcoef(y_valid_cv.ravel(),yvalid_hat.ravel())[0,1])\n",
    "    \n",
    "vID.chrono_show()\n",
    "\n",
    "#######################################################################################\n",
    "# accuracy of the ANN?\n",
    "# library used: numpy\n",
    "print(f\"{color.BOLD}average MAE of the training set:{color.OFF}   {np.mean(MAEtT):.2f} +/- {np.std(MAEtT):.2f}\")\n",
    "print(f\"{color.BOLD}average MAE of the validation set:{color.OFF} {np.mean(MAEvT):.2f} +/- {np.std(MAEvT):.2f}\")\n",
    "\n",
    "figCV, axCV = plt.subplots(1, 1)\n",
    "figCV.set_size_inches(12,5)\n",
    "axCV.errorbar(x=np.arange(len(meantT)), y=meantT, yerr=MAEtT, label='training sets', fmt='o-', capsize=10)\n",
    "axCV.errorbar(x=np.arange(len(meanvT))+0.1, y=meanvT, yerr=MAEvT, label='validation sets', fmt='o-', capsize=10)\n",
    "axCV.legend(loc='lower left', shadow=True, fontsize='14')\n",
    "axCV.set_xlabel('cross-validation k-values ',fontdict={'fontsize':16})\n",
    "axCV.set_ylabel('$\\hat{y}-y_{\\mathrm{actual}}$',fontdict={'fontsize':16})\n",
    "axCV.tick_params(labelsize = 14)\n",
    "plt.savefig('./CO2-images/KFold-cv-AppliedToSong_etal.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d639724-2e98-43dd-b6e5-7f0767cfb27d",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "You have probably found results close to those reported in the following error plot:\n",
    "<p style=\"text-align: center\"><img width=\"650px\" src=\"./CO2-images/KFold-cv-AppliedToSong_etalK.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_ResultsSong\"></p>\n",
    "    <b>This error plot shows a bad performance of the original ML algorithm of Song <i>et al</i>. (<i>i.e.</i> without standardization of the data), with a strong variation of error bars.</b>\n",
    "    \n",
    "Either the authors did actually apply a standardization preprocessing and they forgot to mention it in the article, or they ran several optimization algorithms of the ANN until they found a seemingly performant one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1998a632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**End at:** Friday 24 June 2022, 09:24:18  \n",
       "**Duration:** 00:04:02 002ms"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"./svg/logoEnd.svg\" style=\"margin-left:auto; margin-right:auto\"/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vID.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb92c04-23d1-4c52-885d-fa8e5eee43c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
