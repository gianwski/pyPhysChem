{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10024ca9-7e94-4f5c-a982-b4537b919d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: bold;\n",
       "}\n",
       "body, exercice {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: 200;\n",
       "}\n",
       "h1 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 30px ;\n",
       "  color: white;\n",
       "  background: #b11d01;\n",
       "  text-align: center;\n",
       "}\n",
       "h2 {\n",
       "  border: 3px solid #333;\n",
       "  padding: 18px ;\n",
       "  color: #b11d01;\n",
       "  background: #ffffff;\n",
       "  text-align: center;\n",
       "}\n",
       "h3 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 12px ;\n",
       "  color: #000000;\n",
       "  background: #c1c1c1;\n",
       "  text-align: left;\n",
       "}\n",
       "h4 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #d9fffc;\n",
       "  text-align: left;\n",
       "}\n",
       "h5 {\n",
       "  border: 1px solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #ffffff;\n",
       "  text-align: left;\n",
       "}\n",
       ".rq {    \n",
       "    background-color: #fcf2f2;\n",
       "    border-color: #dFb5b4;\n",
       "    border-left: 5px solid #dfb5b4;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       ".intro {    \n",
       "    background-color: #f1f1f1;\n",
       "    border-color: #969696;\n",
       "    border-left: 5px solid #969696;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       ".exold {    \n",
       "    background-color: #b2dbea80;\n",
       "    border-color: #0055ff;\n",
       "    border-left: 10px solid #0055ff;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       ".ex {    \n",
       "    background-color: #b2dbea80;\n",
       "    padding: 0.5em;\n",
       "    padding-top: 0em;\n",
       "    font-weight: 200;\n",
       "    position:relative;\n",
       "    }\n",
       ".ex::before {\n",
       "    background-color: #b2dbea;\n",
       "    content:\"Exercice\";\n",
       "    margin-left:-0.5em;\n",
       "    margin-right:-0.5em;\n",
       "    padding-left:0.5em;\n",
       "    padding-right:0.5em;\n",
       "    font-weight: 600;\n",
       "    display: block;\n",
       "    }\n",
       ".app {    \n",
       "    background-color: #b2dbea80;\n",
       "    padding: 0.5em;\n",
       "    padding-top: 0em;\n",
       "    font-weight: 200;\n",
       "    position:relative;\n",
       "    }\n",
       ".app::before {\n",
       "    background-color: #b2dbea;\n",
       "    content:\"Application\";\n",
       "    margin-left:-0.5em;\n",
       "    margin-right:-0.5em;\n",
       "    padding-left:0.5em;\n",
       "    padding-right:0.5em;\n",
       "    font-weight: 600;\n",
       "    display: block;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Start at:** Thursday 18 May 2023, 13:34:35  \n",
       "**Hostname:** localhost.localdomain (Linux)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"../config/svg/PytChemBanner.svg\" style=\"margin-left:auto; margin-right:auto\"/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "cwd0 = '../config/'\n",
    "sys.path.append(cwd0)\n",
    "\n",
    "import visualID_Eng as vID\n",
    "from visualID_Eng import color\n",
    "vID.init(cwd0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e4b9b",
   "metadata": {},
   "source": [
    "# Prediction by an artificial neural network of the solubility of CO<sub>2</sub> in ionic liquids\n",
    "\n",
    "<div class=\"intro\">\n",
    "    \n",
    "<b>Reference</b>: \n",
    "Z. Song, H. Shi, X. Zhang & T. Zhou (**2020**), Prediction of CO<sub>2</sub> solubility in ionic liquids using machine learning methods, [<i>Chem. Eng. Sci.</i> <b>223</b>: 115752](https://www.doi.org/10.1016/j.ces.2020.115752) \n",
    "<br>\n",
    "<p style=\"text-align: center\"><img width=\"650px\" src=\"../DS4B-CO2-images/AbstractANNCO2-SongEtal.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_AbstractSong\"></p>\n",
    "<br>\n",
    "The main results are graphically reported below.\n",
    "<br>\n",
    "<p style=\"text-align: center\"><img width=\"900px\" src=\"../DS4B-CO2-images/ANNCO2-SongEtal-Results.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_ResultsSong\"></p>\n",
    "<br>\n",
    "Yet, it seems sthat no standardization process of the data has been applied. \n",
    "    \n",
    "<span style=\"color:red\">Moreover, a spurious separation of the data between training and test sets has been applied: \"<i>Instead of performing random selection, we employ a hybrid artificial-random strategy to decompose the dataset. Specifically, the data points consisting of the least frequently used groups are equally divided into five folders\"</i></span> \n",
    "<br><br>\n",
    "<b>It raises doubts about the stability of the algorithm developped in this paper (*unless the authors forgot to mention that data were standardized*).</b>\n",
    "<br>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84860c4-3fcf-45f5-80a3-607ead68da80",
   "metadata": {},
   "source": [
    "<div class=\"rq\">\n",
    "\n",
    "<span style=\"font-weight:bold\">The goal of this exercise is to apply the <i>K</i>-fold cross-validation the ANN part of this article, <i>i.e.</i> without standardized data. </span>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af39c092-f4b7-460b-b698-32e0f2c55461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 13:34:20.495589: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-18 13:34:20.552142: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os,sys\n",
    "from IPython.display import display\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89a89d-fb12-4ba1-8382-1609a0c4f848",
   "metadata": {},
   "source": [
    "<a id=\"data-read\"></a>\n",
    "## **1.** Database reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d2251b-1f22-42f5-b930-e6032fe55170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IL</th>\n",
       "      <th>cation</th>\n",
       "      <th>anion</th>\n",
       "      <th>x_CO2</th>\n",
       "      <th>T (K)</th>\n",
       "      <th>P (bar)</th>\n",
       "      <th>[CH3]</th>\n",
       "      <th>[CH2]</th>\n",
       "      <th>[CH]</th>\n",
       "      <th>[OCH2]</th>\n",
       "      <th>...</th>\n",
       "      <th>[MeSO3]</th>\n",
       "      <th>[TfO]</th>\n",
       "      <th>[NfO]</th>\n",
       "      <th>[TDfO]</th>\n",
       "      <th>[TOS]</th>\n",
       "      <th>[C12PhSO3]</th>\n",
       "      <th>[DMPO4]</th>\n",
       "      <th>[DEPO4]</th>\n",
       "      <th>[DBPO4]</th>\n",
       "      <th>[methide]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>363.15</td>\n",
       "      <td>246.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.500</td>\n",
       "      <td>383.15</td>\n",
       "      <td>235.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>353.15</td>\n",
       "      <td>223.30</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.500</td>\n",
       "      <td>373.15</td>\n",
       "      <td>198.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>343.15</td>\n",
       "      <td>188.50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10111</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.592</td>\n",
       "      <td>298.15</td>\n",
       "      <td>35.86</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10112</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.239</td>\n",
       "      <td>343.15</td>\n",
       "      <td>27.54</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10113</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.396</td>\n",
       "      <td>298.15</td>\n",
       "      <td>20.15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10114</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.140</td>\n",
       "      <td>343.15</td>\n",
       "      <td>17.93</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10115</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.139</td>\n",
       "      <td>323.15</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10116 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 IL  cation   anion  x_CO2   T (K)  P (bar)  [CH3]  [CH2]   \n",
       "0       [BMIM][BF4]  [BMIM]   [BF4]  0.610  363.15   246.00      1      3  \\\n",
       "1       [BMIM][BF4]  [BMIM]   [BF4]  0.500  383.15   235.00      1      3   \n",
       "2       [BMIM][BF4]  [BMIM]   [BF4]  0.610  353.15   223.30      1      3   \n",
       "3       [BMIM][BF4]  [BMIM]   [BF4]  0.500  373.15   198.00      1      3   \n",
       "4       [BMIM][BF4]  [BMIM]   [BF4]  0.610  343.15   188.50      1      3   \n",
       "...             ...     ...     ...    ...     ...      ...    ...    ...   \n",
       "10111  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.592  298.15    35.86      1      5   \n",
       "10112  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.239  343.15    27.54      1      5   \n",
       "10113  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.396  298.15    20.15      1      5   \n",
       "10114  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.140  343.15    17.93      1      5   \n",
       "10115  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.139  323.15     8.00      1      5   \n",
       "\n",
       "       [CH]  [OCH2]  ...  [MeSO3]  [TfO]  [NfO]  [TDfO]  [TOS]  [C12PhSO3]   \n",
       "0         0       0  ...        0      0      0       0      0           0  \\\n",
       "1         0       0  ...        0      0      0       0      0           0   \n",
       "2         0       0  ...        0      0      0       0      0           0   \n",
       "3         0       0  ...        0      0      0       0      0           0   \n",
       "4         0       0  ...        0      0      0       0      0           0   \n",
       "...     ...     ...  ...      ...    ...    ...     ...    ...         ...   \n",
       "10111     0       0  ...        0      0      0       0      0           0   \n",
       "10112     0       0  ...        0      0      0       0      0           0   \n",
       "10113     0       0  ...        0      0      0       0      0           0   \n",
       "10114     0       0  ...        0      0      0       0      0           0   \n",
       "10115     0       0  ...        0      0      0       0      0           0   \n",
       "\n",
       "       [DMPO4]  [DEPO4]  [DBPO4]  [methide]  \n",
       "0            0        0        0          0  \n",
       "1            0        0        0          0  \n",
       "2            0        0        0          0  \n",
       "3            0        0        0          0  \n",
       "4            0        0        0          0  \n",
       "...        ...      ...      ...        ...  \n",
       "10111        0        0        0          0  \n",
       "10112        0        0        0          0  \n",
       "10113        0        0        0          0  \n",
       "10114        0        0        0          0  \n",
       "10115        0        0        0          0  \n",
       "\n",
       "[10116 rows x 57 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_0d4c5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0d4c5_level0_col0\" class=\"col_heading level0 col0\" >x_CO2</th>\n",
       "      <th id=\"T_0d4c5_level0_col1\" class=\"col_heading level0 col1\" >T (K)</th>\n",
       "      <th id=\"T_0d4c5_level0_col2\" class=\"col_heading level0 col2\" >P (bar)</th>\n",
       "      <th id=\"T_0d4c5_level0_col3\" class=\"col_heading level0 col3\" >[CH3]</th>\n",
       "      <th id=\"T_0d4c5_level0_col4\" class=\"col_heading level0 col4\" >[CH2]</th>\n",
       "      <th id=\"T_0d4c5_level0_col5\" class=\"col_heading level0 col5\" >[CH]</th>\n",
       "      <th id=\"T_0d4c5_level0_col6\" class=\"col_heading level0 col6\" >[OCH2]</th>\n",
       "      <th id=\"T_0d4c5_level0_col7\" class=\"col_heading level0 col7\" >[OCH3]</th>\n",
       "      <th id=\"T_0d4c5_level0_col8\" class=\"col_heading level0 col8\" >[CF2]</th>\n",
       "      <th id=\"T_0d4c5_level0_col9\" class=\"col_heading level0 col9\" >[CF3]</th>\n",
       "      <th id=\"T_0d4c5_level0_col10\" class=\"col_heading level0 col10\" >[OH]</th>\n",
       "      <th id=\"T_0d4c5_level0_col11\" class=\"col_heading level0 col11\" >CH=CH</th>\n",
       "      <th id=\"T_0d4c5_level0_col12\" class=\"col_heading level0 col12\" >CH=CH2</th>\n",
       "      <th id=\"T_0d4c5_level0_col13\" class=\"col_heading level0 col13\" >[Im13]</th>\n",
       "      <th id=\"T_0d4c5_level0_col14\" class=\"col_heading level0 col14\" >[MIm]</th>\n",
       "      <th id=\"T_0d4c5_level0_col15\" class=\"col_heading level0 col15\" >[MMIM]</th>\n",
       "      <th id=\"T_0d4c5_level0_col16\" class=\"col_heading level0 col16\" >[Py]</th>\n",
       "      <th id=\"T_0d4c5_level0_col17\" class=\"col_heading level0 col17\" >[MPy]</th>\n",
       "      <th id=\"T_0d4c5_level0_col18\" class=\"col_heading level0 col18\" >[MPyrro]</th>\n",
       "      <th id=\"T_0d4c5_level0_col19\" class=\"col_heading level0 col19\" >[MPip]</th>\n",
       "      <th id=\"T_0d4c5_level0_col20\" class=\"col_heading level0 col20\" >[NH3]</th>\n",
       "      <th id=\"T_0d4c5_level0_col21\" class=\"col_heading level0 col21\" >[NH2]</th>\n",
       "      <th id=\"T_0d4c5_level0_col22\" class=\"col_heading level0 col22\" >[NH]</th>\n",
       "      <th id=\"T_0d4c5_level0_col23\" class=\"col_heading level0 col23\" >[N]</th>\n",
       "      <th id=\"T_0d4c5_level0_col24\" class=\"col_heading level0 col24\" >[P]</th>\n",
       "      <th id=\"T_0d4c5_level0_col25\" class=\"col_heading level0 col25\" >[S]</th>\n",
       "      <th id=\"T_0d4c5_level0_col26\" class=\"col_heading level0 col26\" >[BF4]</th>\n",
       "      <th id=\"T_0d4c5_level0_col27\" class=\"col_heading level0 col27\" >[Cl]</th>\n",
       "      <th id=\"T_0d4c5_level0_col28\" class=\"col_heading level0 col28\" >[DCA]</th>\n",
       "      <th id=\"T_0d4c5_level0_col29\" class=\"col_heading level0 col29\" >[NO3]</th>\n",
       "      <th id=\"T_0d4c5_level0_col30\" class=\"col_heading level0 col30\" >[PF6]</th>\n",
       "      <th id=\"T_0d4c5_level0_col31\" class=\"col_heading level0 col31\" >[SCN]</th>\n",
       "      <th id=\"T_0d4c5_level0_col32\" class=\"col_heading level0 col32\" >[TCB]</th>\n",
       "      <th id=\"T_0d4c5_level0_col33\" class=\"col_heading level0 col33\" >[C(CN)3]</th>\n",
       "      <th id=\"T_0d4c5_level0_col34\" class=\"col_heading level0 col34\" >[HSO4]</th>\n",
       "      <th id=\"T_0d4c5_level0_col35\" class=\"col_heading level0 col35\" >[FSA]</th>\n",
       "      <th id=\"T_0d4c5_level0_col36\" class=\"col_heading level0 col36\" >[Tf2N]</th>\n",
       "      <th id=\"T_0d4c5_level0_col37\" class=\"col_heading level0 col37\" >[BETA]</th>\n",
       "      <th id=\"T_0d4c5_level0_col38\" class=\"col_heading level0 col38\" >[FOR]</th>\n",
       "      <th id=\"T_0d4c5_level0_col39\" class=\"col_heading level0 col39\" >[TFA]</th>\n",
       "      <th id=\"T_0d4c5_level0_col40\" class=\"col_heading level0 col40\" >[C3F7CO2]</th>\n",
       "      <th id=\"T_0d4c5_level0_col41\" class=\"col_heading level0 col41\" >[MeSO4]</th>\n",
       "      <th id=\"T_0d4c5_level0_col42\" class=\"col_heading level0 col42\" >[EtSO4]</th>\n",
       "      <th id=\"T_0d4c5_level0_col43\" class=\"col_heading level0 col43\" >[MDEGSO4]</th>\n",
       "      <th id=\"T_0d4c5_level0_col44\" class=\"col_heading level0 col44\" >[MeSO3]</th>\n",
       "      <th id=\"T_0d4c5_level0_col45\" class=\"col_heading level0 col45\" >[TfO]</th>\n",
       "      <th id=\"T_0d4c5_level0_col46\" class=\"col_heading level0 col46\" >[NfO]</th>\n",
       "      <th id=\"T_0d4c5_level0_col47\" class=\"col_heading level0 col47\" >[TDfO]</th>\n",
       "      <th id=\"T_0d4c5_level0_col48\" class=\"col_heading level0 col48\" >[TOS]</th>\n",
       "      <th id=\"T_0d4c5_level0_col49\" class=\"col_heading level0 col49\" >[C12PhSO3]</th>\n",
       "      <th id=\"T_0d4c5_level0_col50\" class=\"col_heading level0 col50\" >[DMPO4]</th>\n",
       "      <th id=\"T_0d4c5_level0_col51\" class=\"col_heading level0 col51\" >[DEPO4]</th>\n",
       "      <th id=\"T_0d4c5_level0_col52\" class=\"col_heading level0 col52\" >[DBPO4]</th>\n",
       "      <th id=\"T_0d4c5_level0_col53\" class=\"col_heading level0 col53\" >[methide]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0d4c5_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_0d4c5_row0_col0\" class=\"data row0 col0\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col1\" class=\"data row0 col1\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col2\" class=\"data row0 col2\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col3\" class=\"data row0 col3\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col4\" class=\"data row0 col4\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col5\" class=\"data row0 col5\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col6\" class=\"data row0 col6\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col7\" class=\"data row0 col7\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col8\" class=\"data row0 col8\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col9\" class=\"data row0 col9\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col10\" class=\"data row0 col10\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col11\" class=\"data row0 col11\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col12\" class=\"data row0 col12\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col13\" class=\"data row0 col13\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col14\" class=\"data row0 col14\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col15\" class=\"data row0 col15\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col16\" class=\"data row0 col16\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col17\" class=\"data row0 col17\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col18\" class=\"data row0 col18\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col19\" class=\"data row0 col19\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col20\" class=\"data row0 col20\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col21\" class=\"data row0 col21\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col22\" class=\"data row0 col22\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col23\" class=\"data row0 col23\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col24\" class=\"data row0 col24\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col25\" class=\"data row0 col25\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col26\" class=\"data row0 col26\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col27\" class=\"data row0 col27\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col28\" class=\"data row0 col28\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col29\" class=\"data row0 col29\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col30\" class=\"data row0 col30\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col31\" class=\"data row0 col31\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col32\" class=\"data row0 col32\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col33\" class=\"data row0 col33\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col34\" class=\"data row0 col34\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col35\" class=\"data row0 col35\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col36\" class=\"data row0 col36\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col37\" class=\"data row0 col37\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col38\" class=\"data row0 col38\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col39\" class=\"data row0 col39\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col40\" class=\"data row0 col40\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col41\" class=\"data row0 col41\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col42\" class=\"data row0 col42\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col43\" class=\"data row0 col43\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col44\" class=\"data row0 col44\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col45\" class=\"data row0 col45\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col46\" class=\"data row0 col46\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col47\" class=\"data row0 col47\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col48\" class=\"data row0 col48\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col49\" class=\"data row0 col49\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col50\" class=\"data row0 col50\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col51\" class=\"data row0 col51\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col52\" class=\"data row0 col52\" >10116.00</td>\n",
       "      <td id=\"T_0d4c5_row0_col53\" class=\"data row0 col53\" >10116.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d4c5_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_0d4c5_row1_col0\" class=\"data row1 col0\" >0.33</td>\n",
       "      <td id=\"T_0d4c5_row1_col1\" class=\"data row1 col1\" >325.27</td>\n",
       "      <td id=\"T_0d4c5_row1_col2\" class=\"data row1 col2\" >54.21</td>\n",
       "      <td id=\"T_0d4c5_row1_col3\" class=\"data row1 col3\" >1.18</td>\n",
       "      <td id=\"T_0d4c5_row1_col4\" class=\"data row1 col4\" >4.72</td>\n",
       "      <td id=\"T_0d4c5_row1_col5\" class=\"data row1 col5\" >0.02</td>\n",
       "      <td id=\"T_0d4c5_row1_col6\" class=\"data row1 col6\" >0.02</td>\n",
       "      <td id=\"T_0d4c5_row1_col7\" class=\"data row1 col7\" >0.04</td>\n",
       "      <td id=\"T_0d4c5_row1_col8\" class=\"data row1 col8\" >0.04</td>\n",
       "      <td id=\"T_0d4c5_row1_col9\" class=\"data row1 col9\" >0.01</td>\n",
       "      <td id=\"T_0d4c5_row1_col10\" class=\"data row1 col10\" >0.06</td>\n",
       "      <td id=\"T_0d4c5_row1_col11\" class=\"data row1 col11\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row1_col12\" class=\"data row1 col12\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row1_col13\" class=\"data row1 col13\" >0.01</td>\n",
       "      <td id=\"T_0d4c5_row1_col14\" class=\"data row1 col14\" >0.77</td>\n",
       "      <td id=\"T_0d4c5_row1_col15\" class=\"data row1 col15\" >0.01</td>\n",
       "      <td id=\"T_0d4c5_row1_col16\" class=\"data row1 col16\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row1_col17\" class=\"data row1 col17\" >0.01</td>\n",
       "      <td id=\"T_0d4c5_row1_col18\" class=\"data row1 col18\" >0.09</td>\n",
       "      <td id=\"T_0d4c5_row1_col19\" class=\"data row1 col19\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row1_col20\" class=\"data row1 col20\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row1_col21\" class=\"data row1 col21\" >0.01</td>\n",
       "      <td id=\"T_0d4c5_row1_col22\" class=\"data row1 col22\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row1_col23\" class=\"data row1 col23\" >0.02</td>\n",
       "      <td id=\"T_0d4c5_row1_col24\" class=\"data row1 col24\" >0.05</td>\n",
       "      <td id=\"T_0d4c5_row1_col25\" class=\"data row1 col25\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row1_col26\" class=\"data row1 col26\" >0.11</td>\n",
       "      <td id=\"T_0d4c5_row1_col27\" class=\"data row1 col27\" >0.02</td>\n",
       "      <td id=\"T_0d4c5_row1_col28\" class=\"data row1 col28\" >0.02</td>\n",
       "      <td id=\"T_0d4c5_row1_col29\" class=\"data row1 col29\" >0.02</td>\n",
       "      <td id=\"T_0d4c5_row1_col30\" class=\"data row1 col30\" >0.11</td>\n",
       "      <td id=\"T_0d4c5_row1_col31\" class=\"data row1 col31\" >0.02</td>\n",
       "      <td id=\"T_0d4c5_row1_col32\" class=\"data row1 col32\" >0.01</td>\n",
       "      <td id=\"T_0d4c5_row1_col33\" class=\"data row1 col33\" >0.07</td>\n",
       "      <td id=\"T_0d4c5_row1_col34\" class=\"data row1 col34\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row1_col35\" class=\"data row1 col35\" >0.01</td>\n",
       "      <td id=\"T_0d4c5_row1_col36\" class=\"data row1 col36\" >0.43</td>\n",
       "      <td id=\"T_0d4c5_row1_col37\" class=\"data row1 col37\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row1_col38\" class=\"data row1 col38\" >0.01</td>\n",
       "      <td id=\"T_0d4c5_row1_col39\" class=\"data row1 col39\" >0.01</td>\n",
       "      <td id=\"T_0d4c5_row1_col40\" class=\"data row1 col40\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row1_col41\" class=\"data row1 col41\" >0.02</td>\n",
       "      <td id=\"T_0d4c5_row1_col42\" class=\"data row1 col42\" >0.01</td>\n",
       "      <td id=\"T_0d4c5_row1_col43\" class=\"data row1 col43\" >0.01</td>\n",
       "      <td id=\"T_0d4c5_row1_col44\" class=\"data row1 col44\" >0.02</td>\n",
       "      <td id=\"T_0d4c5_row1_col45\" class=\"data row1 col45\" >0.05</td>\n",
       "      <td id=\"T_0d4c5_row1_col46\" class=\"data row1 col46\" >0.01</td>\n",
       "      <td id=\"T_0d4c5_row1_col47\" class=\"data row1 col47\" >0.01</td>\n",
       "      <td id=\"T_0d4c5_row1_col48\" class=\"data row1 col48\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row1_col49\" class=\"data row1 col49\" >0.01</td>\n",
       "      <td id=\"T_0d4c5_row1_col50\" class=\"data row1 col50\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row1_col51\" class=\"data row1 col51\" >0.01</td>\n",
       "      <td id=\"T_0d4c5_row1_col52\" class=\"data row1 col52\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row1_col53\" class=\"data row1 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d4c5_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_0d4c5_row2_col0\" class=\"data row2 col0\" >0.24</td>\n",
       "      <td id=\"T_0d4c5_row2_col1\" class=\"data row2 col1\" >25.24</td>\n",
       "      <td id=\"T_0d4c5_row2_col2\" class=\"data row2 col2\" >76.66</td>\n",
       "      <td id=\"T_0d4c5_row2_col3\" class=\"data row2 col3\" >0.96</td>\n",
       "      <td id=\"T_0d4c5_row2_col4\" class=\"data row2 col4\" >5.48</td>\n",
       "      <td id=\"T_0d4c5_row2_col5\" class=\"data row2 col5\" >0.25</td>\n",
       "      <td id=\"T_0d4c5_row2_col6\" class=\"data row2 col6\" >0.16</td>\n",
       "      <td id=\"T_0d4c5_row2_col7\" class=\"data row2 col7\" >0.20</td>\n",
       "      <td id=\"T_0d4c5_row2_col8\" class=\"data row2 col8\" >0.39</td>\n",
       "      <td id=\"T_0d4c5_row2_col9\" class=\"data row2 col9\" >0.10</td>\n",
       "      <td id=\"T_0d4c5_row2_col10\" class=\"data row2 col10\" >0.28</td>\n",
       "      <td id=\"T_0d4c5_row2_col11\" class=\"data row2 col11\" >0.06</td>\n",
       "      <td id=\"T_0d4c5_row2_col12\" class=\"data row2 col12\" >0.06</td>\n",
       "      <td id=\"T_0d4c5_row2_col13\" class=\"data row2 col13\" >0.10</td>\n",
       "      <td id=\"T_0d4c5_row2_col14\" class=\"data row2 col14\" >0.42</td>\n",
       "      <td id=\"T_0d4c5_row2_col15\" class=\"data row2 col15\" >0.08</td>\n",
       "      <td id=\"T_0d4c5_row2_col16\" class=\"data row2 col16\" >0.07</td>\n",
       "      <td id=\"T_0d4c5_row2_col17\" class=\"data row2 col17\" >0.11</td>\n",
       "      <td id=\"T_0d4c5_row2_col18\" class=\"data row2 col18\" >0.29</td>\n",
       "      <td id=\"T_0d4c5_row2_col19\" class=\"data row2 col19\" >0.06</td>\n",
       "      <td id=\"T_0d4c5_row2_col20\" class=\"data row2 col20\" >0.07</td>\n",
       "      <td id=\"T_0d4c5_row2_col21\" class=\"data row2 col21\" >0.09</td>\n",
       "      <td id=\"T_0d4c5_row2_col22\" class=\"data row2 col22\" >0.06</td>\n",
       "      <td id=\"T_0d4c5_row2_col23\" class=\"data row2 col23\" >0.16</td>\n",
       "      <td id=\"T_0d4c5_row2_col24\" class=\"data row2 col24\" >0.23</td>\n",
       "      <td id=\"T_0d4c5_row2_col25\" class=\"data row2 col25\" >0.06</td>\n",
       "      <td id=\"T_0d4c5_row2_col26\" class=\"data row2 col26\" >0.31</td>\n",
       "      <td id=\"T_0d4c5_row2_col27\" class=\"data row2 col27\" >0.12</td>\n",
       "      <td id=\"T_0d4c5_row2_col28\" class=\"data row2 col28\" >0.15</td>\n",
       "      <td id=\"T_0d4c5_row2_col29\" class=\"data row2 col29\" >0.12</td>\n",
       "      <td id=\"T_0d4c5_row2_col30\" class=\"data row2 col30\" >0.31</td>\n",
       "      <td id=\"T_0d4c5_row2_col31\" class=\"data row2 col31\" >0.14</td>\n",
       "      <td id=\"T_0d4c5_row2_col32\" class=\"data row2 col32\" >0.08</td>\n",
       "      <td id=\"T_0d4c5_row2_col33\" class=\"data row2 col33\" >0.26</td>\n",
       "      <td id=\"T_0d4c5_row2_col34\" class=\"data row2 col34\" >0.04</td>\n",
       "      <td id=\"T_0d4c5_row2_col35\" class=\"data row2 col35\" >0.11</td>\n",
       "      <td id=\"T_0d4c5_row2_col36\" class=\"data row2 col36\" >0.49</td>\n",
       "      <td id=\"T_0d4c5_row2_col37\" class=\"data row2 col37\" >0.03</td>\n",
       "      <td id=\"T_0d4c5_row2_col38\" class=\"data row2 col38\" >0.11</td>\n",
       "      <td id=\"T_0d4c5_row2_col39\" class=\"data row2 col39\" >0.11</td>\n",
       "      <td id=\"T_0d4c5_row2_col40\" class=\"data row2 col40\" >0.05</td>\n",
       "      <td id=\"T_0d4c5_row2_col41\" class=\"data row2 col41\" >0.13</td>\n",
       "      <td id=\"T_0d4c5_row2_col42\" class=\"data row2 col42\" >0.11</td>\n",
       "      <td id=\"T_0d4c5_row2_col43\" class=\"data row2 col43\" >0.10</td>\n",
       "      <td id=\"T_0d4c5_row2_col44\" class=\"data row2 col44\" >0.15</td>\n",
       "      <td id=\"T_0d4c5_row2_col45\" class=\"data row2 col45\" >0.23</td>\n",
       "      <td id=\"T_0d4c5_row2_col46\" class=\"data row2 col46\" >0.09</td>\n",
       "      <td id=\"T_0d4c5_row2_col47\" class=\"data row2 col47\" >0.08</td>\n",
       "      <td id=\"T_0d4c5_row2_col48\" class=\"data row2 col48\" >0.06</td>\n",
       "      <td id=\"T_0d4c5_row2_col49\" class=\"data row2 col49\" >0.10</td>\n",
       "      <td id=\"T_0d4c5_row2_col50\" class=\"data row2 col50\" >0.03</td>\n",
       "      <td id=\"T_0d4c5_row2_col51\" class=\"data row2 col51\" >0.07</td>\n",
       "      <td id=\"T_0d4c5_row2_col52\" class=\"data row2 col52\" >0.04</td>\n",
       "      <td id=\"T_0d4c5_row2_col53\" class=\"data row2 col53\" >0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d4c5_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_0d4c5_row3_col0\" class=\"data row3 col0\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col1\" class=\"data row3 col1\" >243.20</td>\n",
       "      <td id=\"T_0d4c5_row3_col2\" class=\"data row3 col2\" >0.01</td>\n",
       "      <td id=\"T_0d4c5_row3_col3\" class=\"data row3 col3\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col4\" class=\"data row3 col4\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col5\" class=\"data row3 col5\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col6\" class=\"data row3 col6\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col7\" class=\"data row3 col7\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col8\" class=\"data row3 col8\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col9\" class=\"data row3 col9\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col10\" class=\"data row3 col10\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col11\" class=\"data row3 col11\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col12\" class=\"data row3 col12\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col13\" class=\"data row3 col13\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col14\" class=\"data row3 col14\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col15\" class=\"data row3 col15\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col16\" class=\"data row3 col16\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col17\" class=\"data row3 col17\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col18\" class=\"data row3 col18\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col19\" class=\"data row3 col19\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col20\" class=\"data row3 col20\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col21\" class=\"data row3 col21\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col22\" class=\"data row3 col22\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col23\" class=\"data row3 col23\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col24\" class=\"data row3 col24\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col25\" class=\"data row3 col25\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col26\" class=\"data row3 col26\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col27\" class=\"data row3 col27\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col28\" class=\"data row3 col28\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col29\" class=\"data row3 col29\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col30\" class=\"data row3 col30\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col31\" class=\"data row3 col31\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col32\" class=\"data row3 col32\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col33\" class=\"data row3 col33\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col34\" class=\"data row3 col34\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col35\" class=\"data row3 col35\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col36\" class=\"data row3 col36\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col37\" class=\"data row3 col37\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col38\" class=\"data row3 col38\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col39\" class=\"data row3 col39\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col40\" class=\"data row3 col40\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col41\" class=\"data row3 col41\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col42\" class=\"data row3 col42\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col43\" class=\"data row3 col43\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col44\" class=\"data row3 col44\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col45\" class=\"data row3 col45\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col46\" class=\"data row3 col46\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col47\" class=\"data row3 col47\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col48\" class=\"data row3 col48\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col49\" class=\"data row3 col49\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col50\" class=\"data row3 col50\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col51\" class=\"data row3 col51\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col52\" class=\"data row3 col52\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row3_col53\" class=\"data row3 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d4c5_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_0d4c5_row4_col0\" class=\"data row4 col0\" >0.14</td>\n",
       "      <td id=\"T_0d4c5_row4_col1\" class=\"data row4 col1\" >308.15</td>\n",
       "      <td id=\"T_0d4c5_row4_col2\" class=\"data row4 col2\" >10.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col3\" class=\"data row4 col3\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col4\" class=\"data row4 col4\" >3.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col5\" class=\"data row4 col5\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col6\" class=\"data row4 col6\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col7\" class=\"data row4 col7\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col8\" class=\"data row4 col8\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col9\" class=\"data row4 col9\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col10\" class=\"data row4 col10\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col11\" class=\"data row4 col11\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col12\" class=\"data row4 col12\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col13\" class=\"data row4 col13\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col14\" class=\"data row4 col14\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col15\" class=\"data row4 col15\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col16\" class=\"data row4 col16\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col17\" class=\"data row4 col17\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col18\" class=\"data row4 col18\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col19\" class=\"data row4 col19\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col20\" class=\"data row4 col20\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col21\" class=\"data row4 col21\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col22\" class=\"data row4 col22\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col23\" class=\"data row4 col23\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col24\" class=\"data row4 col24\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col25\" class=\"data row4 col25\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col26\" class=\"data row4 col26\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col27\" class=\"data row4 col27\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col28\" class=\"data row4 col28\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col29\" class=\"data row4 col29\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col30\" class=\"data row4 col30\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col31\" class=\"data row4 col31\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col32\" class=\"data row4 col32\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col33\" class=\"data row4 col33\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col34\" class=\"data row4 col34\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col35\" class=\"data row4 col35\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col36\" class=\"data row4 col36\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col37\" class=\"data row4 col37\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col38\" class=\"data row4 col38\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col39\" class=\"data row4 col39\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col40\" class=\"data row4 col40\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col41\" class=\"data row4 col41\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col42\" class=\"data row4 col42\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col43\" class=\"data row4 col43\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col44\" class=\"data row4 col44\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col45\" class=\"data row4 col45\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col46\" class=\"data row4 col46\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col47\" class=\"data row4 col47\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col48\" class=\"data row4 col48\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col49\" class=\"data row4 col49\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col50\" class=\"data row4 col50\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col51\" class=\"data row4 col51\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col52\" class=\"data row4 col52\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row4_col53\" class=\"data row4 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d4c5_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_0d4c5_row5_col0\" class=\"data row5 col0\" >0.30</td>\n",
       "      <td id=\"T_0d4c5_row5_col1\" class=\"data row5 col1\" >323.15</td>\n",
       "      <td id=\"T_0d4c5_row5_col2\" class=\"data row5 col2\" >26.80</td>\n",
       "      <td id=\"T_0d4c5_row5_col3\" class=\"data row5 col3\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col4\" class=\"data row5 col4\" >3.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col5\" class=\"data row5 col5\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col6\" class=\"data row5 col6\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col7\" class=\"data row5 col7\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col8\" class=\"data row5 col8\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col9\" class=\"data row5 col9\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col10\" class=\"data row5 col10\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col11\" class=\"data row5 col11\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col12\" class=\"data row5 col12\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col13\" class=\"data row5 col13\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col14\" class=\"data row5 col14\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col15\" class=\"data row5 col15\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col16\" class=\"data row5 col16\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col17\" class=\"data row5 col17\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col18\" class=\"data row5 col18\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col19\" class=\"data row5 col19\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col20\" class=\"data row5 col20\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col21\" class=\"data row5 col21\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col22\" class=\"data row5 col22\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col23\" class=\"data row5 col23\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col24\" class=\"data row5 col24\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col25\" class=\"data row5 col25\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col26\" class=\"data row5 col26\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col27\" class=\"data row5 col27\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col28\" class=\"data row5 col28\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col29\" class=\"data row5 col29\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col30\" class=\"data row5 col30\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col31\" class=\"data row5 col31\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col32\" class=\"data row5 col32\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col33\" class=\"data row5 col33\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col34\" class=\"data row5 col34\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col35\" class=\"data row5 col35\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col36\" class=\"data row5 col36\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col37\" class=\"data row5 col37\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col38\" class=\"data row5 col38\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col39\" class=\"data row5 col39\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col40\" class=\"data row5 col40\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col41\" class=\"data row5 col41\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col42\" class=\"data row5 col42\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col43\" class=\"data row5 col43\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col44\" class=\"data row5 col44\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col45\" class=\"data row5 col45\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col46\" class=\"data row5 col46\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col47\" class=\"data row5 col47\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col48\" class=\"data row5 col48\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col49\" class=\"data row5 col49\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col50\" class=\"data row5 col50\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col51\" class=\"data row5 col51\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col52\" class=\"data row5 col52\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row5_col53\" class=\"data row5 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d4c5_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_0d4c5_row6_col0\" class=\"data row6 col0\" >0.51</td>\n",
       "      <td id=\"T_0d4c5_row6_col1\" class=\"data row6 col1\" >342.59</td>\n",
       "      <td id=\"T_0d4c5_row6_col2\" class=\"data row6 col2\" >64.76</td>\n",
       "      <td id=\"T_0d4c5_row6_col3\" class=\"data row6 col3\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col4\" class=\"data row6 col4\" >5.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col5\" class=\"data row6 col5\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col6\" class=\"data row6 col6\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col7\" class=\"data row6 col7\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col8\" class=\"data row6 col8\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col9\" class=\"data row6 col9\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col10\" class=\"data row6 col10\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col11\" class=\"data row6 col11\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col12\" class=\"data row6 col12\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col13\" class=\"data row6 col13\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col14\" class=\"data row6 col14\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col15\" class=\"data row6 col15\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col16\" class=\"data row6 col16\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col17\" class=\"data row6 col17\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col18\" class=\"data row6 col18\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col19\" class=\"data row6 col19\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col20\" class=\"data row6 col20\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col21\" class=\"data row6 col21\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col22\" class=\"data row6 col22\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col23\" class=\"data row6 col23\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col24\" class=\"data row6 col24\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col25\" class=\"data row6 col25\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col26\" class=\"data row6 col26\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col27\" class=\"data row6 col27\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col28\" class=\"data row6 col28\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col29\" class=\"data row6 col29\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col30\" class=\"data row6 col30\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col31\" class=\"data row6 col31\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col32\" class=\"data row6 col32\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col33\" class=\"data row6 col33\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col34\" class=\"data row6 col34\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col35\" class=\"data row6 col35\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col36\" class=\"data row6 col36\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col37\" class=\"data row6 col37\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col38\" class=\"data row6 col38\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col39\" class=\"data row6 col39\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col40\" class=\"data row6 col40\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col41\" class=\"data row6 col41\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col42\" class=\"data row6 col42\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col43\" class=\"data row6 col43\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col44\" class=\"data row6 col44\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col45\" class=\"data row6 col45\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col46\" class=\"data row6 col46\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col47\" class=\"data row6 col47\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col48\" class=\"data row6 col48\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col49\" class=\"data row6 col49\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col50\" class=\"data row6 col50\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col51\" class=\"data row6 col51\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col52\" class=\"data row6 col52\" >0.00</td>\n",
       "      <td id=\"T_0d4c5_row6_col53\" class=\"data row6 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d4c5_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_0d4c5_row7_col0\" class=\"data row7 col0\" >0.95</td>\n",
       "      <td id=\"T_0d4c5_row7_col1\" class=\"data row7 col1\" >453.15</td>\n",
       "      <td id=\"T_0d4c5_row7_col2\" class=\"data row7 col2\" >499.90</td>\n",
       "      <td id=\"T_0d4c5_row7_col3\" class=\"data row7 col3\" >7.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col4\" class=\"data row7 col4\" >28.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col5\" class=\"data row7 col5\" >3.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col6\" class=\"data row7 col6\" >2.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col7\" class=\"data row7 col7\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col8\" class=\"data row7 col8\" >5.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col9\" class=\"data row7 col9\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col10\" class=\"data row7 col10\" >3.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col11\" class=\"data row7 col11\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col12\" class=\"data row7 col12\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col13\" class=\"data row7 col13\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col14\" class=\"data row7 col14\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col15\" class=\"data row7 col15\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col16\" class=\"data row7 col16\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col17\" class=\"data row7 col17\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col18\" class=\"data row7 col18\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col19\" class=\"data row7 col19\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col20\" class=\"data row7 col20\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col21\" class=\"data row7 col21\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col22\" class=\"data row7 col22\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col23\" class=\"data row7 col23\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col24\" class=\"data row7 col24\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col25\" class=\"data row7 col25\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col26\" class=\"data row7 col26\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col27\" class=\"data row7 col27\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col28\" class=\"data row7 col28\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col29\" class=\"data row7 col29\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col30\" class=\"data row7 col30\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col31\" class=\"data row7 col31\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col32\" class=\"data row7 col32\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col33\" class=\"data row7 col33\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col34\" class=\"data row7 col34\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col35\" class=\"data row7 col35\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col36\" class=\"data row7 col36\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col37\" class=\"data row7 col37\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col38\" class=\"data row7 col38\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col39\" class=\"data row7 col39\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col40\" class=\"data row7 col40\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col41\" class=\"data row7 col41\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col42\" class=\"data row7 col42\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col43\" class=\"data row7 col43\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col44\" class=\"data row7 col44\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col45\" class=\"data row7 col45\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col46\" class=\"data row7 col46\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col47\" class=\"data row7 col47\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col48\" class=\"data row7 col48\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col49\" class=\"data row7 col49\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col50\" class=\"data row7 col50\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col51\" class=\"data row7 col51\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col52\" class=\"data row7 col52\" >1.00</td>\n",
       "      <td id=\"T_0d4c5_row7_col53\" class=\"data row7 col53\" >1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc3677943d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataCO2f='../DS4B-CO2-data'+'/'+'dataCO2.csv'\n",
    "dataCO2=pd.read_csv(dataCO2f,sep=\";\",header=0)\n",
    "display(dataCO2)\n",
    "# describe() generates descriptive statistics\n",
    "display(dataCO2.describe().style.format(\"{0:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf1e66-0756-47f2-928d-d0d578613bba",
   "metadata": {},
   "source": [
    "## 2. Assessment of the stability of the original ML algorithm of Song *et al*. by *K*-fold cross validation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b3ba1f-2f90-441f-b931-28bc9e64748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# separation of the data set into two subsets: (1) training of the ANN & (2) test of the ANN\n",
    "# library used: pandas\n",
    "xdata = dataCO2.drop(['IL','cation','anion','x_CO2'],axis=1)\n",
    "ydata = dataCO2['x_CO2']\n",
    "\n",
    "#######################################################################################\n",
    "# ANN: 1 input layer (53 neurons) / 2 hidden layers (20 and 7 neurons) / 1 output layer (1 neuron) \n",
    "# library used: keras\n",
    "\n",
    "def defANN(shape,acthL):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name='iLayer'))\n",
    "    model.add(keras.layers.Dense(7, activation=acthL, name='hLayer'))\n",
    "    model.add(keras.layers.Dense(1, name='oLayer'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "acthL='tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a3138b7-5dc7-4804-93cb-caab9887319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[91mFold 0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 13:34:39.487653: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0287 - mae: 0.1371 - mse: 0.0287 - val_loss: 0.0172 - val_mae: 0.1043 - val_mse: 0.0172\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0151 - mae: 0.0953 - mse: 0.0151 - val_loss: 0.0154 - val_mae: 0.0955 - val_mse: 0.0154\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0141 - mae: 0.0907 - mse: 0.0141 - val_loss: 0.0138 - val_mae: 0.0893 - val_mse: 0.0138\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0122 - mae: 0.0831 - mse: 0.0122 - val_loss: 0.0120 - val_mae: 0.0826 - val_mse: 0.0120\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0098 - mae: 0.0734 - mse: 0.0098 - val_loss: 0.0092 - val_mae: 0.0698 - val_mse: 0.0092\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0674 - mse: 0.0086 - val_loss: 0.0086 - val_mae: 0.0660 - val_mse: 0.0086\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0657 - mse: 0.0083 - val_loss: 0.0088 - val_mae: 0.0677 - val_mse: 0.0088\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0075 - mae: 0.0617 - mse: 0.0075 - val_loss: 0.0080 - val_mae: 0.0634 - val_mse: 0.0080\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0073 - mae: 0.0609 - mse: 0.0073 - val_loss: 0.0074 - val_mae: 0.0613 - val_mse: 0.0074\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0596 - mse: 0.0068 - val_loss: 0.0076 - val_mae: 0.0633 - val_mse: 0.0076\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0588 - mse: 0.0065 - val_loss: 0.0067 - val_mae: 0.0600 - val_mse: 0.0067\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0577 - mse: 0.0062 - val_loss: 0.0067 - val_mae: 0.0595 - val_mse: 0.0067\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0570 - mse: 0.0060 - val_loss: 0.0063 - val_mae: 0.0592 - val_mse: 0.0063\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0055 - mae: 0.0543 - mse: 0.0055 - val_loss: 0.0080 - val_mae: 0.0708 - val_mse: 0.0080\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0054 - mae: 0.0536 - mse: 0.0054 - val_loss: 0.0065 - val_mae: 0.0631 - val_mse: 0.0065\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0551 - mse: 0.0056 - val_loss: 0.0058 - val_mae: 0.0549 - val_mse: 0.0058\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0053 - mae: 0.0540 - mse: 0.0053 - val_loss: 0.0058 - val_mae: 0.0571 - val_mse: 0.0058\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0051 - mae: 0.0522 - mse: 0.0051 - val_loss: 0.0052 - val_mae: 0.0522 - val_mse: 0.0052\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0518 - mse: 0.0050 - val_loss: 0.0060 - val_mae: 0.0566 - val_mse: 0.0060\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0049 - mae: 0.0511 - mse: 0.0049 - val_loss: 0.0057 - val_mae: 0.0564 - val_mse: 0.0057\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0049 - mae: 0.0520 - mse: 0.0049 - val_loss: 0.0048 - val_mae: 0.0507 - val_mse: 0.0048\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0048 - mae: 0.0513 - mse: 0.0048 - val_loss: 0.0053 - val_mae: 0.0554 - val_mse: 0.0053\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0496 - mse: 0.0046 - val_loss: 0.0052 - val_mae: 0.0536 - val_mse: 0.0052\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0048 - mae: 0.0517 - mse: 0.0048 - val_loss: 0.0052 - val_mae: 0.0533 - val_mse: 0.0052\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0498 - mse: 0.0045 - val_loss: 0.0045 - val_mae: 0.0499 - val_mse: 0.0045\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0501 - mse: 0.0045 - val_loss: 0.0043 - val_mae: 0.0482 - val_mse: 0.0043\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0504 - mse: 0.0045 - val_loss: 0.0048 - val_mae: 0.0525 - val_mse: 0.0048\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0043 - mae: 0.0488 - mse: 0.0043 - val_loss: 0.0071 - val_mae: 0.0686 - val_mse: 0.0071\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0043 - mae: 0.0492 - mse: 0.0043 - val_loss: 0.0040 - val_mae: 0.0470 - val_mse: 0.0040\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0507 - mse: 0.0045 - val_loss: 0.0041 - val_mae: 0.0481 - val_mse: 0.0041\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0042 - mae: 0.0491 - mse: 0.0042 - val_loss: 0.0039 - val_mae: 0.0469 - val_mse: 0.0039\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0042 - mae: 0.0489 - mse: 0.0042 - val_loss: 0.0047 - val_mae: 0.0544 - val_mse: 0.0047\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0041 - mae: 0.0487 - mse: 0.0041 - val_loss: 0.0039 - val_mae: 0.0474 - val_mse: 0.0039\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0040 - mae: 0.0480 - mse: 0.0040 - val_loss: 0.0037 - val_mae: 0.0452 - val_mse: 0.0037\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0042 - mae: 0.0496 - mse: 0.0042 - val_loss: 0.0042 - val_mae: 0.0483 - val_mse: 0.0042\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0039 - mae: 0.0472 - mse: 0.0039 - val_loss: 0.0037 - val_mae: 0.0464 - val_mse: 0.0037\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0041 - mae: 0.0493 - mse: 0.0041 - val_loss: 0.0038 - val_mae: 0.0452 - val_mse: 0.0038\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0040 - mae: 0.0480 - mse: 0.0040 - val_loss: 0.0040 - val_mae: 0.0499 - val_mse: 0.0040\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0040 - mae: 0.0487 - mse: 0.0040 - val_loss: 0.0036 - val_mae: 0.0449 - val_mse: 0.0036\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0457 - mse: 0.0037 - val_loss: 0.0037 - val_mae: 0.0470 - val_mse: 0.0037\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0468 - mse: 0.0038 - val_loss: 0.0040 - val_mae: 0.0486 - val_mse: 0.0040\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0040 - mae: 0.0480 - mse: 0.0040 - val_loss: 0.0036 - val_mae: 0.0444 - val_mse: 0.0036\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0039 - mae: 0.0478 - mse: 0.0039 - val_loss: 0.0036 - val_mae: 0.0467 - val_mse: 0.0036\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0459 - mse: 0.0037 - val_loss: 0.0051 - val_mae: 0.0562 - val_mse: 0.0051\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0458 - mse: 0.0036 - val_loss: 0.0045 - val_mae: 0.0539 - val_mse: 0.0045\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0463 - mse: 0.0037 - val_loss: 0.0035 - val_mae: 0.0439 - val_mse: 0.0035\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0039 - mae: 0.0475 - mse: 0.0039 - val_loss: 0.0050 - val_mae: 0.0574 - val_mse: 0.0050\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0472 - mse: 0.0038 - val_loss: 0.0033 - val_mae: 0.0426 - val_mse: 0.0033\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0454 - mse: 0.0036 - val_loss: 0.0037 - val_mae: 0.0464 - val_mse: 0.0037\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0454 - mse: 0.0036 - val_loss: 0.0032 - val_mae: 0.0425 - val_mse: 0.0032\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0452 - mse: 0.0035 - val_loss: 0.0032 - val_mae: 0.0424 - val_mse: 0.0032\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0461 - mse: 0.0037 - val_loss: 0.0050 - val_mae: 0.0559 - val_mse: 0.0050\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0469 - mse: 0.0038 - val_loss: 0.0045 - val_mae: 0.0524 - val_mse: 0.0045\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0452 - mse: 0.0036 - val_loss: 0.0037 - val_mae: 0.0474 - val_mse: 0.0037\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0457 - mse: 0.0036 - val_loss: 0.0035 - val_mae: 0.0466 - val_mse: 0.0035\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0461 - mse: 0.0036 - val_loss: 0.0033 - val_mae: 0.0440 - val_mse: 0.0033\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0451 - mse: 0.0035 - val_loss: 0.0031 - val_mae: 0.0421 - val_mse: 0.0031\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0444 - mse: 0.0035 - val_loss: 0.0043 - val_mae: 0.0530 - val_mse: 0.0043\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0447 - mse: 0.0034 - val_loss: 0.0032 - val_mae: 0.0439 - val_mse: 0.0032\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0452 - mse: 0.0035 - val_loss: 0.0035 - val_mae: 0.0468 - val_mse: 0.0035\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0441 - mse: 0.0033 - val_loss: 0.0037 - val_mae: 0.0452 - val_mse: 0.0037\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0455 - mse: 0.0036 - val_loss: 0.0032 - val_mae: 0.0418 - val_mse: 0.0032\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0455 - mse: 0.0035 - val_loss: 0.0055 - val_mae: 0.0594 - val_mse: 0.0055\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0456 - mse: 0.0035 - val_loss: 0.0033 - val_mae: 0.0451 - val_mse: 0.0033\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0443 - mse: 0.0034 - val_loss: 0.0034 - val_mae: 0.0428 - val_mse: 0.0034\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0438 - mse: 0.0033 - val_loss: 0.0030 - val_mae: 0.0423 - val_mse: 0.0030\n",
      "Epoch 67/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0432 - mse: 0.0033 - val_loss: 0.0029 - val_mae: 0.0412 - val_mse: 0.0029\n",
      "Epoch 68/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0454 - mse: 0.0035 - val_loss: 0.0035 - val_mae: 0.0442 - val_mse: 0.0035\n",
      "Epoch 69/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0436 - mse: 0.0033 - val_loss: 0.0033 - val_mae: 0.0449 - val_mse: 0.0033\n",
      "Epoch 70/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0455 - mse: 0.0035 - val_loss: 0.0030 - val_mae: 0.0415 - val_mse: 0.0030\n",
      "Epoch 71/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0437 - mse: 0.0033 - val_loss: 0.0049 - val_mae: 0.0555 - val_mse: 0.0049\n",
      "Epoch 72/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0449 - mse: 0.0035 - val_loss: 0.0033 - val_mae: 0.0419 - val_mse: 0.0033\n",
      "Epoch 73/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0439 - mse: 0.0033 - val_loss: 0.0043 - val_mae: 0.0509 - val_mse: 0.0043\n",
      "Epoch 74/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0431 - mse: 0.0032 - val_loss: 0.0029 - val_mae: 0.0397 - val_mse: 0.0029\n",
      "Epoch 75/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0430 - mse: 0.0033 - val_loss: 0.0032 - val_mae: 0.0438 - val_mse: 0.0032\n",
      "Epoch 76/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0450 - mse: 0.0034 - val_loss: 0.0035 - val_mae: 0.0460 - val_mse: 0.0035\n",
      "Epoch 77/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0466 - mse: 0.0037 - val_loss: 0.0040 - val_mae: 0.0485 - val_mse: 0.0040\n",
      "Epoch 78/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0434 - mse: 0.0032 - val_loss: 0.0029 - val_mae: 0.0400 - val_mse: 0.0029\n",
      "Epoch 79/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0440 - mse: 0.0033 - val_loss: 0.0028 - val_mae: 0.0395 - val_mse: 0.0028\n",
      "Epoch 80/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0433 - mse: 0.0032 - val_loss: 0.0030 - val_mae: 0.0410 - val_mse: 0.0030\n",
      "Epoch 81/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0443 - mse: 0.0034 - val_loss: 0.0036 - val_mae: 0.0449 - val_mse: 0.0036\n",
      "Epoch 82/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0442 - mse: 0.0034 - val_loss: 0.0044 - val_mae: 0.0532 - val_mse: 0.0044\n",
      "Epoch 83/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0448 - mse: 0.0034 - val_loss: 0.0032 - val_mae: 0.0437 - val_mse: 0.0032\n",
      "Epoch 84/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0429 - mse: 0.0032 - val_loss: 0.0038 - val_mae: 0.0477 - val_mse: 0.0038\n",
      "Epoch 85/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0444 - mse: 0.0034 - val_loss: 0.0032 - val_mae: 0.0414 - val_mse: 0.0032\n",
      "Epoch 86/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0432 - mse: 0.0033 - val_loss: 0.0059 - val_mae: 0.0640 - val_mse: 0.0059\n",
      "Epoch 87/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0427 - mse: 0.0032 - val_loss: 0.0065 - val_mae: 0.0666 - val_mse: 0.0065\n",
      "Epoch 88/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0443 - mse: 0.0034 - val_loss: 0.0042 - val_mae: 0.0505 - val_mse: 0.0042\n",
      "Epoch 89/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0445 - mse: 0.0034 - val_loss: 0.0048 - val_mae: 0.0568 - val_mse: 0.0048\n",
      "Epoch 90/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0428 - mse: 0.0032 - val_loss: 0.0028 - val_mae: 0.0388 - val_mse: 0.0028\n",
      "Epoch 91/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0452 - mse: 0.0035 - val_loss: 0.0036 - val_mae: 0.0481 - val_mse: 0.0036\n",
      "Epoch 92/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0419 - mse: 0.0031 - val_loss: 0.0035 - val_mae: 0.0469 - val_mse: 0.0035\n",
      "Epoch 93/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0451 - mse: 0.0035 - val_loss: 0.0039 - val_mae: 0.0476 - val_mse: 0.0039\n",
      "Epoch 94/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0440 - mse: 0.0033 - val_loss: 0.0043 - val_mae: 0.0525 - val_mse: 0.0043\n",
      "Epoch 95/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0443 - mse: 0.0034 - val_loss: 0.0027 - val_mae: 0.0389 - val_mse: 0.0027\n",
      "Epoch 96/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0418 - mse: 0.0031 - val_loss: 0.0038 - val_mae: 0.0465 - val_mse: 0.0038\n",
      "Epoch 97/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0440 - mse: 0.0034 - val_loss: 0.0043 - val_mae: 0.0536 - val_mse: 0.0043\n",
      "Epoch 98/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0429 - mse: 0.0032 - val_loss: 0.0029 - val_mae: 0.0415 - val_mse: 0.0029\n",
      "Epoch 99/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0434 - mse: 0.0033 - val_loss: 0.0033 - val_mae: 0.0425 - val_mse: 0.0033\n",
      "Epoch 100/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0431 - mse: 0.0032 - val_loss: 0.0071 - val_mae: 0.0696 - val_mse: 0.0071\n",
      "Epoch 101/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0424 - mse: 0.0031 - val_loss: 0.0031 - val_mae: 0.0427 - val_mse: 0.0031\n",
      "Epoch 102/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0440 - mse: 0.0033 - val_loss: 0.0044 - val_mae: 0.0537 - val_mse: 0.0044\n",
      "Epoch 103/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0422 - mse: 0.0031 - val_loss: 0.0045 - val_mae: 0.0547 - val_mse: 0.0045\n",
      "Epoch 104/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0426 - mse: 0.0031 - val_loss: 0.0033 - val_mae: 0.0462 - val_mse: 0.0033\n",
      "Epoch 105/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0430 - mse: 0.0032 - val_loss: 0.0029 - val_mae: 0.0404 - val_mse: 0.0029\n",
      "Epoch 106/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0431 - mse: 0.0033 - val_loss: 0.0028 - val_mae: 0.0402 - val_mse: 0.0028\n",
      "Epoch 107/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0448 - mse: 0.0035 - val_loss: 0.0027 - val_mae: 0.0391 - val_mse: 0.0027\n",
      "Epoch 108/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0429 - mse: 0.0032 - val_loss: 0.0028 - val_mae: 0.0390 - val_mse: 0.0028\n",
      "Epoch 109/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0428 - mse: 0.0032 - val_loss: 0.0035 - val_mae: 0.0435 - val_mse: 0.0035\n",
      "Epoch 110/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0432 - mse: 0.0032 - val_loss: 0.0030 - val_mae: 0.0399 - val_mse: 0.0030\n",
      "Epoch 111/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0431 - mse: 0.0032 - val_loss: 0.0034 - val_mae: 0.0450 - val_mse: 0.0034\n",
      "Epoch 112/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0419 - mse: 0.0031 - val_loss: 0.0033 - val_mae: 0.0458 - val_mse: 0.0033\n",
      "Epoch 113/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0420 - mse: 0.0031 - val_loss: 0.0041 - val_mae: 0.0497 - val_mse: 0.0041\n",
      "Epoch 114/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0417 - mse: 0.0031 - val_loss: 0.0031 - val_mae: 0.0408 - val_mse: 0.0031\n",
      "Epoch 115/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0428 - mse: 0.0032 - val_loss: 0.0065 - val_mae: 0.0661 - val_mse: 0.0065\n",
      "Epoch 116/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0429 - mse: 0.0032 - val_loss: 0.0033 - val_mae: 0.0424 - val_mse: 0.0033\n",
      "Epoch 117/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0425 - mse: 0.0032 - val_loss: 0.0029 - val_mae: 0.0414 - val_mse: 0.0029\n",
      "Epoch 118/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0429 - mse: 0.0032 - val_loss: 0.0047 - val_mae: 0.0543 - val_mse: 0.0047\n",
      "Epoch 119/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0427 - mse: 0.0032 - val_loss: 0.0037 - val_mae: 0.0489 - val_mse: 0.0037\n",
      "Epoch 120/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0409 - mse: 0.0030 - val_loss: 0.0026 - val_mae: 0.0384 - val_mse: 0.0026\n",
      "Epoch 121/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0442 - mse: 0.0034 - val_loss: 0.0031 - val_mae: 0.0418 - val_mse: 0.0031\n",
      "Epoch 122/200\n",
      "324/324 [==============================] - 0s 1ms/step - loss: 0.0030 - mae: 0.0414 - mse: 0.0030 - val_loss: 0.0031 - val_mae: 0.0406 - val_mse: 0.0031\n",
      "Epoch 123/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0429 - mse: 0.0032 - val_loss: 0.0029 - val_mae: 0.0390 - val_mse: 0.0029\n",
      "Epoch 124/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0440 - mse: 0.0034 - val_loss: 0.0030 - val_mae: 0.0427 - val_mse: 0.0030\n",
      "Epoch 125/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0426 - mse: 0.0032 - val_loss: 0.0031 - val_mae: 0.0436 - val_mse: 0.0031\n",
      "Epoch 126/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0419 - mse: 0.0031 - val_loss: 0.0036 - val_mae: 0.0472 - val_mse: 0.0036\n",
      "Epoch 127/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0437 - mse: 0.0033 - val_loss: 0.0027 - val_mae: 0.0388 - val_mse: 0.0027\n",
      "Epoch 128/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0427 - mse: 0.0032 - val_loss: 0.0032 - val_mae: 0.0443 - val_mse: 0.0032\n",
      "Epoch 129/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0448 - mse: 0.0034 - val_loss: 0.0033 - val_mae: 0.0433 - val_mse: 0.0033\n",
      "Epoch 130/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0431 - mse: 0.0032 - val_loss: 0.0036 - val_mae: 0.0446 - val_mse: 0.0036\n",
      "Epoch 131/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0413 - mse: 0.0030 - val_loss: 0.0027 - val_mae: 0.0388 - val_mse: 0.0027\n",
      "Epoch 132/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0440 - mse: 0.0034 - val_loss: 0.0033 - val_mae: 0.0424 - val_mse: 0.0033\n",
      "Epoch 133/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0406 - mse: 0.0029 - val_loss: 0.0036 - val_mae: 0.0476 - val_mse: 0.0036\n",
      "Epoch 134/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0405 - mse: 0.0029 - val_loss: 0.0033 - val_mae: 0.0452 - val_mse: 0.0033\n",
      "Epoch 135/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0433 - mse: 0.0033 - val_loss: 0.0030 - val_mae: 0.0395 - val_mse: 0.0030\n",
      "Epoch 135: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 726us/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.006755646134182838    std:  0.052349376306583414    MAE:  0.03862610470312658     R2:  0.9760550075666986\n",
      "Test. mean:  -0.00940937847050425    std:  0.05365416284427968    MAE:  0.03951845977216028     R2:  0.9753779962772553\n",
      "\u001b[1m\u001b[91mFold 1\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.2713 - mae: 0.3726 - mse: 0.2713 - val_loss: 0.0373 - val_mae: 0.1592 - val_mse: 0.0373\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0379 - mae: 0.1585 - mse: 0.0379 - val_loss: 0.0324 - val_mae: 0.1477 - val_mse: 0.0324\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0310 - mae: 0.1433 - mse: 0.0310 - val_loss: 0.0295 - val_mae: 0.1401 - val_mse: 0.0295\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0273 - mae: 0.1340 - mse: 0.0273 - val_loss: 0.0259 - val_mae: 0.1310 - val_mse: 0.0259\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0247 - mae: 0.1276 - mse: 0.0247 - val_loss: 0.0255 - val_mae: 0.1276 - val_mse: 0.0255\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0223 - mae: 0.1214 - mse: 0.0223 - val_loss: 0.0235 - val_mae: 0.1239 - val_mse: 0.0235\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0205 - mae: 0.1152 - mse: 0.0205 - val_loss: 0.0194 - val_mae: 0.1117 - val_mse: 0.0194\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0182 - mae: 0.1080 - mse: 0.0182 - val_loss: 0.0165 - val_mae: 0.1025 - val_mse: 0.0165\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0174 - mae: 0.1042 - mse: 0.0174 - val_loss: 0.0150 - val_mae: 0.0981 - val_mse: 0.0150\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0152 - mae: 0.0978 - mse: 0.0152 - val_loss: 0.0143 - val_mae: 0.0944 - val_mse: 0.0143\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0147 - mae: 0.0950 - mse: 0.0147 - val_loss: 0.0146 - val_mae: 0.0953 - val_mse: 0.0146\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0141 - mae: 0.0923 - mse: 0.0141 - val_loss: 0.0135 - val_mae: 0.0916 - val_mse: 0.0135\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0131 - mae: 0.0879 - mse: 0.0131 - val_loss: 0.0114 - val_mae: 0.0799 - val_mse: 0.0114\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0109 - mae: 0.0781 - mse: 0.0109 - val_loss: 0.0102 - val_mae: 0.0753 - val_mse: 0.0102\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0706 - mse: 0.0091 - val_loss: 0.0090 - val_mae: 0.0692 - val_mse: 0.0090\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0687 - mse: 0.0087 - val_loss: 0.0080 - val_mae: 0.0640 - val_mse: 0.0080\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0662 - mse: 0.0082 - val_loss: 0.0081 - val_mae: 0.0660 - val_mse: 0.0081\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0665 - mse: 0.0082 - val_loss: 0.0072 - val_mae: 0.0604 - val_mse: 0.0072\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0078 - mae: 0.0642 - mse: 0.0078 - val_loss: 0.0069 - val_mae: 0.0589 - val_mse: 0.0069\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0073 - mae: 0.0616 - mse: 0.0073 - val_loss: 0.0067 - val_mae: 0.0582 - val_mse: 0.0067\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0070 - mae: 0.0599 - mse: 0.0070 - val_loss: 0.0065 - val_mae: 0.0567 - val_mse: 0.0065\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0611 - mse: 0.0071 - val_loss: 0.0077 - val_mae: 0.0669 - val_mse: 0.0077\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0593 - mse: 0.0065 - val_loss: 0.0058 - val_mae: 0.0556 - val_mse: 0.0058\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0568 - mse: 0.0060 - val_loss: 0.0052 - val_mae: 0.0530 - val_mse: 0.0052\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0587 - mse: 0.0062 - val_loss: 0.0052 - val_mae: 0.0528 - val_mse: 0.0052\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0564 - mse: 0.0058 - val_loss: 0.0051 - val_mae: 0.0527 - val_mse: 0.0051\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0574 - mse: 0.0060 - val_loss: 0.0049 - val_mae: 0.0514 - val_mse: 0.0049\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0559 - mse: 0.0057 - val_loss: 0.0051 - val_mae: 0.0528 - val_mse: 0.0051\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0563 - mse: 0.0058 - val_loss: 0.0051 - val_mae: 0.0537 - val_mse: 0.0051\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0564 - mse: 0.0058 - val_loss: 0.0049 - val_mae: 0.0520 - val_mse: 0.0049\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0556 - mse: 0.0056 - val_loss: 0.0050 - val_mae: 0.0517 - val_mse: 0.0050\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 0s 1ms/step - loss: 0.0056 - mae: 0.0553 - mse: 0.0056 - val_loss: 0.0050 - val_mae: 0.0533 - val_mse: 0.0050\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0561 - mse: 0.0057 - val_loss: 0.0049 - val_mae: 0.0529 - val_mse: 0.0049\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0055 - mae: 0.0549 - mse: 0.0055 - val_loss: 0.0056 - val_mae: 0.0578 - val_mse: 0.0056\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0055 - mae: 0.0549 - mse: 0.0055 - val_loss: 0.0049 - val_mae: 0.0533 - val_mse: 0.0049\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0054 - mae: 0.0545 - mse: 0.0054 - val_loss: 0.0052 - val_mae: 0.0550 - val_mse: 0.0052\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0055 - mae: 0.0551 - mse: 0.0055 - val_loss: 0.0048 - val_mae: 0.0517 - val_mse: 0.0048\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0053 - mae: 0.0539 - mse: 0.0053 - val_loss: 0.0049 - val_mae: 0.0520 - val_mse: 0.0049\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0053 - mae: 0.0535 - mse: 0.0053 - val_loss: 0.0046 - val_mae: 0.0499 - val_mse: 0.0046\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 0s 1ms/step - loss: 0.0052 - mae: 0.0531 - mse: 0.0052 - val_loss: 0.0050 - val_mae: 0.0536 - val_mse: 0.0050\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 0s 2ms/step - loss: 0.0053 - mae: 0.0539 - mse: 0.0053 - val_loss: 0.0047 - val_mae: 0.0501 - val_mse: 0.0047\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0052 - mae: 0.0534 - mse: 0.0052 - val_loss: 0.0045 - val_mae: 0.0499 - val_mse: 0.0045\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0055 - mae: 0.0547 - mse: 0.0055 - val_loss: 0.0043 - val_mae: 0.0481 - val_mse: 0.0043\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0525 - mse: 0.0050 - val_loss: 0.0049 - val_mae: 0.0528 - val_mse: 0.0049\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0049 - mae: 0.0521 - mse: 0.0049 - val_loss: 0.0051 - val_mae: 0.0549 - val_mse: 0.0051\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0524 - mse: 0.0050 - val_loss: 0.0047 - val_mae: 0.0512 - val_mse: 0.0047\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0048 - mae: 0.0511 - mse: 0.0048 - val_loss: 0.0045 - val_mae: 0.0506 - val_mse: 0.0045\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0508 - mse: 0.0047 - val_loss: 0.0040 - val_mae: 0.0460 - val_mse: 0.0040\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0048 - mae: 0.0515 - mse: 0.0048 - val_loss: 0.0053 - val_mae: 0.0557 - val_mse: 0.0053\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0511 - mse: 0.0047 - val_loss: 0.0064 - val_mae: 0.0649 - val_mse: 0.0064\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0508 - mse: 0.0046 - val_loss: 0.0052 - val_mae: 0.0559 - val_mse: 0.0052\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0044 - mae: 0.0489 - mse: 0.0044 - val_loss: 0.0040 - val_mae: 0.0468 - val_mse: 0.0040\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0044 - mae: 0.0497 - mse: 0.0044 - val_loss: 0.0039 - val_mae: 0.0464 - val_mse: 0.0039\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0044 - mae: 0.0492 - mse: 0.0044 - val_loss: 0.0043 - val_mae: 0.0503 - val_mse: 0.0043\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0043 - mae: 0.0493 - mse: 0.0043 - val_loss: 0.0040 - val_mae: 0.0475 - val_mse: 0.0040\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0042 - mae: 0.0486 - mse: 0.0042 - val_loss: 0.0039 - val_mae: 0.0468 - val_mse: 0.0039\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0042 - mae: 0.0486 - mse: 0.0042 - val_loss: 0.0042 - val_mae: 0.0497 - val_mse: 0.0042\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0041 - mae: 0.0480 - mse: 0.0041 - val_loss: 0.0041 - val_mae: 0.0490 - val_mse: 0.0041\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0040 - mae: 0.0474 - mse: 0.0040 - val_loss: 0.0037 - val_mae: 0.0440 - val_mse: 0.0037\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0041 - mae: 0.0481 - mse: 0.0041 - val_loss: 0.0046 - val_mae: 0.0521 - val_mse: 0.0046\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0043 - mae: 0.0499 - mse: 0.0043 - val_loss: 0.0041 - val_mae: 0.0491 - val_mse: 0.0041\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0040 - mae: 0.0474 - mse: 0.0040 - val_loss: 0.0044 - val_mae: 0.0515 - val_mse: 0.0044\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0040 - mae: 0.0479 - mse: 0.0040 - val_loss: 0.0040 - val_mae: 0.0486 - val_mse: 0.0040\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0040 - mae: 0.0480 - mse: 0.0040 - val_loss: 0.0058 - val_mae: 0.0623 - val_mse: 0.0058\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0041 - mae: 0.0487 - mse: 0.0041 - val_loss: 0.0044 - val_mae: 0.0520 - val_mse: 0.0044\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0459 - mse: 0.0037 - val_loss: 0.0039 - val_mae: 0.0478 - val_mse: 0.0039\n",
      "Epoch 67/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0455 - mse: 0.0036 - val_loss: 0.0035 - val_mae: 0.0446 - val_mse: 0.0035\n",
      "Epoch 68/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0451 - mse: 0.0035 - val_loss: 0.0039 - val_mae: 0.0468 - val_mse: 0.0039\n",
      "Epoch 69/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0449 - mse: 0.0035 - val_loss: 0.0033 - val_mae: 0.0429 - val_mse: 0.0033\n",
      "Epoch 70/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0432 - mse: 0.0033 - val_loss: 0.0037 - val_mae: 0.0459 - val_mse: 0.0037\n",
      "Epoch 71/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0423 - mse: 0.0031 - val_loss: 0.0031 - val_mae: 0.0424 - val_mse: 0.0031\n",
      "Epoch 72/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0409 - mse: 0.0030 - val_loss: 0.0033 - val_mae: 0.0454 - val_mse: 0.0033\n",
      "Epoch 73/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0445 - mse: 0.0034 - val_loss: 0.0029 - val_mae: 0.0392 - val_mse: 0.0029\n",
      "Epoch 74/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0409 - mse: 0.0029 - val_loss: 0.0030 - val_mae: 0.0416 - val_mse: 0.0030\n",
      "Epoch 75/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0418 - mse: 0.0031 - val_loss: 0.0033 - val_mae: 0.0444 - val_mse: 0.0033\n",
      "Epoch 76/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0408 - mse: 0.0029 - val_loss: 0.0030 - val_mae: 0.0422 - val_mse: 0.0030\n",
      "Epoch 77/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0405 - mse: 0.0029 - val_loss: 0.0029 - val_mae: 0.0396 - val_mse: 0.0029\n",
      "Epoch 78/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0028 - mae: 0.0394 - mse: 0.0028 - val_loss: 0.0028 - val_mae: 0.0385 - val_mse: 0.0028\n",
      "Epoch 79/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0028 - mae: 0.0395 - mse: 0.0028 - val_loss: 0.0039 - val_mae: 0.0492 - val_mse: 0.0039\n",
      "Epoch 80/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0417 - mse: 0.0031 - val_loss: 0.0027 - val_mae: 0.0376 - val_mse: 0.0027\n",
      "Epoch 81/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0028 - mae: 0.0393 - mse: 0.0028 - val_loss: 0.0028 - val_mae: 0.0379 - val_mse: 0.0028\n",
      "Epoch 82/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0027 - mae: 0.0384 - mse: 0.0027 - val_loss: 0.0026 - val_mae: 0.0370 - val_mse: 0.0026\n",
      "Epoch 83/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0027 - mae: 0.0384 - mse: 0.0027 - val_loss: 0.0032 - val_mae: 0.0414 - val_mse: 0.0032\n",
      "Epoch 84/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0027 - mae: 0.0384 - mse: 0.0027 - val_loss: 0.0031 - val_mae: 0.0428 - val_mse: 0.0031\n",
      "Epoch 85/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0398 - mse: 0.0029 - val_loss: 0.0026 - val_mae: 0.0366 - val_mse: 0.0026\n",
      "Epoch 86/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0027 - mae: 0.0379 - mse: 0.0027 - val_loss: 0.0025 - val_mae: 0.0363 - val_mse: 0.0025\n",
      "Epoch 87/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0026 - mae: 0.0377 - mse: 0.0026 - val_loss: 0.0025 - val_mae: 0.0359 - val_mse: 0.0025\n",
      "Epoch 88/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0026 - mae: 0.0378 - mse: 0.0026 - val_loss: 0.0028 - val_mae: 0.0397 - val_mse: 0.0028\n",
      "Epoch 89/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0026 - mae: 0.0379 - mse: 0.0026 - val_loss: 0.0027 - val_mae: 0.0383 - val_mse: 0.0027\n",
      "Epoch 90/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0026 - mae: 0.0380 - mse: 0.0026 - val_loss: 0.0026 - val_mae: 0.0367 - val_mse: 0.0026\n",
      "Epoch 91/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0027 - mae: 0.0384 - mse: 0.0027 - val_loss: 0.0026 - val_mae: 0.0369 - val_mse: 0.0026\n",
      "Epoch 92/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0026 - mae: 0.0377 - mse: 0.0026 - val_loss: 0.0027 - val_mae: 0.0388 - val_mse: 0.0027\n",
      "Epoch 93/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0025 - mae: 0.0369 - mse: 0.0025 - val_loss: 0.0026 - val_mae: 0.0356 - val_mse: 0.0026\n",
      "Epoch 94/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0024 - mae: 0.0353 - mse: 0.0024 - val_loss: 0.0023 - val_mae: 0.0344 - val_mse: 0.0023\n",
      "Epoch 95/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0025 - mae: 0.0370 - mse: 0.0025 - val_loss: 0.0032 - val_mae: 0.0441 - val_mse: 0.0032\n",
      "Epoch 96/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0024 - mae: 0.0359 - mse: 0.0024 - val_loss: 0.0026 - val_mae: 0.0393 - val_mse: 0.0026\n",
      "Epoch 97/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0024 - mae: 0.0361 - mse: 0.0024 - val_loss: 0.0026 - val_mae: 0.0393 - val_mse: 0.0026\n",
      "Epoch 98/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0025 - mae: 0.0368 - mse: 0.0025 - val_loss: 0.0022 - val_mae: 0.0330 - val_mse: 0.0022\n",
      "Epoch 99/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0025 - mae: 0.0366 - mse: 0.0025 - val_loss: 0.0024 - val_mae: 0.0358 - val_mse: 0.0024\n",
      "Epoch 100/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0024 - mae: 0.0356 - mse: 0.0024 - val_loss: 0.0022 - val_mae: 0.0334 - val_mse: 0.0022\n",
      "Epoch 101/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0023 - mae: 0.0354 - mse: 0.0023 - val_loss: 0.0022 - val_mae: 0.0331 - val_mse: 0.0022\n",
      "Epoch 102/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0023 - mae: 0.0352 - mse: 0.0023 - val_loss: 0.0022 - val_mae: 0.0343 - val_mse: 0.0022\n",
      "Epoch 103/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0024 - mae: 0.0361 - mse: 0.0024 - val_loss: 0.0029 - val_mae: 0.0422 - val_mse: 0.0029\n",
      "Epoch 104/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0026 - mae: 0.0375 - mse: 0.0026 - val_loss: 0.0024 - val_mae: 0.0351 - val_mse: 0.0024\n",
      "Epoch 105/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0024 - mae: 0.0358 - mse: 0.0024 - val_loss: 0.0031 - val_mae: 0.0412 - val_mse: 0.0031\n",
      "Epoch 106/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0024 - mae: 0.0361 - mse: 0.0024 - val_loss: 0.0023 - val_mae: 0.0338 - val_mse: 0.0023\n",
      "Epoch 107/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0023 - mae: 0.0350 - mse: 0.0023 - val_loss: 0.0023 - val_mae: 0.0371 - val_mse: 0.0023\n",
      "Epoch 108/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0025 - mae: 0.0369 - mse: 0.0025 - val_loss: 0.0023 - val_mae: 0.0350 - val_mse: 0.0023\n",
      "Epoch 109/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0023 - mae: 0.0354 - mse: 0.0023 - val_loss: 0.0021 - val_mae: 0.0322 - val_mse: 0.0021\n",
      "Epoch 110/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0022 - mae: 0.0344 - mse: 0.0022 - val_loss: 0.0021 - val_mae: 0.0343 - val_mse: 0.0021\n",
      "Epoch 111/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0022 - mae: 0.0341 - mse: 0.0022 - val_loss: 0.0022 - val_mae: 0.0345 - val_mse: 0.0022\n",
      "Epoch 112/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0024 - mae: 0.0361 - mse: 0.0024 - val_loss: 0.0027 - val_mae: 0.0415 - val_mse: 0.0027\n",
      "Epoch 113/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0024 - mae: 0.0359 - mse: 0.0024 - val_loss: 0.0021 - val_mae: 0.0329 - val_mse: 0.0021\n",
      "Epoch 114/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0025 - mae: 0.0367 - mse: 0.0025 - val_loss: 0.0036 - val_mae: 0.0486 - val_mse: 0.0036\n",
      "Epoch 115/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0023 - mae: 0.0352 - mse: 0.0023 - val_loss: 0.0023 - val_mae: 0.0363 - val_mse: 0.0023\n",
      "Epoch 116/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0023 - mae: 0.0350 - mse: 0.0023 - val_loss: 0.0026 - val_mae: 0.0380 - val_mse: 0.0026\n",
      "Epoch 117/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0022 - mae: 0.0347 - mse: 0.0022 - val_loss: 0.0023 - val_mae: 0.0353 - val_mse: 0.0023\n",
      "Epoch 118/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0023 - mae: 0.0350 - mse: 0.0023 - val_loss: 0.0024 - val_mae: 0.0376 - val_mse: 0.0024\n",
      "Epoch 119/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0023 - mae: 0.0355 - mse: 0.0023 - val_loss: 0.0022 - val_mae: 0.0332 - val_mse: 0.0022\n",
      "Epoch 120/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0022 - mae: 0.0348 - mse: 0.0022 - val_loss: 0.0020 - val_mae: 0.0324 - val_mse: 0.0020\n",
      "Epoch 121/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0023 - mae: 0.0352 - mse: 0.0023 - val_loss: 0.0024 - val_mae: 0.0360 - val_mse: 0.0024\n",
      "Epoch 122/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0022 - mae: 0.0348 - mse: 0.0022 - val_loss: 0.0023 - val_mae: 0.0348 - val_mse: 0.0023\n",
      "Epoch 123/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0025 - mae: 0.0370 - mse: 0.0025 - val_loss: 0.0025 - val_mae: 0.0368 - val_mse: 0.0025\n",
      "Epoch 124/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0022 - mae: 0.0342 - mse: 0.0022 - val_loss: 0.0022 - val_mae: 0.0363 - val_mse: 0.0022\n",
      "Epoch 125/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0023 - mae: 0.0353 - mse: 0.0023 - val_loss: 0.0028 - val_mae: 0.0397 - val_mse: 0.0028\n",
      "Epoch 126/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0021 - mae: 0.0339 - mse: 0.0021 - val_loss: 0.0019 - val_mae: 0.0313 - val_mse: 0.0019\n",
      "Epoch 127/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0022 - mae: 0.0348 - mse: 0.0022 - val_loss: 0.0020 - val_mae: 0.0330 - val_mse: 0.0020\n",
      "Epoch 128/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0023 - mae: 0.0353 - mse: 0.0023 - val_loss: 0.0026 - val_mae: 0.0369 - val_mse: 0.0026\n",
      "Epoch 129/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0023 - mae: 0.0354 - mse: 0.0023 - val_loss: 0.0024 - val_mae: 0.0364 - val_mse: 0.0024\n",
      "Epoch 130/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0024 - mae: 0.0367 - mse: 0.0024 - val_loss: 0.0021 - val_mae: 0.0331 - val_mse: 0.0021\n",
      "Epoch 131/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0022 - mae: 0.0343 - mse: 0.0022 - val_loss: 0.0021 - val_mae: 0.0345 - val_mse: 0.0021\n",
      "Epoch 132/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0022 - mae: 0.0346 - mse: 0.0022 - val_loss: 0.0025 - val_mae: 0.0388 - val_mse: 0.0025\n",
      "Epoch 133/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0022 - mae: 0.0349 - mse: 0.0022 - val_loss: 0.0022 - val_mae: 0.0329 - val_mse: 0.0022\n",
      "Epoch 134/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0022 - mae: 0.0342 - mse: 0.0022 - val_loss: 0.0020 - val_mae: 0.0334 - val_mse: 0.0020\n",
      "Epoch 135/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0022 - mae: 0.0348 - mse: 0.0022 - val_loss: 0.0025 - val_mae: 0.0390 - val_mse: 0.0025\n",
      "Epoch 136/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0021 - mae: 0.0340 - mse: 0.0021 - val_loss: 0.0024 - val_mae: 0.0383 - val_mse: 0.0024\n",
      "Epoch 137/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0023 - mae: 0.0353 - mse: 0.0023 - val_loss: 0.0022 - val_mae: 0.0335 - val_mse: 0.0022\n",
      "Epoch 138/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0021 - mae: 0.0340 - mse: 0.0021 - val_loss: 0.0024 - val_mae: 0.0356 - val_mse: 0.0024\n",
      "Epoch 139/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0024 - mae: 0.0370 - mse: 0.0024 - val_loss: 0.0023 - val_mae: 0.0374 - val_mse: 0.0023\n",
      "Epoch 140/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0023 - mae: 0.0354 - mse: 0.0023 - val_loss: 0.0020 - val_mae: 0.0341 - val_mse: 0.0020\n",
      "Epoch 141/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0022 - mae: 0.0348 - mse: 0.0022 - val_loss: 0.0026 - val_mae: 0.0407 - val_mse: 0.0026\n",
      "Epoch 141: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 684us/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  0.024138522381020103    std:  0.042970916563050875    MAE:  0.038906241028482585     R2:  0.9836255840872067\n",
      "Test. mean:  0.02591701733272193    std:  0.04363719620663232    MAE:  0.040651120689959054     R2:  0.9829014860048862\n",
      "\u001b[1m\u001b[91mFold 2\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.1029 - mae: 0.2476 - mse: 0.1029 - val_loss: 0.0537 - val_mae: 0.1966 - val_mse: 0.0537\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2006 - mse: 0.0560 - val_loss: 0.0541 - val_mae: 0.1967 - val_mse: 0.0541\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2015 - mse: 0.0562 - val_loss: 0.0542 - val_mae: 0.1975 - val_mse: 0.0542\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2016 - mse: 0.0562 - val_loss: 0.0542 - val_mae: 0.1968 - val_mse: 0.0542\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2013 - mse: 0.0562 - val_loss: 0.0541 - val_mae: 0.1973 - val_mse: 0.0541\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2015 - mse: 0.0563 - val_loss: 0.0544 - val_mae: 0.1994 - val_mse: 0.0544\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0551 - mae: 0.1992 - mse: 0.0551 - val_loss: 0.0506 - val_mae: 0.1890 - val_mse: 0.0506\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0328 - mae: 0.1453 - mse: 0.0328 - val_loss: 0.0197 - val_mae: 0.1115 - val_mse: 0.0197\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0194 - mae: 0.1111 - mse: 0.0194 - val_loss: 0.0170 - val_mae: 0.1052 - val_mse: 0.0170\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0165 - mae: 0.1009 - mse: 0.0165 - val_loss: 0.0129 - val_mae: 0.0887 - val_mse: 0.0129\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0136 - mae: 0.0908 - mse: 0.0136 - val_loss: 0.0109 - val_mae: 0.0814 - val_mse: 0.0109\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0117 - mae: 0.0844 - mse: 0.0117 - val_loss: 0.0100 - val_mae: 0.0778 - val_mse: 0.0100\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0106 - mae: 0.0795 - mse: 0.0106 - val_loss: 0.0092 - val_mae: 0.0735 - val_mse: 0.0092\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0101 - mae: 0.0769 - mse: 0.0101 - val_loss: 0.0102 - val_mae: 0.0785 - val_mse: 0.0102\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0099 - mae: 0.0764 - mse: 0.0099 - val_loss: 0.0087 - val_mae: 0.0711 - val_mse: 0.0087\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0098 - mae: 0.0762 - mse: 0.0098 - val_loss: 0.0088 - val_mae: 0.0712 - val_mse: 0.0088\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0095 - mae: 0.0749 - mse: 0.0095 - val_loss: 0.0085 - val_mae: 0.0704 - val_mse: 0.0085\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0094 - mae: 0.0741 - mse: 0.0094 - val_loss: 0.0087 - val_mae: 0.0728 - val_mse: 0.0087\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0093 - mae: 0.0739 - mse: 0.0093 - val_loss: 0.0096 - val_mae: 0.0753 - val_mse: 0.0096\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0095 - mae: 0.0745 - mse: 0.0095 - val_loss: 0.0101 - val_mae: 0.0808 - val_mse: 0.0101\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0733 - mse: 0.0092 - val_loss: 0.0082 - val_mae: 0.0688 - val_mse: 0.0082\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0733 - mse: 0.0092 - val_loss: 0.0084 - val_mae: 0.0712 - val_mse: 0.0084\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0732 - mse: 0.0091 - val_loss: 0.0086 - val_mae: 0.0724 - val_mse: 0.0086\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0725 - mse: 0.0089 - val_loss: 0.0081 - val_mae: 0.0685 - val_mse: 0.0081\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0716 - mse: 0.0088 - val_loss: 0.0085 - val_mae: 0.0690 - val_mse: 0.0085\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0720 - mse: 0.0089 - val_loss: 0.0081 - val_mae: 0.0679 - val_mse: 0.0081\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0728 - mse: 0.0091 - val_loss: 0.0088 - val_mae: 0.0746 - val_mse: 0.0088\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0718 - mse: 0.0088 - val_loss: 0.0079 - val_mae: 0.0664 - val_mse: 0.0079\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0724 - mse: 0.0090 - val_loss: 0.0086 - val_mae: 0.0731 - val_mse: 0.0086\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0713 - mse: 0.0088 - val_loss: 0.0081 - val_mae: 0.0663 - val_mse: 0.0081\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0710 - mse: 0.0087 - val_loss: 0.0086 - val_mae: 0.0677 - val_mse: 0.0086\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0724 - mse: 0.0089 - val_loss: 0.0078 - val_mae: 0.0661 - val_mse: 0.0078\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0723 - mse: 0.0089 - val_loss: 0.0079 - val_mae: 0.0683 - val_mse: 0.0079\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0725 - mse: 0.0090 - val_loss: 0.0083 - val_mae: 0.0716 - val_mse: 0.0083\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0730 - mse: 0.0090 - val_loss: 0.0081 - val_mae: 0.0703 - val_mse: 0.0081\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0723 - mse: 0.0089 - val_loss: 0.0078 - val_mae: 0.0661 - val_mse: 0.0078\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0718 - mse: 0.0087 - val_loss: 0.0088 - val_mae: 0.0697 - val_mse: 0.0088\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0721 - mse: 0.0088 - val_loss: 0.0079 - val_mae: 0.0684 - val_mse: 0.0079\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0720 - mse: 0.0088 - val_loss: 0.0083 - val_mae: 0.0679 - val_mse: 0.0083\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0705 - mse: 0.0085 - val_loss: 0.0078 - val_mae: 0.0661 - val_mse: 0.0078\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0708 - mse: 0.0085 - val_loss: 0.0078 - val_mae: 0.0682 - val_mse: 0.0078\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0717 - mse: 0.0087 - val_loss: 0.0078 - val_mae: 0.0658 - val_mse: 0.0078\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0717 - mse: 0.0087 - val_loss: 0.0080 - val_mae: 0.0660 - val_mse: 0.0080\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0716 - mse: 0.0087 - val_loss: 0.0079 - val_mae: 0.0658 - val_mse: 0.0079\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0705 - mse: 0.0085 - val_loss: 0.0078 - val_mae: 0.0677 - val_mse: 0.0078\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0709 - mse: 0.0086 - val_loss: 0.0090 - val_mae: 0.0708 - val_mse: 0.0090\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0725 - mse: 0.0088 - val_loss: 0.0078 - val_mae: 0.0684 - val_mse: 0.0078\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0710 - mse: 0.0086 - val_loss: 0.0092 - val_mae: 0.0722 - val_mse: 0.0092\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0726 - mse: 0.0089 - val_loss: 0.0079 - val_mae: 0.0693 - val_mse: 0.0079\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0706 - mse: 0.0085 - val_loss: 0.0083 - val_mae: 0.0719 - val_mse: 0.0083\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0707 - mse: 0.0085 - val_loss: 0.0078 - val_mae: 0.0674 - val_mse: 0.0078\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0716 - mse: 0.0087 - val_loss: 0.0077 - val_mae: 0.0675 - val_mse: 0.0077\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0715 - mse: 0.0087 - val_loss: 0.0078 - val_mae: 0.0661 - val_mse: 0.0078\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0721 - mse: 0.0088 - val_loss: 0.0084 - val_mae: 0.0674 - val_mse: 0.0084\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0716 - mse: 0.0087 - val_loss: 0.0082 - val_mae: 0.0715 - val_mse: 0.0082\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0716 - mse: 0.0087 - val_loss: 0.0087 - val_mae: 0.0693 - val_mse: 0.0087\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0715 - mse: 0.0086 - val_loss: 0.0097 - val_mae: 0.0808 - val_mse: 0.0097\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0718 - mse: 0.0087 - val_loss: 0.0078 - val_mae: 0.0680 - val_mse: 0.0078\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0718 - mse: 0.0087 - val_loss: 0.0078 - val_mae: 0.0679 - val_mse: 0.0078\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0712 - mse: 0.0086 - val_loss: 0.0081 - val_mae: 0.0707 - val_mse: 0.0081\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0707 - mse: 0.0085 - val_loss: 0.0079 - val_mae: 0.0697 - val_mse: 0.0079\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0713 - mse: 0.0086 - val_loss: 0.0078 - val_mae: 0.0682 - val_mse: 0.0078\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0715 - mse: 0.0086 - val_loss: 0.0080 - val_mae: 0.0661 - val_mse: 0.0080\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0706 - mse: 0.0085 - val_loss: 0.0079 - val_mae: 0.0690 - val_mse: 0.0079\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0710 - mse: 0.0086 - val_loss: 0.0080 - val_mae: 0.0662 - val_mse: 0.0080\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0710 - mse: 0.0085 - val_loss: 0.0078 - val_mae: 0.0656 - val_mse: 0.0078\n",
      "Epoch 67/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0710 - mse: 0.0085 - val_loss: 0.0079 - val_mae: 0.0686 - val_mse: 0.0079\n",
      "Epoch 67: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 990us/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.007223296597388476    std:  0.09017376524414841    MAE:  0.07033557404211412     R2:  0.9247892968314027\n",
      "Test. mean:  -0.007158623643837556    std:  0.08848672791573794    MAE:  0.06858844138738063     R2:  0.9249862731056077\n",
      "\u001b[1m\u001b[91mFold 3\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.3021 - mae: 0.4228 - mse: 0.3021 - val_loss: 0.0577 - val_mae: 0.2040 - val_mse: 0.0577\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1996 - mse: 0.0554 - val_loss: 0.0576 - val_mae: 0.2046 - val_mse: 0.0576\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1996 - mse: 0.0554 - val_loss: 0.0576 - val_mae: 0.2046 - val_mse: 0.0576\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1997 - mse: 0.0554 - val_loss: 0.0577 - val_mae: 0.2041 - val_mse: 0.0577\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1998 - mse: 0.0554 - val_loss: 0.0577 - val_mae: 0.2042 - val_mse: 0.0577\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1997 - mse: 0.0554 - val_loss: 0.0579 - val_mae: 0.2038 - val_mse: 0.0579\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1996 - mse: 0.0554 - val_loss: 0.0577 - val_mae: 0.2041 - val_mse: 0.0577\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1996 - mse: 0.0554 - val_loss: 0.0576 - val_mae: 0.2053 - val_mse: 0.0576\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.1998 - mse: 0.0555 - val_loss: 0.0576 - val_mae: 0.2052 - val_mse: 0.0576\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1999 - mse: 0.0554 - val_loss: 0.0578 - val_mae: 0.2039 - val_mse: 0.0578\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.1998 - mse: 0.0555 - val_loss: 0.0578 - val_mae: 0.2062 - val_mse: 0.0578\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1998 - mse: 0.0554 - val_loss: 0.0576 - val_mae: 0.2054 - val_mse: 0.0576\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1998 - mse: 0.0554 - val_loss: 0.0576 - val_mae: 0.2045 - val_mse: 0.0576\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1997 - mse: 0.0554 - val_loss: 0.0576 - val_mae: 0.2050 - val_mse: 0.0576\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.1998 - mse: 0.0555 - val_loss: 0.0582 - val_mae: 0.2077 - val_mse: 0.0582\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.1999 - mse: 0.0555 - val_loss: 0.0576 - val_mae: 0.2046 - val_mse: 0.0576\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.1998 - mse: 0.0555 - val_loss: 0.0577 - val_mae: 0.2056 - val_mse: 0.0577\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.2000 - mse: 0.0555 - val_loss: 0.0577 - val_mae: 0.2042 - val_mse: 0.0577\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.1999 - mse: 0.0555 - val_loss: 0.0576 - val_mae: 0.2049 - val_mse: 0.0576\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.1998 - mse: 0.0555 - val_loss: 0.0576 - val_mae: 0.2049 - val_mse: 0.0576\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.1998 - mse: 0.0555 - val_loss: 0.0577 - val_mae: 0.2042 - val_mse: 0.0577\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.1999 - mse: 0.0555 - val_loss: 0.0577 - val_mae: 0.2041 - val_mse: 0.0577\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1996 - mse: 0.0554 - val_loss: 0.0578 - val_mae: 0.2064 - val_mse: 0.0578\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.1999 - mse: 0.0555 - val_loss: 0.0580 - val_mae: 0.2070 - val_mse: 0.0580\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.1998 - mse: 0.0555 - val_loss: 0.0577 - val_mae: 0.2043 - val_mse: 0.0577\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.1997 - mse: 0.0555 - val_loss: 0.0577 - val_mae: 0.2055 - val_mse: 0.0577\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.1998 - mse: 0.0555 - val_loss: 0.0576 - val_mae: 0.2053 - val_mse: 0.0576\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1996 - mse: 0.0554 - val_loss: 0.0576 - val_mae: 0.2052 - val_mse: 0.0576\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1998 - mse: 0.0554 - val_loss: 0.0584 - val_mae: 0.2032 - val_mse: 0.0584\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0556 - mae: 0.1999 - mse: 0.0556 - val_loss: 0.0578 - val_mae: 0.2061 - val_mse: 0.0578\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1998 - mse: 0.0554 - val_loss: 0.0577 - val_mae: 0.2055 - val_mse: 0.0577\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.1998 - mse: 0.0555 - val_loss: 0.0580 - val_mae: 0.2068 - val_mse: 0.0580\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.2000 - mse: 0.0555 - val_loss: 0.0576 - val_mae: 0.2045 - val_mse: 0.0576\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.1998 - mse: 0.0555 - val_loss: 0.0581 - val_mae: 0.2035 - val_mse: 0.0581\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0557 - mae: 0.2001 - mse: 0.0557 - val_loss: 0.0576 - val_mae: 0.2053 - val_mse: 0.0576\n",
      "Epoch 35: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 739us/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  0.006144115758032746    std:  0.2351880555738691    MAE:  0.20032857004728613     R2:  nan\n",
      "Test. mean:  0.00476685233752914    std:  0.2400091645280366    MAE:  0.2052509230604421     R2:  nan\n",
      "\u001b[1m\u001b[91mFold 4\u001b[0m\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romuald/anaconda3/envs/ML/lib/python3.10/site-packages/numpy/lib/function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/romuald/anaconda3/envs/ML/lib/python3.10/site-packages/numpy/lib/function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 1s 2ms/step - loss: 0.1092 - mae: 0.2012 - mse: 0.1092 - val_loss: 0.0339 - val_mae: 0.1354 - val_mse: 0.0339\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0291 - mae: 0.1317 - mse: 0.0291 - val_loss: 0.0267 - val_mae: 0.1265 - val_mse: 0.0267\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0244 - mae: 0.1244 - mse: 0.0244 - val_loss: 0.0221 - val_mae: 0.1210 - val_mse: 0.0221\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0223 - mae: 0.1213 - mse: 0.0223 - val_loss: 0.0215 - val_mae: 0.1192 - val_mse: 0.0215\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0219 - mae: 0.1203 - mse: 0.0219 - val_loss: 0.0230 - val_mae: 0.1233 - val_mse: 0.0230\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0215 - mae: 0.1189 - mse: 0.0215 - val_loss: 0.0211 - val_mae: 0.1173 - val_mse: 0.0211\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0212 - mae: 0.1181 - mse: 0.0212 - val_loss: 0.0208 - val_mae: 0.1173 - val_mse: 0.0208\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0209 - mae: 0.1168 - mse: 0.0209 - val_loss: 0.0205 - val_mae: 0.1162 - val_mse: 0.0205\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0203 - mae: 0.1152 - mse: 0.0203 - val_loss: 0.0200 - val_mae: 0.1149 - val_mse: 0.0200\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0196 - mae: 0.1132 - mse: 0.0196 - val_loss: 0.0187 - val_mae: 0.1108 - val_mse: 0.0187\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0185 - mae: 0.1090 - mse: 0.0185 - val_loss: 0.0163 - val_mae: 0.1029 - val_mse: 0.0163\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0146 - mae: 0.0951 - mse: 0.0146 - val_loss: 0.0119 - val_mae: 0.0835 - val_mse: 0.0119\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0117 - mae: 0.0846 - mse: 0.0117 - val_loss: 0.0105 - val_mae: 0.0794 - val_mse: 0.0105\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0103 - mae: 0.0785 - mse: 0.0103 - val_loss: 0.0094 - val_mae: 0.0739 - val_mse: 0.0094\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0095 - mae: 0.0750 - mse: 0.0095 - val_loss: 0.0101 - val_mae: 0.0794 - val_mse: 0.0101\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0728 - mse: 0.0090 - val_loss: 0.0091 - val_mae: 0.0741 - val_mse: 0.0091\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0722 - mse: 0.0088 - val_loss: 0.0084 - val_mae: 0.0698 - val_mse: 0.0084\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0702 - mse: 0.0085 - val_loss: 0.0088 - val_mae: 0.0726 - val_mse: 0.0088\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0081 - mae: 0.0686 - mse: 0.0081 - val_loss: 0.0085 - val_mae: 0.0687 - val_mse: 0.0085\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0081 - mae: 0.0686 - mse: 0.0081 - val_loss: 0.0080 - val_mae: 0.0689 - val_mse: 0.0080\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0079 - mae: 0.0675 - mse: 0.0079 - val_loss: 0.0092 - val_mae: 0.0728 - val_mse: 0.0092\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0076 - mae: 0.0661 - mse: 0.0076 - val_loss: 0.0078 - val_mae: 0.0678 - val_mse: 0.0078\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0076 - mae: 0.0662 - mse: 0.0076 - val_loss: 0.0076 - val_mae: 0.0662 - val_mse: 0.0076\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0079 - mae: 0.0681 - mse: 0.0079 - val_loss: 0.0075 - val_mae: 0.0661 - val_mse: 0.0075\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0073 - mae: 0.0643 - mse: 0.0073 - val_loss: 0.0074 - val_mae: 0.0632 - val_mse: 0.0074\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0073 - mae: 0.0646 - mse: 0.0073 - val_loss: 0.0075 - val_mae: 0.0637 - val_mse: 0.0075\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0074 - mae: 0.0649 - mse: 0.0074 - val_loss: 0.0102 - val_mae: 0.0794 - val_mse: 0.0102\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0075 - mae: 0.0657 - mse: 0.0075 - val_loss: 0.0076 - val_mae: 0.0661 - val_mse: 0.0076\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0636 - mse: 0.0071 - val_loss: 0.0099 - val_mae: 0.0760 - val_mse: 0.0099\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0640 - mse: 0.0072 - val_loss: 0.0078 - val_mae: 0.0652 - val_mse: 0.0078\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0640 - mse: 0.0072 - val_loss: 0.0072 - val_mae: 0.0619 - val_mse: 0.0072\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0625 - mse: 0.0069 - val_loss: 0.0069 - val_mae: 0.0610 - val_mse: 0.0069\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0635 - mse: 0.0071 - val_loss: 0.0068 - val_mae: 0.0606 - val_mse: 0.0068\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0619 - mse: 0.0068 - val_loss: 0.0085 - val_mae: 0.0738 - val_mse: 0.0085\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0623 - mse: 0.0069 - val_loss: 0.0079 - val_mae: 0.0668 - val_mse: 0.0079\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0607 - mse: 0.0066 - val_loss: 0.0066 - val_mae: 0.0612 - val_mse: 0.0066\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0632 - mse: 0.0069 - val_loss: 0.0065 - val_mae: 0.0595 - val_mse: 0.0065\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0619 - mse: 0.0067 - val_loss: 0.0064 - val_mae: 0.0586 - val_mse: 0.0064\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0617 - mse: 0.0066 - val_loss: 0.0063 - val_mae: 0.0581 - val_mse: 0.0063\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0070 - mae: 0.0638 - mse: 0.0070 - val_loss: 0.0071 - val_mae: 0.0625 - val_mse: 0.0071\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0614 - mse: 0.0066 - val_loss: 0.0064 - val_mae: 0.0592 - val_mse: 0.0064\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0615 - mse: 0.0067 - val_loss: 0.0063 - val_mae: 0.0596 - val_mse: 0.0063\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0604 - mse: 0.0065 - val_loss: 0.0063 - val_mae: 0.0583 - val_mse: 0.0063\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0631 - mse: 0.0069 - val_loss: 0.0072 - val_mae: 0.0641 - val_mse: 0.0072\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0608 - mse: 0.0065 - val_loss: 0.0066 - val_mae: 0.0598 - val_mse: 0.0066\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0609 - mse: 0.0065 - val_loss: 0.0062 - val_mae: 0.0577 - val_mse: 0.0062\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0605 - mse: 0.0065 - val_loss: 0.0082 - val_mae: 0.0730 - val_mse: 0.0082\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0604 - mse: 0.0064 - val_loss: 0.0060 - val_mae: 0.0571 - val_mse: 0.0060\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0604 - mse: 0.0064 - val_loss: 0.0060 - val_mae: 0.0570 - val_mse: 0.0060\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0609 - mse: 0.0066 - val_loss: 0.0063 - val_mae: 0.0584 - val_mse: 0.0063\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0603 - mse: 0.0064 - val_loss: 0.0062 - val_mae: 0.0584 - val_mse: 0.0062\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0604 - mse: 0.0065 - val_loss: 0.0061 - val_mae: 0.0589 - val_mse: 0.0061\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0609 - mse: 0.0065 - val_loss: 0.0064 - val_mae: 0.0594 - val_mse: 0.0064\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0595 - mse: 0.0063 - val_loss: 0.0066 - val_mae: 0.0605 - val_mse: 0.0066\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0614 - mse: 0.0066 - val_loss: 0.0069 - val_mae: 0.0624 - val_mse: 0.0069\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0598 - mse: 0.0063 - val_loss: 0.0063 - val_mae: 0.0598 - val_mse: 0.0063\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0598 - mse: 0.0063 - val_loss: 0.0071 - val_mae: 0.0636 - val_mse: 0.0071\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0629 - mse: 0.0069 - val_loss: 0.0080 - val_mae: 0.0720 - val_mse: 0.0080\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0598 - mse: 0.0063 - val_loss: 0.0064 - val_mae: 0.0619 - val_mse: 0.0064\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0597 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0589 - val_mse: 0.0061\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0591 - mse: 0.0062 - val_loss: 0.0063 - val_mae: 0.0581 - val_mse: 0.0063\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0598 - mse: 0.0063 - val_loss: 0.0072 - val_mae: 0.0643 - val_mse: 0.0072\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0603 - mse: 0.0064 - val_loss: 0.0064 - val_mae: 0.0594 - val_mse: 0.0064\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0603 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0592 - val_mse: 0.0061\n",
      "Epoch 64: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 1ms/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.01411306735043534    std:  0.07581712988890939    MAE:  0.058773519636903165     R2:  0.9473434256804404\n",
      "Test. mean:  -0.012098327895685749    std:  0.0768606962245906    MAE:  0.059183180146456155     R2:  0.9444731972012246\n",
      "\n",
      "Duration :  00:04:41 382ms\n",
      "\u001b[1maverage MAE of the training set:\u001b[0m   0.08 +/- 0.06\n",
      "\u001b[1maverage MAE of the validation set:\u001b[0m 0.08 +/- 0.06\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#######################################################################################\n",
    "# optimization of the ANN\n",
    "# library used: keras and scikit learn for the KFold cross-validator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "VERBOSE = 1\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 25\n",
    "N_SPLIT = 5\n",
    "\n",
    "vID.chrono_start()\n",
    "\n",
    "# variables created to save at each iteration of the KFold process: the man error, the standard deviation, MAE, R2\n",
    "meantT=list()\n",
    "stdtT=list()\n",
    "MAEtT=list()\n",
    "R2tT=list()\n",
    "meanvT=list()\n",
    "stdvT=list()\n",
    "MAEvT=list()\n",
    "R2vT=list()\n",
    "\n",
    "kfold = KFold(n_splits=N_SPLIT,shuffle=True,random_state=42) # k-fold is here!\n",
    "#print(list(kfold.split(x_train,y_train)))\n",
    "\n",
    "j = 0 # Variable for keeping count of split we are executing\n",
    "# The KFold cv provides train/test indices to split data in train/test sets\n",
    "for train_idx, val_idx in list(kfold.split(xdata,ydata)):\n",
    "\n",
    "    x_train_cv = xdata.iloc[train_idx]\n",
    "    x_valid_cv = xdata.iloc[val_idx]\n",
    "    y_train_cv = ydata.iloc[train_idx]\n",
    "    y_valid_cv = ydata.iloc[val_idx]\n",
    "#    display(x_train_cv,x_valid_cv)\n",
    "# This part has been commented with respect to the original script\n",
    "    # scaler = preprocessing.StandardScaler()\n",
    "    # scaler.fit(x_train_cv.values)\n",
    "    # xt_scaled = scaler.transform(x_train_cv.values) #returns a numpy array\n",
    "    # xv_scaled = scaler.transform(x_valid_cv.values) #returns a numpy array\n",
    "    # x_train_cv = pd.DataFrame(xt_scaled, index=x_train_cv.index, columns=x_train_cv.columns)\n",
    "    # x_valid_cv = pd.DataFrame(xv_scaled, index=x_valid_cv.index, columns=x_valid_cv.columns)\n",
    "    # del xt_scaled, xv_scaled\n",
    "##############\n",
    "#    display(x_train_cv.describe().style.format(\"{0:.2f}\").set_caption(\"Training set after normalization (with scikit-learn):\"))\n",
    "#    display(x_valid_cv.describe().style.format(\"{0:.2f}\").set_caption(\"Validation set after normalization (with scikit-learn):\"))\n",
    "    print(f\"{color.BOLD}{color.RED}Fold {j}{color.OFF}\")\n",
    "    j+=1\n",
    "    ANNmodel=defANN( (53,), acthL )\n",
    "    ANNhistory = ANNmodel.fit(x_train_cv,\n",
    "                        y_train_cv,\n",
    "                        epochs          = EPOCHS,\n",
    "                        batch_size      = BATCH_SIZE,\n",
    "                        verbose         = VERBOSE,\n",
    "                        validation_data = (x_valid_cv, y_valid_cv),\n",
    "                        callbacks=[es])\n",
    "    ytrain_hat=ANNmodel.predict(x_train_cv)\n",
    "    yvalid_hat=ANNmodel.predict(x_valid_cv)\n",
    "    diffyt = ytrain_hat.ravel() - y_train_cv.ravel()\n",
    "    diffyv = yvalid_hat.ravel() - y_valid_cv.ravel()\n",
    "\n",
    "    print()\n",
    "    print(\"xCO2(predicted) - xCO2(actual)\")\n",
    "    print(\n",
    "          \"Train.\",\"mean: \", np.mean(diffyt),\n",
    "          \"   std: \", np.std(diffyt),\n",
    "          \"   MAE: \", np.average(abs(diffyt)),\n",
    "          \"    R2: \", np.corrcoef(y_train_cv.ravel(),ytrain_hat.ravel())[0,1]\n",
    "         )\n",
    "    print(\n",
    "          \"Test.\",\"mean: \", np.mean(diffyv),\n",
    "          \"   std: \", np.std(diffyv),\n",
    "          \"   MAE: \", np.average(abs(diffyv)),\n",
    "          \"    R2: \", np.corrcoef(y_valid_cv.ravel(),yvalid_hat.ravel())[0,1]\n",
    "         )\n",
    "    meantT.append(np.mean(diffyt))\n",
    "    meanvT.append(np.mean(diffyv))\n",
    "    stdtT.append(np.std(diffyt))\n",
    "    stdvT.append(np.std(diffyv))\n",
    "    MAEtT.append(np.average(abs(diffyt)))\n",
    "    MAEvT.append(np.average(abs(diffyv)))\n",
    "    R2tT.append(np.corrcoef(y_train_cv.ravel(),ytrain_hat.ravel())[0,1])\n",
    "    R2vT.append(np.corrcoef(y_valid_cv.ravel(),yvalid_hat.ravel())[0,1])\n",
    "    \n",
    "vID.chrono_show()\n",
    "\n",
    "#######################################################################################\n",
    "# accuracy of the ANN?\n",
    "# library used: numpy\n",
    "print(f\"{color.BOLD}average MAE of the training set:{color.OFF}   {np.mean(MAEtT):.2f} +/- {np.std(MAEtT):.2f}\")\n",
    "print(f\"{color.BOLD}average MAE of the validation set:{color.OFF} {np.mean(MAEvT):.2f} +/- {np.std(MAEvT):.2f}\")\n",
    "\n",
    "figCV, axCV = plt.subplots(1, 1)\n",
    "figCV.set_size_inches(12,5)\n",
    "axCV.errorbar(x=np.arange(len(meantT)), y=meantT, yerr=MAEtT, label='training sets', fmt='o-', capsize=10)\n",
    "axCV.errorbar(x=np.arange(len(meanvT))+0.1, y=meanvT, yerr=MAEvT, label='validation sets', fmt='o-', capsize=10)\n",
    "axCV.legend(loc='lower left', shadow=True, fontsize='14')\n",
    "axCV.set_xlabel('cross-validation k-values ',fontdict={'fontsize':16})\n",
    "axCV.set_ylabel('$\\hat{y}-y_{\\mathrm{actual}}$',fontdict={'fontsize':16})\n",
    "axCV.tick_params(labelsize = 14)\n",
    "plt.savefig('../DS4B-CO2-images/KFold-cv-AppliedToSong_etal.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d639724-2e98-43dd-b6e5-7f0767cfb27d",
   "metadata": {},
   "source": [
    "<div class=\"rq\">\n",
    "    \n",
    "You have probably found results similar to those reported in the following error plot:\n",
    "<p style=\"text-align: center\"><img width=\"650px\" src=\"../DS4B-CO2-images/KFold-cv-AppliedToSong_etalK-saved.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_ResultsSong\"></p>\n",
    "    <b>This error plot shows a bad performance of the original ML algorithm of Song <i>et al</i>. (<i>i.e.</i> without standardization of the data), with a strong variation of error bars.</b>\n",
    "    \n",
    "Either the authors did actually apply a standardization preprocessing and they forgot to mention it in the article, or they ran several optimization algorithms of the ANN until they found a seemingly performant one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1998a632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**End at:** Thursday 18 May 2023, 13:39:37  \n",
       "**Duration:** 00:05:02 963ms"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"../config/svg/logoEnd.svg\" style=\"margin-left:auto; margin-right:auto\"/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vID.end(cwd0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff54979-735d-4f02-8b52-307d46455e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
