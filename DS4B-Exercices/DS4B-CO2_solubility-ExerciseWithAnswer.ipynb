{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10024ca9-7e94-4f5c-a982-b4537b919d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: bold;\n",
       "}\n",
       "body, exercice {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: 200;\n",
       "}\n",
       "h1 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 30px ;\n",
       "  color: white;\n",
       "  background: #b11d01;\n",
       "  text-align: center;\n",
       "}\n",
       "h2 {\n",
       "  border: 3px solid #333;\n",
       "  padding: 18px ;\n",
       "  color: #b11d01;\n",
       "  background: #ffffff;\n",
       "  text-align: center;\n",
       "}\n",
       "h3 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 12px ;\n",
       "  color: #000000;\n",
       "  background: #c1c1c1;\n",
       "  text-align: left;\n",
       "}\n",
       "h4 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #d9fffc;\n",
       "  text-align: left;\n",
       "}\n",
       "h5 {\n",
       "  border: 1px solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #ffffff;\n",
       "  text-align: left;\n",
       "}\n",
       ".rq {    \n",
       "    background-color: #fcf2f2;\n",
       "    border-color: #dFb5b4;\n",
       "    border-left: 5px solid #dfb5b4;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       ".intro {    \n",
       "    background-color: #f1f1f1;\n",
       "    border-color: #969696;\n",
       "    border-left: 5px solid #969696;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       ".exold {    \n",
       "    background-color: #b2dbea80;\n",
       "    border-color: #0055ff;\n",
       "    border-left: 10px solid #0055ff;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       ".ex {    \n",
       "    background-color: #b2dbea80;\n",
       "    padding: 0.5em;\n",
       "    padding-top: 0em;\n",
       "    font-weight: 200;\n",
       "    position:relative;\n",
       "    }\n",
       ".ex::before {\n",
       "    background-color: #b2dbea;\n",
       "    content:\"Exercice\";\n",
       "    margin-left:-0.5em;\n",
       "    margin-right:-0.5em;\n",
       "    padding-left:0.5em;\n",
       "    padding-right:0.5em;\n",
       "    font-weight: 600;\n",
       "    display: block;\n",
       "    }\n",
       ".app {    \n",
       "    background-color: #b2dbea80;\n",
       "    padding: 0.5em;\n",
       "    padding-top: 0em;\n",
       "    font-weight: 200;\n",
       "    position:relative;\n",
       "    }\n",
       ".app::before {\n",
       "    background-color: #b2dbea;\n",
       "    content:\"Application\";\n",
       "    margin-left:-0.5em;\n",
       "    margin-right:-0.5em;\n",
       "    padding-left:0.5em;\n",
       "    padding-right:0.5em;\n",
       "    font-weight: 600;\n",
       "    display: block;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Start at:** Thursday 12 January 2023, 22:19:27  \n",
       "**Hostname:** desktop-5h3j30j.home (Linux)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"../config/svg/logoPytChem.svg\" style=\"margin-left:auto; margin-right:auto\"/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "cwd0 = '../config/'\n",
    "sys.path.append(cwd0)\n",
    "\n",
    "import visualID_Eng as vID\n",
    "from visualID_Eng import color\n",
    "vID.init(cwd0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e4b9b",
   "metadata": {},
   "source": [
    "# Prediction by an artificial neural network of the solubility of CO<sub>2</sub> in ionic liquids\n",
    "\n",
    "<div class=\"rq\">\n",
    "    \n",
    "<b>Reference</b>: \n",
    "Z. Song, H. Shi, X. Zhang & T. Zhou (**2020**), Prediction of CO<sub>2</sub> solubility in ionic liquids using machine learning methods, [<i>Chem. Eng. Sci.</i> <b>223</b>: 115752](https://www.doi.org/10.1016/j.ces.2020.115752) \n",
    "<br>\n",
    "<p style=\"text-align: center\"><img width=\"650px\" src=\"../DS4B-CO2-images/AbstractANNCO2-SongEtal.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_AbstractSong\"></p>\n",
    "<br>\n",
    "The main results are graphically reported below.\n",
    "<br>\n",
    "<p style=\"text-align: center\"><img width=\"900px\" src=\"../DS4B-CO2-images/ANNCO2-SongEtal-Results.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_ResultsSong\"></p>\n",
    "<br>\n",
    "Yet, it seems sthat no standardization process of the data has been applied. \n",
    "    \n",
    "<span style=\"color:red\">Moreover, a spurious separation of the data between training and test sets has been applied: \"<i>Instead of performing random selection, we employ a hybrid artificial-random strategy to decompose the dataset. Specifically, the data points consisting of the least frequently used groups are equally divided into five folders\"</i></span> \n",
    "<br><br>\n",
    "<b>It raises doubts about the stability of the algorithm developped in this paper (*unless the authors forgot to mention that data were standardized*).</b>\n",
    "<br>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84860c4-3fcf-45f5-80a3-607ead68da80",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "<span style=\"font-weight:bold\">The goal of this exercise is to apply the <i>K</i>-fold cross-validation the ANN part of this article, <i>i.e.</i> without standardized data. </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af39c092-f4b7-460b-b698-32e0f2c55461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 22:19:29.310607: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-12 22:19:29.363579: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os,sys\n",
    "from IPython.display import display\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   OFF = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89a89d-fb12-4ba1-8382-1609a0c4f848",
   "metadata": {},
   "source": [
    "<a id=\"data-read\"></a>\n",
    "## **1.** Database reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7d2251b-1f22-42f5-b930-e6032fe55170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IL</th>\n",
       "      <th>cation</th>\n",
       "      <th>anion</th>\n",
       "      <th>x_CO2</th>\n",
       "      <th>T (K)</th>\n",
       "      <th>P (bar)</th>\n",
       "      <th>[CH3]</th>\n",
       "      <th>[CH2]</th>\n",
       "      <th>[CH]</th>\n",
       "      <th>[OCH2]</th>\n",
       "      <th>...</th>\n",
       "      <th>[MeSO3]</th>\n",
       "      <th>[TfO]</th>\n",
       "      <th>[NfO]</th>\n",
       "      <th>[TDfO]</th>\n",
       "      <th>[TOS]</th>\n",
       "      <th>[C12PhSO3]</th>\n",
       "      <th>[DMPO4]</th>\n",
       "      <th>[DEPO4]</th>\n",
       "      <th>[DBPO4]</th>\n",
       "      <th>[methide]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>363.15</td>\n",
       "      <td>246.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.500</td>\n",
       "      <td>383.15</td>\n",
       "      <td>235.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>353.15</td>\n",
       "      <td>223.30</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.500</td>\n",
       "      <td>373.15</td>\n",
       "      <td>198.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>343.15</td>\n",
       "      <td>188.50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10111</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.592</td>\n",
       "      <td>298.15</td>\n",
       "      <td>35.86</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10112</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.239</td>\n",
       "      <td>343.15</td>\n",
       "      <td>27.54</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10113</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.396</td>\n",
       "      <td>298.15</td>\n",
       "      <td>20.15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10114</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.140</td>\n",
       "      <td>343.15</td>\n",
       "      <td>17.93</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10115</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.139</td>\n",
       "      <td>323.15</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10116 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 IL  cation   anion  x_CO2   T (K)  P (bar)  [CH3]  [CH2]  \\\n",
       "0       [BMIM][BF4]  [BMIM]   [BF4]  0.610  363.15   246.00      1      3   \n",
       "1       [BMIM][BF4]  [BMIM]   [BF4]  0.500  383.15   235.00      1      3   \n",
       "2       [BMIM][BF4]  [BMIM]   [BF4]  0.610  353.15   223.30      1      3   \n",
       "3       [BMIM][BF4]  [BMIM]   [BF4]  0.500  373.15   198.00      1      3   \n",
       "4       [BMIM][BF4]  [BMIM]   [BF4]  0.610  343.15   188.50      1      3   \n",
       "...             ...     ...     ...    ...     ...      ...    ...    ...   \n",
       "10111  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.592  298.15    35.86      1      5   \n",
       "10112  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.239  343.15    27.54      1      5   \n",
       "10113  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.396  298.15    20.15      1      5   \n",
       "10114  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.140  343.15    17.93      1      5   \n",
       "10115  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.139  323.15     8.00      1      5   \n",
       "\n",
       "       [CH]  [OCH2]  ...  [MeSO3]  [TfO]  [NfO]  [TDfO]  [TOS]  [C12PhSO3]  \\\n",
       "0         0       0  ...        0      0      0       0      0           0   \n",
       "1         0       0  ...        0      0      0       0      0           0   \n",
       "2         0       0  ...        0      0      0       0      0           0   \n",
       "3         0       0  ...        0      0      0       0      0           0   \n",
       "4         0       0  ...        0      0      0       0      0           0   \n",
       "...     ...     ...  ...      ...    ...    ...     ...    ...         ...   \n",
       "10111     0       0  ...        0      0      0       0      0           0   \n",
       "10112     0       0  ...        0      0      0       0      0           0   \n",
       "10113     0       0  ...        0      0      0       0      0           0   \n",
       "10114     0       0  ...        0      0      0       0      0           0   \n",
       "10115     0       0  ...        0      0      0       0      0           0   \n",
       "\n",
       "       [DMPO4]  [DEPO4]  [DBPO4]  [methide]  \n",
       "0            0        0        0          0  \n",
       "1            0        0        0          0  \n",
       "2            0        0        0          0  \n",
       "3            0        0        0          0  \n",
       "4            0        0        0          0  \n",
       "...        ...      ...      ...        ...  \n",
       "10111        0        0        0          0  \n",
       "10112        0        0        0          0  \n",
       "10113        0        0        0          0  \n",
       "10114        0        0        0          0  \n",
       "10115        0        0        0          0  \n",
       "\n",
       "[10116 rows x 57 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_cab75\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cab75_level0_col0\" class=\"col_heading level0 col0\" >x_CO2</th>\n",
       "      <th id=\"T_cab75_level0_col1\" class=\"col_heading level0 col1\" >T (K)</th>\n",
       "      <th id=\"T_cab75_level0_col2\" class=\"col_heading level0 col2\" >P (bar)</th>\n",
       "      <th id=\"T_cab75_level0_col3\" class=\"col_heading level0 col3\" >[CH3]</th>\n",
       "      <th id=\"T_cab75_level0_col4\" class=\"col_heading level0 col4\" >[CH2]</th>\n",
       "      <th id=\"T_cab75_level0_col5\" class=\"col_heading level0 col5\" >[CH]</th>\n",
       "      <th id=\"T_cab75_level0_col6\" class=\"col_heading level0 col6\" >[OCH2]</th>\n",
       "      <th id=\"T_cab75_level0_col7\" class=\"col_heading level0 col7\" >[OCH3]</th>\n",
       "      <th id=\"T_cab75_level0_col8\" class=\"col_heading level0 col8\" >[CF2]</th>\n",
       "      <th id=\"T_cab75_level0_col9\" class=\"col_heading level0 col9\" >[CF3]</th>\n",
       "      <th id=\"T_cab75_level0_col10\" class=\"col_heading level0 col10\" >[OH]</th>\n",
       "      <th id=\"T_cab75_level0_col11\" class=\"col_heading level0 col11\" >CH=CH</th>\n",
       "      <th id=\"T_cab75_level0_col12\" class=\"col_heading level0 col12\" >CH=CH2</th>\n",
       "      <th id=\"T_cab75_level0_col13\" class=\"col_heading level0 col13\" >[Im13]</th>\n",
       "      <th id=\"T_cab75_level0_col14\" class=\"col_heading level0 col14\" >[MIm]</th>\n",
       "      <th id=\"T_cab75_level0_col15\" class=\"col_heading level0 col15\" >[MMIM]</th>\n",
       "      <th id=\"T_cab75_level0_col16\" class=\"col_heading level0 col16\" >[Py]</th>\n",
       "      <th id=\"T_cab75_level0_col17\" class=\"col_heading level0 col17\" >[MPy]</th>\n",
       "      <th id=\"T_cab75_level0_col18\" class=\"col_heading level0 col18\" >[MPyrro]</th>\n",
       "      <th id=\"T_cab75_level0_col19\" class=\"col_heading level0 col19\" >[MPip]</th>\n",
       "      <th id=\"T_cab75_level0_col20\" class=\"col_heading level0 col20\" >[NH3]</th>\n",
       "      <th id=\"T_cab75_level0_col21\" class=\"col_heading level0 col21\" >[NH2]</th>\n",
       "      <th id=\"T_cab75_level0_col22\" class=\"col_heading level0 col22\" >[NH]</th>\n",
       "      <th id=\"T_cab75_level0_col23\" class=\"col_heading level0 col23\" >[N]</th>\n",
       "      <th id=\"T_cab75_level0_col24\" class=\"col_heading level0 col24\" >[P]</th>\n",
       "      <th id=\"T_cab75_level0_col25\" class=\"col_heading level0 col25\" >[S]</th>\n",
       "      <th id=\"T_cab75_level0_col26\" class=\"col_heading level0 col26\" >[BF4]</th>\n",
       "      <th id=\"T_cab75_level0_col27\" class=\"col_heading level0 col27\" >[Cl]</th>\n",
       "      <th id=\"T_cab75_level0_col28\" class=\"col_heading level0 col28\" >[DCA]</th>\n",
       "      <th id=\"T_cab75_level0_col29\" class=\"col_heading level0 col29\" >[NO3]</th>\n",
       "      <th id=\"T_cab75_level0_col30\" class=\"col_heading level0 col30\" >[PF6]</th>\n",
       "      <th id=\"T_cab75_level0_col31\" class=\"col_heading level0 col31\" >[SCN]</th>\n",
       "      <th id=\"T_cab75_level0_col32\" class=\"col_heading level0 col32\" >[TCB]</th>\n",
       "      <th id=\"T_cab75_level0_col33\" class=\"col_heading level0 col33\" >[C(CN)3]</th>\n",
       "      <th id=\"T_cab75_level0_col34\" class=\"col_heading level0 col34\" >[HSO4]</th>\n",
       "      <th id=\"T_cab75_level0_col35\" class=\"col_heading level0 col35\" >[FSA]</th>\n",
       "      <th id=\"T_cab75_level0_col36\" class=\"col_heading level0 col36\" >[Tf2N]</th>\n",
       "      <th id=\"T_cab75_level0_col37\" class=\"col_heading level0 col37\" >[BETA]</th>\n",
       "      <th id=\"T_cab75_level0_col38\" class=\"col_heading level0 col38\" >[FOR]</th>\n",
       "      <th id=\"T_cab75_level0_col39\" class=\"col_heading level0 col39\" >[TFA]</th>\n",
       "      <th id=\"T_cab75_level0_col40\" class=\"col_heading level0 col40\" >[C3F7CO2]</th>\n",
       "      <th id=\"T_cab75_level0_col41\" class=\"col_heading level0 col41\" >[MeSO4]</th>\n",
       "      <th id=\"T_cab75_level0_col42\" class=\"col_heading level0 col42\" >[EtSO4]</th>\n",
       "      <th id=\"T_cab75_level0_col43\" class=\"col_heading level0 col43\" >[MDEGSO4]</th>\n",
       "      <th id=\"T_cab75_level0_col44\" class=\"col_heading level0 col44\" >[MeSO3]</th>\n",
       "      <th id=\"T_cab75_level0_col45\" class=\"col_heading level0 col45\" >[TfO]</th>\n",
       "      <th id=\"T_cab75_level0_col46\" class=\"col_heading level0 col46\" >[NfO]</th>\n",
       "      <th id=\"T_cab75_level0_col47\" class=\"col_heading level0 col47\" >[TDfO]</th>\n",
       "      <th id=\"T_cab75_level0_col48\" class=\"col_heading level0 col48\" >[TOS]</th>\n",
       "      <th id=\"T_cab75_level0_col49\" class=\"col_heading level0 col49\" >[C12PhSO3]</th>\n",
       "      <th id=\"T_cab75_level0_col50\" class=\"col_heading level0 col50\" >[DMPO4]</th>\n",
       "      <th id=\"T_cab75_level0_col51\" class=\"col_heading level0 col51\" >[DEPO4]</th>\n",
       "      <th id=\"T_cab75_level0_col52\" class=\"col_heading level0 col52\" >[DBPO4]</th>\n",
       "      <th id=\"T_cab75_level0_col53\" class=\"col_heading level0 col53\" >[methide]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cab75_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_cab75_row0_col0\" class=\"data row0 col0\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col1\" class=\"data row0 col1\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col2\" class=\"data row0 col2\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col3\" class=\"data row0 col3\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col4\" class=\"data row0 col4\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col5\" class=\"data row0 col5\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col6\" class=\"data row0 col6\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col7\" class=\"data row0 col7\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col8\" class=\"data row0 col8\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col9\" class=\"data row0 col9\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col10\" class=\"data row0 col10\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col11\" class=\"data row0 col11\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col12\" class=\"data row0 col12\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col13\" class=\"data row0 col13\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col14\" class=\"data row0 col14\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col15\" class=\"data row0 col15\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col16\" class=\"data row0 col16\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col17\" class=\"data row0 col17\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col18\" class=\"data row0 col18\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col19\" class=\"data row0 col19\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col20\" class=\"data row0 col20\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col21\" class=\"data row0 col21\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col22\" class=\"data row0 col22\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col23\" class=\"data row0 col23\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col24\" class=\"data row0 col24\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col25\" class=\"data row0 col25\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col26\" class=\"data row0 col26\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col27\" class=\"data row0 col27\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col28\" class=\"data row0 col28\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col29\" class=\"data row0 col29\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col30\" class=\"data row0 col30\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col31\" class=\"data row0 col31\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col32\" class=\"data row0 col32\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col33\" class=\"data row0 col33\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col34\" class=\"data row0 col34\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col35\" class=\"data row0 col35\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col36\" class=\"data row0 col36\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col37\" class=\"data row0 col37\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col38\" class=\"data row0 col38\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col39\" class=\"data row0 col39\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col40\" class=\"data row0 col40\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col41\" class=\"data row0 col41\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col42\" class=\"data row0 col42\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col43\" class=\"data row0 col43\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col44\" class=\"data row0 col44\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col45\" class=\"data row0 col45\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col46\" class=\"data row0 col46\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col47\" class=\"data row0 col47\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col48\" class=\"data row0 col48\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col49\" class=\"data row0 col49\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col50\" class=\"data row0 col50\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col51\" class=\"data row0 col51\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col52\" class=\"data row0 col52\" >10116.00</td>\n",
       "      <td id=\"T_cab75_row0_col53\" class=\"data row0 col53\" >10116.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cab75_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_cab75_row1_col0\" class=\"data row1 col0\" >0.33</td>\n",
       "      <td id=\"T_cab75_row1_col1\" class=\"data row1 col1\" >325.27</td>\n",
       "      <td id=\"T_cab75_row1_col2\" class=\"data row1 col2\" >54.21</td>\n",
       "      <td id=\"T_cab75_row1_col3\" class=\"data row1 col3\" >1.18</td>\n",
       "      <td id=\"T_cab75_row1_col4\" class=\"data row1 col4\" >4.72</td>\n",
       "      <td id=\"T_cab75_row1_col5\" class=\"data row1 col5\" >0.02</td>\n",
       "      <td id=\"T_cab75_row1_col6\" class=\"data row1 col6\" >0.02</td>\n",
       "      <td id=\"T_cab75_row1_col7\" class=\"data row1 col7\" >0.04</td>\n",
       "      <td id=\"T_cab75_row1_col8\" class=\"data row1 col8\" >0.04</td>\n",
       "      <td id=\"T_cab75_row1_col9\" class=\"data row1 col9\" >0.01</td>\n",
       "      <td id=\"T_cab75_row1_col10\" class=\"data row1 col10\" >0.06</td>\n",
       "      <td id=\"T_cab75_row1_col11\" class=\"data row1 col11\" >0.00</td>\n",
       "      <td id=\"T_cab75_row1_col12\" class=\"data row1 col12\" >0.00</td>\n",
       "      <td id=\"T_cab75_row1_col13\" class=\"data row1 col13\" >0.01</td>\n",
       "      <td id=\"T_cab75_row1_col14\" class=\"data row1 col14\" >0.77</td>\n",
       "      <td id=\"T_cab75_row1_col15\" class=\"data row1 col15\" >0.01</td>\n",
       "      <td id=\"T_cab75_row1_col16\" class=\"data row1 col16\" >0.00</td>\n",
       "      <td id=\"T_cab75_row1_col17\" class=\"data row1 col17\" >0.01</td>\n",
       "      <td id=\"T_cab75_row1_col18\" class=\"data row1 col18\" >0.09</td>\n",
       "      <td id=\"T_cab75_row1_col19\" class=\"data row1 col19\" >0.00</td>\n",
       "      <td id=\"T_cab75_row1_col20\" class=\"data row1 col20\" >0.00</td>\n",
       "      <td id=\"T_cab75_row1_col21\" class=\"data row1 col21\" >0.01</td>\n",
       "      <td id=\"T_cab75_row1_col22\" class=\"data row1 col22\" >0.00</td>\n",
       "      <td id=\"T_cab75_row1_col23\" class=\"data row1 col23\" >0.02</td>\n",
       "      <td id=\"T_cab75_row1_col24\" class=\"data row1 col24\" >0.05</td>\n",
       "      <td id=\"T_cab75_row1_col25\" class=\"data row1 col25\" >0.00</td>\n",
       "      <td id=\"T_cab75_row1_col26\" class=\"data row1 col26\" >0.11</td>\n",
       "      <td id=\"T_cab75_row1_col27\" class=\"data row1 col27\" >0.02</td>\n",
       "      <td id=\"T_cab75_row1_col28\" class=\"data row1 col28\" >0.02</td>\n",
       "      <td id=\"T_cab75_row1_col29\" class=\"data row1 col29\" >0.02</td>\n",
       "      <td id=\"T_cab75_row1_col30\" class=\"data row1 col30\" >0.11</td>\n",
       "      <td id=\"T_cab75_row1_col31\" class=\"data row1 col31\" >0.02</td>\n",
       "      <td id=\"T_cab75_row1_col32\" class=\"data row1 col32\" >0.01</td>\n",
       "      <td id=\"T_cab75_row1_col33\" class=\"data row1 col33\" >0.07</td>\n",
       "      <td id=\"T_cab75_row1_col34\" class=\"data row1 col34\" >0.00</td>\n",
       "      <td id=\"T_cab75_row1_col35\" class=\"data row1 col35\" >0.01</td>\n",
       "      <td id=\"T_cab75_row1_col36\" class=\"data row1 col36\" >0.43</td>\n",
       "      <td id=\"T_cab75_row1_col37\" class=\"data row1 col37\" >0.00</td>\n",
       "      <td id=\"T_cab75_row1_col38\" class=\"data row1 col38\" >0.01</td>\n",
       "      <td id=\"T_cab75_row1_col39\" class=\"data row1 col39\" >0.01</td>\n",
       "      <td id=\"T_cab75_row1_col40\" class=\"data row1 col40\" >0.00</td>\n",
       "      <td id=\"T_cab75_row1_col41\" class=\"data row1 col41\" >0.02</td>\n",
       "      <td id=\"T_cab75_row1_col42\" class=\"data row1 col42\" >0.01</td>\n",
       "      <td id=\"T_cab75_row1_col43\" class=\"data row1 col43\" >0.01</td>\n",
       "      <td id=\"T_cab75_row1_col44\" class=\"data row1 col44\" >0.02</td>\n",
       "      <td id=\"T_cab75_row1_col45\" class=\"data row1 col45\" >0.05</td>\n",
       "      <td id=\"T_cab75_row1_col46\" class=\"data row1 col46\" >0.01</td>\n",
       "      <td id=\"T_cab75_row1_col47\" class=\"data row1 col47\" >0.01</td>\n",
       "      <td id=\"T_cab75_row1_col48\" class=\"data row1 col48\" >0.00</td>\n",
       "      <td id=\"T_cab75_row1_col49\" class=\"data row1 col49\" >0.01</td>\n",
       "      <td id=\"T_cab75_row1_col50\" class=\"data row1 col50\" >0.00</td>\n",
       "      <td id=\"T_cab75_row1_col51\" class=\"data row1 col51\" >0.01</td>\n",
       "      <td id=\"T_cab75_row1_col52\" class=\"data row1 col52\" >0.00</td>\n",
       "      <td id=\"T_cab75_row1_col53\" class=\"data row1 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cab75_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_cab75_row2_col0\" class=\"data row2 col0\" >0.24</td>\n",
       "      <td id=\"T_cab75_row2_col1\" class=\"data row2 col1\" >25.24</td>\n",
       "      <td id=\"T_cab75_row2_col2\" class=\"data row2 col2\" >76.66</td>\n",
       "      <td id=\"T_cab75_row2_col3\" class=\"data row2 col3\" >0.96</td>\n",
       "      <td id=\"T_cab75_row2_col4\" class=\"data row2 col4\" >5.48</td>\n",
       "      <td id=\"T_cab75_row2_col5\" class=\"data row2 col5\" >0.25</td>\n",
       "      <td id=\"T_cab75_row2_col6\" class=\"data row2 col6\" >0.16</td>\n",
       "      <td id=\"T_cab75_row2_col7\" class=\"data row2 col7\" >0.20</td>\n",
       "      <td id=\"T_cab75_row2_col8\" class=\"data row2 col8\" >0.39</td>\n",
       "      <td id=\"T_cab75_row2_col9\" class=\"data row2 col9\" >0.10</td>\n",
       "      <td id=\"T_cab75_row2_col10\" class=\"data row2 col10\" >0.28</td>\n",
       "      <td id=\"T_cab75_row2_col11\" class=\"data row2 col11\" >0.06</td>\n",
       "      <td id=\"T_cab75_row2_col12\" class=\"data row2 col12\" >0.06</td>\n",
       "      <td id=\"T_cab75_row2_col13\" class=\"data row2 col13\" >0.10</td>\n",
       "      <td id=\"T_cab75_row2_col14\" class=\"data row2 col14\" >0.42</td>\n",
       "      <td id=\"T_cab75_row2_col15\" class=\"data row2 col15\" >0.08</td>\n",
       "      <td id=\"T_cab75_row2_col16\" class=\"data row2 col16\" >0.07</td>\n",
       "      <td id=\"T_cab75_row2_col17\" class=\"data row2 col17\" >0.11</td>\n",
       "      <td id=\"T_cab75_row2_col18\" class=\"data row2 col18\" >0.29</td>\n",
       "      <td id=\"T_cab75_row2_col19\" class=\"data row2 col19\" >0.06</td>\n",
       "      <td id=\"T_cab75_row2_col20\" class=\"data row2 col20\" >0.07</td>\n",
       "      <td id=\"T_cab75_row2_col21\" class=\"data row2 col21\" >0.09</td>\n",
       "      <td id=\"T_cab75_row2_col22\" class=\"data row2 col22\" >0.06</td>\n",
       "      <td id=\"T_cab75_row2_col23\" class=\"data row2 col23\" >0.16</td>\n",
       "      <td id=\"T_cab75_row2_col24\" class=\"data row2 col24\" >0.23</td>\n",
       "      <td id=\"T_cab75_row2_col25\" class=\"data row2 col25\" >0.06</td>\n",
       "      <td id=\"T_cab75_row2_col26\" class=\"data row2 col26\" >0.31</td>\n",
       "      <td id=\"T_cab75_row2_col27\" class=\"data row2 col27\" >0.12</td>\n",
       "      <td id=\"T_cab75_row2_col28\" class=\"data row2 col28\" >0.15</td>\n",
       "      <td id=\"T_cab75_row2_col29\" class=\"data row2 col29\" >0.12</td>\n",
       "      <td id=\"T_cab75_row2_col30\" class=\"data row2 col30\" >0.31</td>\n",
       "      <td id=\"T_cab75_row2_col31\" class=\"data row2 col31\" >0.14</td>\n",
       "      <td id=\"T_cab75_row2_col32\" class=\"data row2 col32\" >0.08</td>\n",
       "      <td id=\"T_cab75_row2_col33\" class=\"data row2 col33\" >0.26</td>\n",
       "      <td id=\"T_cab75_row2_col34\" class=\"data row2 col34\" >0.04</td>\n",
       "      <td id=\"T_cab75_row2_col35\" class=\"data row2 col35\" >0.11</td>\n",
       "      <td id=\"T_cab75_row2_col36\" class=\"data row2 col36\" >0.49</td>\n",
       "      <td id=\"T_cab75_row2_col37\" class=\"data row2 col37\" >0.03</td>\n",
       "      <td id=\"T_cab75_row2_col38\" class=\"data row2 col38\" >0.11</td>\n",
       "      <td id=\"T_cab75_row2_col39\" class=\"data row2 col39\" >0.11</td>\n",
       "      <td id=\"T_cab75_row2_col40\" class=\"data row2 col40\" >0.05</td>\n",
       "      <td id=\"T_cab75_row2_col41\" class=\"data row2 col41\" >0.13</td>\n",
       "      <td id=\"T_cab75_row2_col42\" class=\"data row2 col42\" >0.11</td>\n",
       "      <td id=\"T_cab75_row2_col43\" class=\"data row2 col43\" >0.10</td>\n",
       "      <td id=\"T_cab75_row2_col44\" class=\"data row2 col44\" >0.15</td>\n",
       "      <td id=\"T_cab75_row2_col45\" class=\"data row2 col45\" >0.23</td>\n",
       "      <td id=\"T_cab75_row2_col46\" class=\"data row2 col46\" >0.09</td>\n",
       "      <td id=\"T_cab75_row2_col47\" class=\"data row2 col47\" >0.08</td>\n",
       "      <td id=\"T_cab75_row2_col48\" class=\"data row2 col48\" >0.06</td>\n",
       "      <td id=\"T_cab75_row2_col49\" class=\"data row2 col49\" >0.10</td>\n",
       "      <td id=\"T_cab75_row2_col50\" class=\"data row2 col50\" >0.03</td>\n",
       "      <td id=\"T_cab75_row2_col51\" class=\"data row2 col51\" >0.07</td>\n",
       "      <td id=\"T_cab75_row2_col52\" class=\"data row2 col52\" >0.04</td>\n",
       "      <td id=\"T_cab75_row2_col53\" class=\"data row2 col53\" >0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cab75_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_cab75_row3_col0\" class=\"data row3 col0\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col1\" class=\"data row3 col1\" >243.20</td>\n",
       "      <td id=\"T_cab75_row3_col2\" class=\"data row3 col2\" >0.01</td>\n",
       "      <td id=\"T_cab75_row3_col3\" class=\"data row3 col3\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col4\" class=\"data row3 col4\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col5\" class=\"data row3 col5\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col6\" class=\"data row3 col6\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col7\" class=\"data row3 col7\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col8\" class=\"data row3 col8\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col9\" class=\"data row3 col9\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col10\" class=\"data row3 col10\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col11\" class=\"data row3 col11\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col12\" class=\"data row3 col12\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col13\" class=\"data row3 col13\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col14\" class=\"data row3 col14\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col15\" class=\"data row3 col15\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col16\" class=\"data row3 col16\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col17\" class=\"data row3 col17\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col18\" class=\"data row3 col18\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col19\" class=\"data row3 col19\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col20\" class=\"data row3 col20\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col21\" class=\"data row3 col21\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col22\" class=\"data row3 col22\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col23\" class=\"data row3 col23\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col24\" class=\"data row3 col24\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col25\" class=\"data row3 col25\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col26\" class=\"data row3 col26\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col27\" class=\"data row3 col27\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col28\" class=\"data row3 col28\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col29\" class=\"data row3 col29\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col30\" class=\"data row3 col30\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col31\" class=\"data row3 col31\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col32\" class=\"data row3 col32\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col33\" class=\"data row3 col33\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col34\" class=\"data row3 col34\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col35\" class=\"data row3 col35\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col36\" class=\"data row3 col36\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col37\" class=\"data row3 col37\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col38\" class=\"data row3 col38\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col39\" class=\"data row3 col39\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col40\" class=\"data row3 col40\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col41\" class=\"data row3 col41\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col42\" class=\"data row3 col42\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col43\" class=\"data row3 col43\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col44\" class=\"data row3 col44\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col45\" class=\"data row3 col45\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col46\" class=\"data row3 col46\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col47\" class=\"data row3 col47\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col48\" class=\"data row3 col48\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col49\" class=\"data row3 col49\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col50\" class=\"data row3 col50\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col51\" class=\"data row3 col51\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col52\" class=\"data row3 col52\" >0.00</td>\n",
       "      <td id=\"T_cab75_row3_col53\" class=\"data row3 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cab75_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_cab75_row4_col0\" class=\"data row4 col0\" >0.14</td>\n",
       "      <td id=\"T_cab75_row4_col1\" class=\"data row4 col1\" >308.15</td>\n",
       "      <td id=\"T_cab75_row4_col2\" class=\"data row4 col2\" >10.00</td>\n",
       "      <td id=\"T_cab75_row4_col3\" class=\"data row4 col3\" >1.00</td>\n",
       "      <td id=\"T_cab75_row4_col4\" class=\"data row4 col4\" >3.00</td>\n",
       "      <td id=\"T_cab75_row4_col5\" class=\"data row4 col5\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col6\" class=\"data row4 col6\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col7\" class=\"data row4 col7\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col8\" class=\"data row4 col8\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col9\" class=\"data row4 col9\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col10\" class=\"data row4 col10\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col11\" class=\"data row4 col11\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col12\" class=\"data row4 col12\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col13\" class=\"data row4 col13\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col14\" class=\"data row4 col14\" >1.00</td>\n",
       "      <td id=\"T_cab75_row4_col15\" class=\"data row4 col15\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col16\" class=\"data row4 col16\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col17\" class=\"data row4 col17\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col18\" class=\"data row4 col18\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col19\" class=\"data row4 col19\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col20\" class=\"data row4 col20\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col21\" class=\"data row4 col21\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col22\" class=\"data row4 col22\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col23\" class=\"data row4 col23\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col24\" class=\"data row4 col24\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col25\" class=\"data row4 col25\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col26\" class=\"data row4 col26\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col27\" class=\"data row4 col27\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col28\" class=\"data row4 col28\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col29\" class=\"data row4 col29\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col30\" class=\"data row4 col30\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col31\" class=\"data row4 col31\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col32\" class=\"data row4 col32\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col33\" class=\"data row4 col33\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col34\" class=\"data row4 col34\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col35\" class=\"data row4 col35\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col36\" class=\"data row4 col36\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col37\" class=\"data row4 col37\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col38\" class=\"data row4 col38\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col39\" class=\"data row4 col39\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col40\" class=\"data row4 col40\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col41\" class=\"data row4 col41\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col42\" class=\"data row4 col42\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col43\" class=\"data row4 col43\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col44\" class=\"data row4 col44\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col45\" class=\"data row4 col45\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col46\" class=\"data row4 col46\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col47\" class=\"data row4 col47\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col48\" class=\"data row4 col48\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col49\" class=\"data row4 col49\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col50\" class=\"data row4 col50\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col51\" class=\"data row4 col51\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col52\" class=\"data row4 col52\" >0.00</td>\n",
       "      <td id=\"T_cab75_row4_col53\" class=\"data row4 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cab75_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_cab75_row5_col0\" class=\"data row5 col0\" >0.30</td>\n",
       "      <td id=\"T_cab75_row5_col1\" class=\"data row5 col1\" >323.15</td>\n",
       "      <td id=\"T_cab75_row5_col2\" class=\"data row5 col2\" >26.80</td>\n",
       "      <td id=\"T_cab75_row5_col3\" class=\"data row5 col3\" >1.00</td>\n",
       "      <td id=\"T_cab75_row5_col4\" class=\"data row5 col4\" >3.00</td>\n",
       "      <td id=\"T_cab75_row5_col5\" class=\"data row5 col5\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col6\" class=\"data row5 col6\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col7\" class=\"data row5 col7\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col8\" class=\"data row5 col8\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col9\" class=\"data row5 col9\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col10\" class=\"data row5 col10\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col11\" class=\"data row5 col11\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col12\" class=\"data row5 col12\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col13\" class=\"data row5 col13\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col14\" class=\"data row5 col14\" >1.00</td>\n",
       "      <td id=\"T_cab75_row5_col15\" class=\"data row5 col15\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col16\" class=\"data row5 col16\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col17\" class=\"data row5 col17\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col18\" class=\"data row5 col18\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col19\" class=\"data row5 col19\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col20\" class=\"data row5 col20\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col21\" class=\"data row5 col21\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col22\" class=\"data row5 col22\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col23\" class=\"data row5 col23\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col24\" class=\"data row5 col24\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col25\" class=\"data row5 col25\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col26\" class=\"data row5 col26\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col27\" class=\"data row5 col27\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col28\" class=\"data row5 col28\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col29\" class=\"data row5 col29\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col30\" class=\"data row5 col30\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col31\" class=\"data row5 col31\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col32\" class=\"data row5 col32\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col33\" class=\"data row5 col33\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col34\" class=\"data row5 col34\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col35\" class=\"data row5 col35\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col36\" class=\"data row5 col36\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col37\" class=\"data row5 col37\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col38\" class=\"data row5 col38\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col39\" class=\"data row5 col39\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col40\" class=\"data row5 col40\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col41\" class=\"data row5 col41\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col42\" class=\"data row5 col42\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col43\" class=\"data row5 col43\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col44\" class=\"data row5 col44\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col45\" class=\"data row5 col45\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col46\" class=\"data row5 col46\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col47\" class=\"data row5 col47\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col48\" class=\"data row5 col48\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col49\" class=\"data row5 col49\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col50\" class=\"data row5 col50\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col51\" class=\"data row5 col51\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col52\" class=\"data row5 col52\" >0.00</td>\n",
       "      <td id=\"T_cab75_row5_col53\" class=\"data row5 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cab75_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_cab75_row6_col0\" class=\"data row6 col0\" >0.51</td>\n",
       "      <td id=\"T_cab75_row6_col1\" class=\"data row6 col1\" >342.59</td>\n",
       "      <td id=\"T_cab75_row6_col2\" class=\"data row6 col2\" >64.76</td>\n",
       "      <td id=\"T_cab75_row6_col3\" class=\"data row6 col3\" >1.00</td>\n",
       "      <td id=\"T_cab75_row6_col4\" class=\"data row6 col4\" >5.00</td>\n",
       "      <td id=\"T_cab75_row6_col5\" class=\"data row6 col5\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col6\" class=\"data row6 col6\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col7\" class=\"data row6 col7\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col8\" class=\"data row6 col8\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col9\" class=\"data row6 col9\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col10\" class=\"data row6 col10\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col11\" class=\"data row6 col11\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col12\" class=\"data row6 col12\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col13\" class=\"data row6 col13\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col14\" class=\"data row6 col14\" >1.00</td>\n",
       "      <td id=\"T_cab75_row6_col15\" class=\"data row6 col15\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col16\" class=\"data row6 col16\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col17\" class=\"data row6 col17\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col18\" class=\"data row6 col18\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col19\" class=\"data row6 col19\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col20\" class=\"data row6 col20\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col21\" class=\"data row6 col21\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col22\" class=\"data row6 col22\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col23\" class=\"data row6 col23\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col24\" class=\"data row6 col24\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col25\" class=\"data row6 col25\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col26\" class=\"data row6 col26\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col27\" class=\"data row6 col27\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col28\" class=\"data row6 col28\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col29\" class=\"data row6 col29\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col30\" class=\"data row6 col30\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col31\" class=\"data row6 col31\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col32\" class=\"data row6 col32\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col33\" class=\"data row6 col33\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col34\" class=\"data row6 col34\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col35\" class=\"data row6 col35\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col36\" class=\"data row6 col36\" >1.00</td>\n",
       "      <td id=\"T_cab75_row6_col37\" class=\"data row6 col37\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col38\" class=\"data row6 col38\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col39\" class=\"data row6 col39\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col40\" class=\"data row6 col40\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col41\" class=\"data row6 col41\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col42\" class=\"data row6 col42\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col43\" class=\"data row6 col43\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col44\" class=\"data row6 col44\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col45\" class=\"data row6 col45\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col46\" class=\"data row6 col46\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col47\" class=\"data row6 col47\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col48\" class=\"data row6 col48\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col49\" class=\"data row6 col49\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col50\" class=\"data row6 col50\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col51\" class=\"data row6 col51\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col52\" class=\"data row6 col52\" >0.00</td>\n",
       "      <td id=\"T_cab75_row6_col53\" class=\"data row6 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cab75_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_cab75_row7_col0\" class=\"data row7 col0\" >0.95</td>\n",
       "      <td id=\"T_cab75_row7_col1\" class=\"data row7 col1\" >453.15</td>\n",
       "      <td id=\"T_cab75_row7_col2\" class=\"data row7 col2\" >499.90</td>\n",
       "      <td id=\"T_cab75_row7_col3\" class=\"data row7 col3\" >7.00</td>\n",
       "      <td id=\"T_cab75_row7_col4\" class=\"data row7 col4\" >28.00</td>\n",
       "      <td id=\"T_cab75_row7_col5\" class=\"data row7 col5\" >3.00</td>\n",
       "      <td id=\"T_cab75_row7_col6\" class=\"data row7 col6\" >2.00</td>\n",
       "      <td id=\"T_cab75_row7_col7\" class=\"data row7 col7\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col8\" class=\"data row7 col8\" >5.00</td>\n",
       "      <td id=\"T_cab75_row7_col9\" class=\"data row7 col9\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col10\" class=\"data row7 col10\" >3.00</td>\n",
       "      <td id=\"T_cab75_row7_col11\" class=\"data row7 col11\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col12\" class=\"data row7 col12\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col13\" class=\"data row7 col13\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col14\" class=\"data row7 col14\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col15\" class=\"data row7 col15\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col16\" class=\"data row7 col16\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col17\" class=\"data row7 col17\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col18\" class=\"data row7 col18\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col19\" class=\"data row7 col19\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col20\" class=\"data row7 col20\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col21\" class=\"data row7 col21\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col22\" class=\"data row7 col22\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col23\" class=\"data row7 col23\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col24\" class=\"data row7 col24\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col25\" class=\"data row7 col25\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col26\" class=\"data row7 col26\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col27\" class=\"data row7 col27\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col28\" class=\"data row7 col28\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col29\" class=\"data row7 col29\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col30\" class=\"data row7 col30\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col31\" class=\"data row7 col31\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col32\" class=\"data row7 col32\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col33\" class=\"data row7 col33\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col34\" class=\"data row7 col34\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col35\" class=\"data row7 col35\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col36\" class=\"data row7 col36\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col37\" class=\"data row7 col37\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col38\" class=\"data row7 col38\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col39\" class=\"data row7 col39\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col40\" class=\"data row7 col40\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col41\" class=\"data row7 col41\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col42\" class=\"data row7 col42\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col43\" class=\"data row7 col43\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col44\" class=\"data row7 col44\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col45\" class=\"data row7 col45\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col46\" class=\"data row7 col46\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col47\" class=\"data row7 col47\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col48\" class=\"data row7 col48\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col49\" class=\"data row7 col49\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col50\" class=\"data row7 col50\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col51\" class=\"data row7 col51\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col52\" class=\"data row7 col52\" >1.00</td>\n",
       "      <td id=\"T_cab75_row7_col53\" class=\"data row7 col53\" >1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5d02d98280>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataCO2f='../DS4B-CO2-data'+'/'+'dataCO2.csv'\n",
    "dataCO2=pd.read_csv(dataCO2f,sep=\";\",header=0)\n",
    "display(dataCO2)\n",
    "# describe() generates descriptive statistics\n",
    "display(dataCO2.describe().style.format(\"{0:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf1e66-0756-47f2-928d-d0d578613bba",
   "metadata": {},
   "source": [
    "## 2. Assessment of the stability of the original ML algorithm of Song *et al*. by *K*-fold cross validation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50b3ba1f-2f90-441f-b931-28bc9e64748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# separation of the data set into two subsets: (1) training of the ANN & (2) test of the ANN\n",
    "# library used: pandas\n",
    "xdata = dataCO2.drop(['IL','cation','anion','x_CO2'],axis=1)\n",
    "ydata = dataCO2['x_CO2']\n",
    "\n",
    "#######################################################################################\n",
    "# ANN: 1 input layer (53 neurons) / 2 hidden layers (20 and 7 neurons) / 1 output layer (1 neuron) \n",
    "# library used: keras\n",
    "\n",
    "def defANN(shape,acthL):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name='iLayer'))\n",
    "    model.add(keras.layers.Dense(7, activation=acthL, name='hLayer'))\n",
    "    model.add(keras.layers.Dense(1, name='oLayer'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "acthL='tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a3138b7-5dc7-4804-93cb-caab9887319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[91mFold 0\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 2s 4ms/step - loss: 0.0775 - mae: 0.2069 - mse: 0.0775 - val_loss: 0.0520 - val_mae: 0.1919 - val_mse: 0.0520\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0475 - mae: 0.1811 - mse: 0.0475 - val_loss: 0.0451 - val_mae: 0.1771 - val_mse: 0.0451\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0329 - mae: 0.1474 - mse: 0.0329 - val_loss: 0.0216 - val_mae: 0.1180 - val_mse: 0.0216\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0216 - mae: 0.1192 - mse: 0.0216 - val_loss: 0.0209 - val_mae: 0.1163 - val_mse: 0.0209\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0214 - mae: 0.1184 - mse: 0.0214 - val_loss: 0.0210 - val_mae: 0.1171 - val_mse: 0.0210\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0211 - mae: 0.1181 - mse: 0.0211 - val_loss: 0.0200 - val_mae: 0.1140 - val_mse: 0.0200\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0207 - mae: 0.1169 - mse: 0.0207 - val_loss: 0.0196 - val_mae: 0.1132 - val_mse: 0.0196\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0201 - mae: 0.1149 - mse: 0.0201 - val_loss: 0.0194 - val_mae: 0.1121 - val_mse: 0.0194\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0192 - mae: 0.1124 - mse: 0.0192 - val_loss: 0.0191 - val_mae: 0.1115 - val_mse: 0.0191\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0178 - mae: 0.1075 - mse: 0.0178 - val_loss: 0.0170 - val_mae: 0.1053 - val_mse: 0.0170\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0152 - mae: 0.0987 - mse: 0.0152 - val_loss: 0.0152 - val_mae: 0.0973 - val_mse: 0.0152\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0128 - mae: 0.0894 - mse: 0.0128 - val_loss: 0.0113 - val_mae: 0.0831 - val_mse: 0.0113\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0114 - mae: 0.0836 - mse: 0.0114 - val_loss: 0.0104 - val_mae: 0.0802 - val_mse: 0.0104\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0103 - mae: 0.0792 - mse: 0.0103 - val_loss: 0.0100 - val_mae: 0.0783 - val_mse: 0.0100\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0098 - mae: 0.0767 - mse: 0.0098 - val_loss: 0.0098 - val_mae: 0.0767 - val_mse: 0.0098\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0095 - mae: 0.0758 - mse: 0.0095 - val_loss: 0.0101 - val_mae: 0.0795 - val_mse: 0.0101\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0743 - mse: 0.0092 - val_loss: 0.0090 - val_mae: 0.0727 - val_mse: 0.0090\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0094 - mae: 0.0755 - mse: 0.0094 - val_loss: 0.0099 - val_mae: 0.0787 - val_mse: 0.0099\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0743 - mse: 0.0092 - val_loss: 0.0090 - val_mae: 0.0738 - val_mse: 0.0090\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0735 - mse: 0.0090 - val_loss: 0.0090 - val_mae: 0.0739 - val_mse: 0.0090\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0728 - mse: 0.0089 - val_loss: 0.0088 - val_mae: 0.0717 - val_mse: 0.0088\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0736 - mse: 0.0090 - val_loss: 0.0095 - val_mae: 0.0770 - val_mse: 0.0095\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0723 - mse: 0.0088 - val_loss: 0.0106 - val_mae: 0.0833 - val_mse: 0.0106\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0727 - mse: 0.0089 - val_loss: 0.0087 - val_mae: 0.0709 - val_mse: 0.0087\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0719 - mse: 0.0087 - val_loss: 0.0095 - val_mae: 0.0775 - val_mse: 0.0095\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0726 - mse: 0.0088 - val_loss: 0.0110 - val_mae: 0.0850 - val_mse: 0.0110\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0723 - mse: 0.0088 - val_loss: 0.0087 - val_mae: 0.0703 - val_mse: 0.0087\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0722 - mse: 0.0087 - val_loss: 0.0093 - val_mae: 0.0770 - val_mse: 0.0093\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0723 - mse: 0.0088 - val_loss: 0.0093 - val_mae: 0.0736 - val_mse: 0.0093\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0727 - mse: 0.0088 - val_loss: 0.0082 - val_mae: 0.0692 - val_mse: 0.0082\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0716 - mse: 0.0087 - val_loss: 0.0088 - val_mae: 0.0738 - val_mse: 0.0088\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0710 - mse: 0.0085 - val_loss: 0.0086 - val_mae: 0.0727 - val_mse: 0.0086\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0711 - mse: 0.0086 - val_loss: 0.0083 - val_mae: 0.0693 - val_mse: 0.0083\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0711 - mse: 0.0086 - val_loss: 0.0094 - val_mae: 0.0775 - val_mse: 0.0094\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0716 - mse: 0.0086 - val_loss: 0.0084 - val_mae: 0.0713 - val_mse: 0.0084\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0711 - mse: 0.0086 - val_loss: 0.0083 - val_mae: 0.0690 - val_mse: 0.0083\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0712 - mse: 0.0086 - val_loss: 0.0083 - val_mae: 0.0706 - val_mse: 0.0083\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0708 - mse: 0.0085 - val_loss: 0.0081 - val_mae: 0.0694 - val_mse: 0.0081\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0710 - mse: 0.0086 - val_loss: 0.0085 - val_mae: 0.0718 - val_mse: 0.0085\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0716 - mse: 0.0087 - val_loss: 0.0081 - val_mae: 0.0688 - val_mse: 0.0081\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0710 - mse: 0.0086 - val_loss: 0.0084 - val_mae: 0.0713 - val_mse: 0.0084\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0711 - mse: 0.0086 - val_loss: 0.0085 - val_mae: 0.0727 - val_mse: 0.0085\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0711 - mse: 0.0085 - val_loss: 0.0081 - val_mae: 0.0692 - val_mse: 0.0081\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0714 - mse: 0.0086 - val_loss: 0.0082 - val_mae: 0.0677 - val_mse: 0.0082\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0714 - mse: 0.0086 - val_loss: 0.0088 - val_mae: 0.0747 - val_mse: 0.0088\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0715 - mse: 0.0087 - val_loss: 0.0099 - val_mae: 0.0806 - val_mse: 0.0099\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0710 - mse: 0.0085 - val_loss: 0.0105 - val_mae: 0.0835 - val_mse: 0.0105\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0707 - mse: 0.0085 - val_loss: 0.0084 - val_mae: 0.0685 - val_mse: 0.0084\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0715 - mse: 0.0087 - val_loss: 0.0104 - val_mae: 0.0833 - val_mse: 0.0104\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0715 - mse: 0.0086 - val_loss: 0.0082 - val_mae: 0.0706 - val_mse: 0.0082\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0709 - mse: 0.0085 - val_loss: 0.0080 - val_mae: 0.0679 - val_mse: 0.0080\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0707 - mse: 0.0084 - val_loss: 0.0081 - val_mae: 0.0679 - val_mse: 0.0081\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0710 - mse: 0.0085 - val_loss: 0.0081 - val_mae: 0.0694 - val_mse: 0.0081\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0727 - mse: 0.0088 - val_loss: 0.0093 - val_mae: 0.0780 - val_mse: 0.0093\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0710 - mse: 0.0085 - val_loss: 0.0089 - val_mae: 0.0708 - val_mse: 0.0089\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0708 - mse: 0.0084 - val_loss: 0.0090 - val_mae: 0.0759 - val_mse: 0.0090\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0709 - mse: 0.0085 - val_loss: 0.0091 - val_mae: 0.0766 - val_mse: 0.0091\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0708 - mse: 0.0084 - val_loss: 0.0082 - val_mae: 0.0683 - val_mse: 0.0082\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0714 - mse: 0.0086 - val_loss: 0.0082 - val_mae: 0.0698 - val_mse: 0.0082\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0714 - mse: 0.0086 - val_loss: 0.0080 - val_mae: 0.0686 - val_mse: 0.0080\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0719 - mse: 0.0087 - val_loss: 0.0083 - val_mae: 0.0689 - val_mse: 0.0083\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0715 - mse: 0.0086 - val_loss: 0.0080 - val_mae: 0.0680 - val_mse: 0.0080\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0700 - mse: 0.0083 - val_loss: 0.0097 - val_mae: 0.0747 - val_mse: 0.0097\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0706 - mse: 0.0085 - val_loss: 0.0086 - val_mae: 0.0699 - val_mse: 0.0086\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0712 - mse: 0.0085 - val_loss: 0.0096 - val_mae: 0.0799 - val_mse: 0.0096\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0712 - mse: 0.0086 - val_loss: 0.0084 - val_mae: 0.0723 - val_mse: 0.0084\n",
      "Epoch 67/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0714 - mse: 0.0086 - val_loss: 0.0086 - val_mae: 0.0697 - val_mse: 0.0086\n",
      "Epoch 68/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0711 - mse: 0.0085 - val_loss: 0.0081 - val_mae: 0.0675 - val_mse: 0.0081\n",
      "Epoch 69/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0705 - mse: 0.0084 - val_loss: 0.0083 - val_mae: 0.0689 - val_mse: 0.0083\n",
      "Epoch 70/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0715 - mse: 0.0086 - val_loss: 0.0081 - val_mae: 0.0699 - val_mse: 0.0081\n",
      "Epoch 71/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0713 - mse: 0.0086 - val_loss: 0.0087 - val_mae: 0.0740 - val_mse: 0.0087\n",
      "Epoch 72/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0709 - mse: 0.0085 - val_loss: 0.0081 - val_mae: 0.0690 - val_mse: 0.0081\n",
      "Epoch 73/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0715 - mse: 0.0085 - val_loss: 0.0080 - val_mae: 0.0678 - val_mse: 0.0080\n",
      "Epoch 74/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0716 - mse: 0.0086 - val_loss: 0.0081 - val_mae: 0.0687 - val_mse: 0.0081\n",
      "Epoch 75/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0715 - mse: 0.0086 - val_loss: 0.0089 - val_mae: 0.0755 - val_mse: 0.0089\n",
      "Epoch 75: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 1ms/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.021353691197268668    std:  0.0919302710974141    MAE:  0.07535818604776258     R2:  0.9211544977562612\n",
      "Test. mean:  -0.024213808868489964    std:  0.09127358130675903    MAE:  0.07545343328135191     R2:  0.9243258096710202\n",
      "\u001b[1m\u001b[91mFold 1\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0706 - mae: 0.2220 - mse: 0.0706 - val_loss: 0.0553 - val_mae: 0.1996 - val_mse: 0.0553\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2013 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.1997 - val_mse: 0.0553\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2011 - mse: 0.0560 - val_loss: 0.0554 - val_mae: 0.2007 - val_mse: 0.0554\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2010 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.2003 - val_mse: 0.0553\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2011 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.2003 - val_mse: 0.0553\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1984 - val_mse: 0.0553\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2011 - mse: 0.0561 - val_loss: 0.0568 - val_mae: 0.2054 - val_mse: 0.0568\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2014 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.2001 - val_mse: 0.0553\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2012 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.1989 - val_mse: 0.0553\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2011 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.1992 - val_mse: 0.0553\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.1982 - val_mse: 0.0554\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2012 - mse: 0.0562 - val_loss: 0.0553 - val_mae: 0.1991 - val_mse: 0.0553\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2010 - mse: 0.0560 - val_loss: 0.0554 - val_mae: 0.2007 - val_mse: 0.0554\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2011 - mse: 0.0560 - val_loss: 0.0554 - val_mae: 0.2006 - val_mse: 0.0554\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2012 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.1999 - val_mse: 0.0553\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2013 - mse: 0.0560 - val_loss: 0.0554 - val_mae: 0.2007 - val_mse: 0.0554\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2010 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.1986 - val_mse: 0.0553\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0556 - val_mae: 0.2017 - val_mse: 0.0556\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2013 - mse: 0.0562 - val_loss: 0.0553 - val_mae: 0.2000 - val_mse: 0.0553\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2009 - mse: 0.0560 - val_loss: 0.0557 - val_mae: 0.2020 - val_mse: 0.0557\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2014 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.2008 - val_mse: 0.0554\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1991 - val_mse: 0.0553\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2014 - mse: 0.0561 - val_loss: 0.0557 - val_mae: 0.2019 - val_mse: 0.0557\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1993 - val_mse: 0.0553\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2011 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.2009 - val_mse: 0.0554\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0559 - mae: 0.2010 - mse: 0.0559 - val_loss: 0.0556 - val_mae: 0.2017 - val_mse: 0.0556\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2011 - mse: 0.0560 - val_loss: 0.0554 - val_mae: 0.2007 - val_mse: 0.0554\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2011 - mse: 0.0560 - val_loss: 0.0554 - val_mae: 0.2007 - val_mse: 0.0554\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2014 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1997 - val_mse: 0.0553\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2011 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.1983 - val_mse: 0.0554\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0555 - val_mae: 0.2013 - val_mse: 0.0555\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.2010 - val_mse: 0.0554\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2012 - mse: 0.0560 - val_loss: 0.0554 - val_mae: 0.1982 - val_mse: 0.0554\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0557 - val_mae: 0.1973 - val_mse: 0.0557\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2011 - mse: 0.0560 - val_loss: 0.0554 - val_mae: 0.2005 - val_mse: 0.0554\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2011 - mse: 0.0561 - val_loss: 0.0557 - val_mae: 0.1973 - val_mse: 0.0557\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2012 - mse: 0.0560 - val_loss: 0.0555 - val_mae: 0.1976 - val_mse: 0.0555\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2011 - mse: 0.0561 - val_loss: 0.0566 - val_mae: 0.2050 - val_mse: 0.0566\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2015 - mse: 0.0562 - val_loss: 0.0553 - val_mae: 0.1994 - val_mse: 0.0553\n",
      "Epoch 39: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 1ms/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.005636940900222737    std:  0.23641055181585133    MAE:  0.20043939787625006     R2:  nan\n",
      "Test. mean:  0.001177426157189078    std:  0.23507950122645024    MAE:  0.19940420354190627     R2:  0.06136529369988125\n",
      "\u001b[1m\u001b[91mFold 2\u001b[0m\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romuald/anaconda3/envs/ML/lib/python3.10/site-packages/numpy/lib/function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/romuald/anaconda3/envs/ML/lib/python3.10/site-packages/numpy/lib/function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 1s 2ms/step - loss: 0.1083 - mae: 0.2523 - mse: 0.1083 - val_loss: 0.0386 - val_mae: 0.1617 - val_mse: 0.0386\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0307 - mae: 0.1374 - mse: 0.0307 - val_loss: 0.0225 - val_mae: 0.1139 - val_mse: 0.0225\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0200 - mae: 0.1086 - mse: 0.0200 - val_loss: 0.0166 - val_mae: 0.0999 - val_mse: 0.0166\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0170 - mae: 0.0967 - mse: 0.0170 - val_loss: 0.0139 - val_mae: 0.0879 - val_mse: 0.0139\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0154 - mae: 0.0901 - mse: 0.0154 - val_loss: 0.0124 - val_mae: 0.0864 - val_mse: 0.0124\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0135 - mae: 0.0847 - mse: 0.0135 - val_loss: 0.0133 - val_mae: 0.0834 - val_mse: 0.0133\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0119 - mae: 0.0820 - mse: 0.0119 - val_loss: 0.0119 - val_mae: 0.0859 - val_mse: 0.0119\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0124 - mae: 0.0818 - mse: 0.0124 - val_loss: 0.0111 - val_mae: 0.0757 - val_mse: 0.0111\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0118 - mae: 0.0796 - mse: 0.0118 - val_loss: 0.0091 - val_mae: 0.0726 - val_mse: 0.0091\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0107 - mae: 0.0778 - mse: 0.0107 - val_loss: 0.0098 - val_mae: 0.0735 - val_mse: 0.0098\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0108 - mae: 0.0766 - mse: 0.0108 - val_loss: 0.0090 - val_mae: 0.0724 - val_mse: 0.0090\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0105 - mae: 0.0767 - mse: 0.0105 - val_loss: 0.0103 - val_mae: 0.0734 - val_mse: 0.0103\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0105 - mae: 0.0756 - mse: 0.0105 - val_loss: 0.0105 - val_mae: 0.0736 - val_mse: 0.0105\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0106 - mae: 0.0761 - mse: 0.0106 - val_loss: 0.0090 - val_mae: 0.0721 - val_mse: 0.0090\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0103 - mae: 0.0748 - mse: 0.0103 - val_loss: 0.0107 - val_mae: 0.0727 - val_mse: 0.0107\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0101 - mae: 0.0752 - mse: 0.0101 - val_loss: 0.0090 - val_mae: 0.0713 - val_mse: 0.0090\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0099 - mae: 0.0744 - mse: 0.0099 - val_loss: 0.0094 - val_mae: 0.0717 - val_mse: 0.0094\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0101 - mae: 0.0749 - mse: 0.0101 - val_loss: 0.0100 - val_mae: 0.0744 - val_mse: 0.0100\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0098 - mae: 0.0740 - mse: 0.0098 - val_loss: 0.0090 - val_mae: 0.0692 - val_mse: 0.0090\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0109 - mae: 0.0749 - mse: 0.0109 - val_loss: 0.0094 - val_mae: 0.0695 - val_mse: 0.0094\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0095 - mae: 0.0736 - mse: 0.0095 - val_loss: 0.0154 - val_mae: 0.0892 - val_mse: 0.0154\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0095 - mae: 0.0740 - mse: 0.0095 - val_loss: 0.0085 - val_mae: 0.0721 - val_mse: 0.0085\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0096 - mae: 0.0723 - mse: 0.0096 - val_loss: 0.0106 - val_mae: 0.0787 - val_mse: 0.0106\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0094 - mae: 0.0725 - mse: 0.0094 - val_loss: 0.0102 - val_mae: 0.0781 - val_mse: 0.0102\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0100 - mae: 0.0758 - mse: 0.0100 - val_loss: 0.0112 - val_mae: 0.0725 - val_mse: 0.0112\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0098 - mae: 0.0730 - mse: 0.0098 - val_loss: 0.0102 - val_mae: 0.0813 - val_mse: 0.0102\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0724 - mse: 0.0091 - val_loss: 0.0088 - val_mae: 0.0687 - val_mse: 0.0088\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0093 - mae: 0.0728 - mse: 0.0093 - val_loss: 0.0079 - val_mae: 0.0661 - val_mse: 0.0079\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0093 - mae: 0.0725 - mse: 0.0093 - val_loss: 0.0098 - val_mae: 0.0759 - val_mse: 0.0098\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0726 - mse: 0.0091 - val_loss: 0.0078 - val_mae: 0.0669 - val_mse: 0.0078\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0718 - mse: 0.0089 - val_loss: 0.0080 - val_mae: 0.0662 - val_mse: 0.0080\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0725 - mse: 0.0091 - val_loss: 0.0084 - val_mae: 0.0679 - val_mse: 0.0084\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0094 - mae: 0.0729 - mse: 0.0094 - val_loss: 0.0081 - val_mae: 0.0708 - val_mse: 0.0081\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0715 - mse: 0.0088 - val_loss: 0.0078 - val_mae: 0.0658 - val_mse: 0.0078\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0097 - mae: 0.0727 - mse: 0.0097 - val_loss: 0.0079 - val_mae: 0.0665 - val_mse: 0.0079\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0726 - mse: 0.0091 - val_loss: 0.0085 - val_mae: 0.0685 - val_mse: 0.0085\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0730 - mse: 0.0091 - val_loss: 0.0090 - val_mae: 0.0707 - val_mse: 0.0090\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0717 - mse: 0.0089 - val_loss: 0.0080 - val_mae: 0.0662 - val_mse: 0.0080\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0713 - mse: 0.0090 - val_loss: 0.0078 - val_mae: 0.0659 - val_mse: 0.0078\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0721 - mse: 0.0091 - val_loss: 0.0077 - val_mae: 0.0676 - val_mse: 0.0077\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0718 - mse: 0.0088 - val_loss: 0.0085 - val_mae: 0.0687 - val_mse: 0.0085\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0712 - mse: 0.0088 - val_loss: 0.0081 - val_mae: 0.0681 - val_mse: 0.0081\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0729 - mse: 0.0092 - val_loss: 0.0084 - val_mae: 0.0721 - val_mse: 0.0084\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0707 - mse: 0.0086 - val_loss: 0.0078 - val_mae: 0.0681 - val_mse: 0.0078\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0714 - mse: 0.0088 - val_loss: 0.0080 - val_mae: 0.0661 - val_mse: 0.0080\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0707 - mse: 0.0086 - val_loss: 0.0079 - val_mae: 0.0681 - val_mse: 0.0079\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0722 - mse: 0.0089 - val_loss: 0.0078 - val_mae: 0.0684 - val_mse: 0.0078\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0705 - mse: 0.0086 - val_loss: 0.0087 - val_mae: 0.0692 - val_mse: 0.0087\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0716 - mse: 0.0089 - val_loss: 0.0085 - val_mae: 0.0729 - val_mse: 0.0085\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0720 - mse: 0.0089 - val_loss: 0.0078 - val_mae: 0.0655 - val_mse: 0.0078\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0714 - mse: 0.0089 - val_loss: 0.0083 - val_mae: 0.0678 - val_mse: 0.0083\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0699 - mse: 0.0086 - val_loss: 0.0083 - val_mae: 0.0657 - val_mse: 0.0083\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0694 - mse: 0.0083 - val_loss: 0.0080 - val_mae: 0.0654 - val_mse: 0.0080\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0694 - mse: 0.0084 - val_loss: 0.0076 - val_mae: 0.0647 - val_mse: 0.0076\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0699 - mse: 0.0085 - val_loss: 0.0089 - val_mae: 0.0714 - val_mse: 0.0089\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0700 - mse: 0.0084 - val_loss: 0.0075 - val_mae: 0.0640 - val_mse: 0.0075\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0698 - mse: 0.0084 - val_loss: 0.0074 - val_mae: 0.0652 - val_mse: 0.0074\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0691 - mse: 0.0083 - val_loss: 0.0075 - val_mae: 0.0638 - val_mse: 0.0075\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0692 - mse: 0.0082 - val_loss: 0.0076 - val_mae: 0.0645 - val_mse: 0.0076\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0081 - mae: 0.0687 - mse: 0.0081 - val_loss: 0.0075 - val_mae: 0.0649 - val_mse: 0.0075\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0692 - mse: 0.0082 - val_loss: 0.0074 - val_mae: 0.0638 - val_mse: 0.0074\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0697 - mse: 0.0083 - val_loss: 0.0074 - val_mae: 0.0657 - val_mse: 0.0074\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0081 - mae: 0.0684 - mse: 0.0081 - val_loss: 0.0088 - val_mae: 0.0757 - val_mse: 0.0088\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0689 - mse: 0.0082 - val_loss: 0.0084 - val_mae: 0.0677 - val_mse: 0.0084\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0703 - mse: 0.0084 - val_loss: 0.0149 - val_mae: 0.1019 - val_mse: 0.0149\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0688 - mse: 0.0082 - val_loss: 0.0078 - val_mae: 0.0651 - val_mse: 0.0078\n",
      "Epoch 67/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0690 - mse: 0.0083 - val_loss: 0.0075 - val_mae: 0.0656 - val_mse: 0.0075\n",
      "Epoch 68/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0706 - mse: 0.0086 - val_loss: 0.0076 - val_mae: 0.0647 - val_mse: 0.0076\n",
      "Epoch 69/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0691 - mse: 0.0082 - val_loss: 0.0077 - val_mae: 0.0686 - val_mse: 0.0077\n",
      "Epoch 70/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0689 - mse: 0.0082 - val_loss: 0.0073 - val_mae: 0.0641 - val_mse: 0.0073\n",
      "Epoch 71/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0695 - mse: 0.0082 - val_loss: 0.0075 - val_mae: 0.0659 - val_mse: 0.0075\n",
      "Epoch 72/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0691 - mse: 0.0082 - val_loss: 0.0083 - val_mae: 0.0733 - val_mse: 0.0083\n",
      "Epoch 73/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0697 - mse: 0.0083 - val_loss: 0.0075 - val_mae: 0.0664 - val_mse: 0.0075\n",
      "Epoch 74/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0698 - mse: 0.0083 - val_loss: 0.0074 - val_mae: 0.0640 - val_mse: 0.0074\n",
      "Epoch 75/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0689 - mse: 0.0082 - val_loss: 0.0076 - val_mae: 0.0638 - val_mse: 0.0076\n",
      "Epoch 76/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0696 - mse: 0.0084 - val_loss: 0.0078 - val_mae: 0.0650 - val_mse: 0.0078\n",
      "Epoch 77/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0081 - mae: 0.0686 - mse: 0.0081 - val_loss: 0.0074 - val_mae: 0.0639 - val_mse: 0.0074\n",
      "Epoch 78/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0694 - mse: 0.0083 - val_loss: 0.0082 - val_mae: 0.0722 - val_mse: 0.0082\n",
      "Epoch 79/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0081 - mae: 0.0683 - mse: 0.0081 - val_loss: 0.0074 - val_mae: 0.0639 - val_mse: 0.0074\n",
      "Epoch 80/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0699 - mse: 0.0085 - val_loss: 0.0076 - val_mae: 0.0663 - val_mse: 0.0076\n",
      "Epoch 81/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0681 - mse: 0.0082 - val_loss: 0.0075 - val_mae: 0.0629 - val_mse: 0.0075\n",
      "Epoch 82/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0680 - mse: 0.0082 - val_loss: 0.0074 - val_mae: 0.0658 - val_mse: 0.0074\n",
      "Epoch 83/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0682 - mse: 0.0082 - val_loss: 0.0073 - val_mae: 0.0656 - val_mse: 0.0073\n",
      "Epoch 84/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0079 - mae: 0.0668 - mse: 0.0079 - val_loss: 0.0072 - val_mae: 0.0665 - val_mse: 0.0072\n",
      "Epoch 85/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0079 - mae: 0.0675 - mse: 0.0079 - val_loss: 0.0066 - val_mae: 0.0607 - val_mse: 0.0066\n",
      "Epoch 86/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0073 - mae: 0.0644 - mse: 0.0073 - val_loss: 0.0071 - val_mae: 0.0666 - val_mse: 0.0071\n",
      "Epoch 87/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0073 - mae: 0.0642 - mse: 0.0073 - val_loss: 0.0091 - val_mae: 0.0787 - val_mse: 0.0091\n",
      "Epoch 88/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0641 - mse: 0.0072 - val_loss: 0.0066 - val_mae: 0.0629 - val_mse: 0.0066\n",
      "Epoch 89/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0639 - mse: 0.0072 - val_loss: 0.0090 - val_mae: 0.0777 - val_mse: 0.0090\n",
      "Epoch 90/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0642 - mse: 0.0072 - val_loss: 0.0088 - val_mae: 0.0704 - val_mse: 0.0088\n",
      "Epoch 91/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0643 - mse: 0.0072 - val_loss: 0.0065 - val_mae: 0.0598 - val_mse: 0.0065\n",
      "Epoch 92/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0637 - mse: 0.0071 - val_loss: 0.0064 - val_mae: 0.0609 - val_mse: 0.0064\n",
      "Epoch 93/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0643 - mse: 0.0072 - val_loss: 0.0072 - val_mae: 0.0678 - val_mse: 0.0072\n",
      "Epoch 94/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0073 - mae: 0.0643 - mse: 0.0073 - val_loss: 0.0074 - val_mae: 0.0625 - val_mse: 0.0074\n",
      "Epoch 95/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0639 - mse: 0.0072 - val_loss: 0.0078 - val_mae: 0.0713 - val_mse: 0.0078\n",
      "Epoch 96/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0073 - mae: 0.0647 - mse: 0.0073 - val_loss: 0.0067 - val_mae: 0.0637 - val_mse: 0.0067\n",
      "Epoch 97/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0634 - mse: 0.0071 - val_loss: 0.0081 - val_mae: 0.0728 - val_mse: 0.0081\n",
      "Epoch 98/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0070 - mae: 0.0629 - mse: 0.0070 - val_loss: 0.0064 - val_mae: 0.0603 - val_mse: 0.0064\n",
      "Epoch 99/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0634 - mse: 0.0071 - val_loss: 0.0066 - val_mae: 0.0597 - val_mse: 0.0066\n",
      "Epoch 100/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0643 - mse: 0.0072 - val_loss: 0.0063 - val_mae: 0.0591 - val_mse: 0.0063\n",
      "Epoch 101/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0636 - mse: 0.0071 - val_loss: 0.0064 - val_mae: 0.0617 - val_mse: 0.0064\n",
      "Epoch 102/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0637 - mse: 0.0071 - val_loss: 0.0062 - val_mae: 0.0591 - val_mse: 0.0062\n",
      "Epoch 103/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0070 - mae: 0.0628 - mse: 0.0070 - val_loss: 0.0062 - val_mae: 0.0590 - val_mse: 0.0062\n",
      "Epoch 104/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0070 - mae: 0.0629 - mse: 0.0070 - val_loss: 0.0065 - val_mae: 0.0627 - val_mse: 0.0065\n",
      "Epoch 105/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0635 - mse: 0.0071 - val_loss: 0.0062 - val_mae: 0.0585 - val_mse: 0.0062\n",
      "Epoch 106/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0634 - mse: 0.0071 - val_loss: 0.0063 - val_mae: 0.0607 - val_mse: 0.0063\n",
      "Epoch 107/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0627 - mse: 0.0069 - val_loss: 0.0064 - val_mae: 0.0609 - val_mse: 0.0064\n",
      "Epoch 108/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0633 - mse: 0.0071 - val_loss: 0.0072 - val_mae: 0.0675 - val_mse: 0.0072\n",
      "Epoch 109/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0623 - mse: 0.0069 - val_loss: 0.0062 - val_mae: 0.0602 - val_mse: 0.0062\n",
      "Epoch 110/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0616 - mse: 0.0068 - val_loss: 0.0064 - val_mae: 0.0584 - val_mse: 0.0064\n",
      "Epoch 111/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0618 - mse: 0.0068 - val_loss: 0.0061 - val_mae: 0.0583 - val_mse: 0.0061\n",
      "Epoch 112/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0070 - mae: 0.0632 - mse: 0.0070 - val_loss: 0.0063 - val_mae: 0.0609 - val_mse: 0.0063\n",
      "Epoch 113/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0620 - mse: 0.0068 - val_loss: 0.0069 - val_mae: 0.0614 - val_mse: 0.0069\n",
      "Epoch 114/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0625 - mse: 0.0069 - val_loss: 0.0063 - val_mae: 0.0580 - val_mse: 0.0063\n",
      "Epoch 115/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0619 - mse: 0.0068 - val_loss: 0.0062 - val_mae: 0.0574 - val_mse: 0.0062\n",
      "Epoch 116/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0070 - mae: 0.0630 - mse: 0.0070 - val_loss: 0.0061 - val_mae: 0.0574 - val_mse: 0.0061\n",
      "Epoch 117/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0613 - mse: 0.0067 - val_loss: 0.0063 - val_mae: 0.0605 - val_mse: 0.0063\n",
      "Epoch 118/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0618 - mse: 0.0068 - val_loss: 0.0061 - val_mae: 0.0586 - val_mse: 0.0061\n",
      "Epoch 119/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0617 - mse: 0.0068 - val_loss: 0.0063 - val_mae: 0.0579 - val_mse: 0.0063\n",
      "Epoch 120/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0623 - mse: 0.0069 - val_loss: 0.0060 - val_mae: 0.0588 - val_mse: 0.0060\n",
      "Epoch 121/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0615 - mse: 0.0067 - val_loss: 0.0084 - val_mae: 0.0701 - val_mse: 0.0084\n",
      "Epoch 122/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0613 - mse: 0.0068 - val_loss: 0.0072 - val_mae: 0.0629 - val_mse: 0.0072\n",
      "Epoch 123/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0617 - mse: 0.0068 - val_loss: 0.0062 - val_mae: 0.0575 - val_mse: 0.0062\n",
      "Epoch 124/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0608 - mse: 0.0066 - val_loss: 0.0063 - val_mae: 0.0614 - val_mse: 0.0063\n",
      "Epoch 125/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0608 - mse: 0.0066 - val_loss: 0.0062 - val_mae: 0.0612 - val_mse: 0.0062\n",
      "Epoch 126/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0598 - mse: 0.0065 - val_loss: 0.0060 - val_mae: 0.0569 - val_mse: 0.0060\n",
      "Epoch 127/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0603 - mse: 0.0066 - val_loss: 0.0061 - val_mae: 0.0596 - val_mse: 0.0061\n",
      "Epoch 128/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0604 - mse: 0.0066 - val_loss: 0.0065 - val_mae: 0.0627 - val_mse: 0.0065\n",
      "Epoch 129/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0601 - mse: 0.0066 - val_loss: 0.0061 - val_mae: 0.0569 - val_mse: 0.0061\n",
      "Epoch 130/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0590 - mse: 0.0064 - val_loss: 0.0061 - val_mae: 0.0597 - val_mse: 0.0061\n",
      "Epoch 131/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0598 - mse: 0.0065 - val_loss: 0.0065 - val_mae: 0.0628 - val_mse: 0.0065\n",
      "Epoch 132/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0597 - mse: 0.0064 - val_loss: 0.0062 - val_mae: 0.0584 - val_mse: 0.0062\n",
      "Epoch 133/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0593 - mse: 0.0064 - val_loss: 0.0059 - val_mae: 0.0581 - val_mse: 0.0059\n",
      "Epoch 134/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0591 - mse: 0.0064 - val_loss: 0.0056 - val_mae: 0.0558 - val_mse: 0.0056\n",
      "Epoch 135/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0588 - mse: 0.0063 - val_loss: 0.0057 - val_mae: 0.0559 - val_mse: 0.0057\n",
      "Epoch 136/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0581 - mse: 0.0062 - val_loss: 0.0058 - val_mae: 0.0555 - val_mse: 0.0058\n",
      "Epoch 137/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0583 - mse: 0.0062 - val_loss: 0.0058 - val_mae: 0.0550 - val_mse: 0.0058\n",
      "Epoch 138/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0582 - mse: 0.0062 - val_loss: 0.0053 - val_mae: 0.0538 - val_mse: 0.0053\n",
      "Epoch 139/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0580 - mse: 0.0061 - val_loss: 0.0055 - val_mae: 0.0555 - val_mse: 0.0055\n",
      "Epoch 140/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0591 - mse: 0.0063 - val_loss: 0.0053 - val_mae: 0.0532 - val_mse: 0.0053\n",
      "Epoch 141/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0572 - mse: 0.0060 - val_loss: 0.0054 - val_mae: 0.0533 - val_mse: 0.0054\n",
      "Epoch 142/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0573 - mse: 0.0059 - val_loss: 0.0054 - val_mae: 0.0533 - val_mse: 0.0054\n",
      "Epoch 143/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0565 - mse: 0.0058 - val_loss: 0.0060 - val_mae: 0.0576 - val_mse: 0.0060\n",
      "Epoch 144/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0569 - mse: 0.0059 - val_loss: 0.0054 - val_mae: 0.0539 - val_mse: 0.0054\n",
      "Epoch 145/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0570 - mse: 0.0059 - val_loss: 0.0052 - val_mae: 0.0535 - val_mse: 0.0052\n",
      "Epoch 146/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0562 - mse: 0.0058 - val_loss: 0.0052 - val_mae: 0.0525 - val_mse: 0.0052\n",
      "Epoch 147/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0564 - mse: 0.0058 - val_loss: 0.0052 - val_mae: 0.0522 - val_mse: 0.0052\n",
      "Epoch 148/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0573 - mse: 0.0059 - val_loss: 0.0052 - val_mae: 0.0524 - val_mse: 0.0052\n",
      "Epoch 149/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0571 - mse: 0.0059 - val_loss: 0.0066 - val_mae: 0.0645 - val_mse: 0.0066\n",
      "Epoch 150/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0570 - mse: 0.0059 - val_loss: 0.0051 - val_mae: 0.0526 - val_mse: 0.0051\n",
      "Epoch 151/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0569 - mse: 0.0059 - val_loss: 0.0051 - val_mae: 0.0533 - val_mse: 0.0051\n",
      "Epoch 152/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0557 - mse: 0.0057 - val_loss: 0.0053 - val_mae: 0.0542 - val_mse: 0.0053\n",
      "Epoch 153/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0559 - mse: 0.0057 - val_loss: 0.0055 - val_mae: 0.0565 - val_mse: 0.0055\n",
      "Epoch 154/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0570 - mse: 0.0059 - val_loss: 0.0061 - val_mae: 0.0573 - val_mse: 0.0061\n",
      "Epoch 155/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0570 - mse: 0.0059 - val_loss: 0.0051 - val_mae: 0.0527 - val_mse: 0.0051\n",
      "Epoch 156/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0566 - mse: 0.0058 - val_loss: 0.0051 - val_mae: 0.0538 - val_mse: 0.0051\n",
      "Epoch 157/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0559 - mse: 0.0057 - val_loss: 0.0059 - val_mae: 0.0557 - val_mse: 0.0059\n",
      "Epoch 158/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0569 - mse: 0.0059 - val_loss: 0.0053 - val_mae: 0.0528 - val_mse: 0.0053\n",
      "Epoch 159/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0565 - mse: 0.0058 - val_loss: 0.0052 - val_mae: 0.0517 - val_mse: 0.0052\n",
      "Epoch 160/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0566 - mse: 0.0058 - val_loss: 0.0056 - val_mae: 0.0578 - val_mse: 0.0056\n",
      "Epoch 161/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0556 - mse: 0.0057 - val_loss: 0.0056 - val_mae: 0.0557 - val_mse: 0.0056\n",
      "Epoch 162/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0561 - mse: 0.0057 - val_loss: 0.0050 - val_mae: 0.0519 - val_mse: 0.0050\n",
      "Epoch 163/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0559 - mse: 0.0057 - val_loss: 0.0054 - val_mae: 0.0559 - val_mse: 0.0054\n",
      "Epoch 164/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0550 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0561 - val_mse: 0.0057\n",
      "Epoch 165/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0568 - mse: 0.0058 - val_loss: 0.0050 - val_mae: 0.0516 - val_mse: 0.0050\n",
      "Epoch 166/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0555 - mse: 0.0056 - val_loss: 0.0051 - val_mae: 0.0527 - val_mse: 0.0051\n",
      "Epoch 167/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0553 - mse: 0.0056 - val_loss: 0.0051 - val_mae: 0.0528 - val_mse: 0.0051\n",
      "Epoch 168/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0552 - mse: 0.0056 - val_loss: 0.0057 - val_mae: 0.0557 - val_mse: 0.0057\n",
      "Epoch 169/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0571 - mse: 0.0059 - val_loss: 0.0055 - val_mae: 0.0562 - val_mse: 0.0055\n",
      "Epoch 170/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0568 - mse: 0.0058 - val_loss: 0.0051 - val_mae: 0.0538 - val_mse: 0.0051\n",
      "Epoch 171/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0566 - mse: 0.0058 - val_loss: 0.0050 - val_mae: 0.0517 - val_mse: 0.0050\n",
      "Epoch 172/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0565 - mse: 0.0057 - val_loss: 0.0053 - val_mae: 0.0558 - val_mse: 0.0053\n",
      "Epoch 173/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0561 - mse: 0.0057 - val_loss: 0.0051 - val_mae: 0.0528 - val_mse: 0.0051\n",
      "Epoch 174/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0560 - mse: 0.0057 - val_loss: 0.0050 - val_mae: 0.0529 - val_mse: 0.0050\n",
      "Epoch 175/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0556 - mse: 0.0056 - val_loss: 0.0049 - val_mae: 0.0521 - val_mse: 0.0049\n",
      "Epoch 176/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0563 - mse: 0.0057 - val_loss: 0.0051 - val_mae: 0.0531 - val_mse: 0.0051\n",
      "Epoch 177/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0557 - mse: 0.0056 - val_loss: 0.0051 - val_mae: 0.0518 - val_mse: 0.0051\n",
      "Epoch 178/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0556 - mse: 0.0057 - val_loss: 0.0058 - val_mae: 0.0598 - val_mse: 0.0058\n",
      "Epoch 179/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0560 - mse: 0.0057 - val_loss: 0.0054 - val_mae: 0.0531 - val_mse: 0.0054\n",
      "Epoch 180/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0553 - mse: 0.0056 - val_loss: 0.0049 - val_mae: 0.0519 - val_mse: 0.0049\n",
      "Epoch 181/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0555 - mse: 0.0056 - val_loss: 0.0050 - val_mae: 0.0517 - val_mse: 0.0050\n",
      "Epoch 182/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0552 - mse: 0.0056 - val_loss: 0.0051 - val_mae: 0.0536 - val_mse: 0.0051\n",
      "Epoch 183/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0551 - mse: 0.0056 - val_loss: 0.0064 - val_mae: 0.0607 - val_mse: 0.0064\n",
      "Epoch 184/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0057 - mae: 0.0557 - mse: 0.0057 - val_loss: 0.0050 - val_mae: 0.0528 - val_mse: 0.0050\n",
      "Epoch 185/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0551 - mse: 0.0056 - val_loss: 0.0052 - val_mae: 0.0547 - val_mse: 0.0052\n",
      "Epoch 186/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0055 - mae: 0.0553 - mse: 0.0055 - val_loss: 0.0050 - val_mae: 0.0523 - val_mse: 0.0050\n",
      "Epoch 187/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0555 - mse: 0.0056 - val_loss: 0.0049 - val_mae: 0.0515 - val_mse: 0.0049\n",
      "Epoch 188/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0055 - mae: 0.0551 - mse: 0.0055 - val_loss: 0.0080 - val_mae: 0.0705 - val_mse: 0.0080\n",
      "Epoch 189/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0553 - mse: 0.0056 - val_loss: 0.0050 - val_mae: 0.0526 - val_mse: 0.0050\n",
      "Epoch 190/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0550 - mse: 0.0056 - val_loss: 0.0050 - val_mae: 0.0529 - val_mse: 0.0050\n",
      "Epoch 190: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 1ms/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.009233478729114269    std:  0.07211210459738053    MAE:  0.054062649752364314     R2:  0.952602514147901\n",
      "Test. mean:  -0.008644609139978356    std:  0.07014046068996882    MAE:  0.05293041864555938     R2:  0.9536403994418695\n",
      "\u001b[1m\u001b[91mFold 3\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.1582 - mae: 0.2913 - mse: 0.1582 - val_loss: 0.0534 - val_mae: 0.1928 - val_mse: 0.0534\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0453 - mae: 0.1763 - mse: 0.0453 - val_loss: 0.0498 - val_mae: 0.1823 - val_mse: 0.0498\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0408 - mae: 0.1637 - mse: 0.0408 - val_loss: 0.0367 - val_mae: 0.1605 - val_mse: 0.0367\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0327 - mae: 0.1471 - mse: 0.0327 - val_loss: 0.0274 - val_mae: 0.1358 - val_mse: 0.0274\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0299 - mae: 0.1326 - mse: 0.0299 - val_loss: 0.0302 - val_mae: 0.1335 - val_mse: 0.0302\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0215 - mae: 0.1165 - mse: 0.0215 - val_loss: 0.0193 - val_mae: 0.1125 - val_mse: 0.0193\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0196 - mae: 0.1106 - mse: 0.0196 - val_loss: 0.0181 - val_mae: 0.1066 - val_mse: 0.0181\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0171 - mae: 0.1007 - mse: 0.0171 - val_loss: 0.0173 - val_mae: 0.1009 - val_mse: 0.0173\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0160 - mae: 0.0981 - mse: 0.0160 - val_loss: 0.0174 - val_mae: 0.1022 - val_mse: 0.0174\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0160 - mae: 0.0971 - mse: 0.0160 - val_loss: 0.0197 - val_mae: 0.1065 - val_mse: 0.0197\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0156 - mae: 0.0956 - mse: 0.0156 - val_loss: 0.0149 - val_mae: 0.0938 - val_mse: 0.0149\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0146 - mae: 0.0926 - mse: 0.0146 - val_loss: 0.0145 - val_mae: 0.0905 - val_mse: 0.0145\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0147 - mae: 0.0919 - mse: 0.0147 - val_loss: 0.0161 - val_mae: 0.0960 - val_mse: 0.0161\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0135 - mae: 0.0871 - mse: 0.0135 - val_loss: 0.0134 - val_mae: 0.0860 - val_mse: 0.0134\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0133 - mae: 0.0874 - mse: 0.0133 - val_loss: 0.0140 - val_mae: 0.0909 - val_mse: 0.0140\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0123 - mae: 0.0836 - mse: 0.0123 - val_loss: 0.0129 - val_mae: 0.0870 - val_mse: 0.0129\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0114 - mae: 0.0798 - mse: 0.0114 - val_loss: 0.0110 - val_mae: 0.0782 - val_mse: 0.0110\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0103 - mae: 0.0758 - mse: 0.0103 - val_loss: 0.0095 - val_mae: 0.0721 - val_mse: 0.0095\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0716 - mse: 0.0092 - val_loss: 0.0084 - val_mae: 0.0677 - val_mse: 0.0084\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0671 - mse: 0.0082 - val_loss: 0.0084 - val_mae: 0.0671 - val_mse: 0.0084\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0079 - mae: 0.0665 - mse: 0.0079 - val_loss: 0.0084 - val_mae: 0.0712 - val_mse: 0.0084\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0077 - mae: 0.0659 - mse: 0.0077 - val_loss: 0.0073 - val_mae: 0.0629 - val_mse: 0.0073\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0075 - mae: 0.0647 - mse: 0.0075 - val_loss: 0.0078 - val_mae: 0.0646 - val_mse: 0.0078\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0631 - mse: 0.0071 - val_loss: 0.0072 - val_mae: 0.0640 - val_mse: 0.0072\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0637 - mse: 0.0072 - val_loss: 0.0072 - val_mae: 0.0630 - val_mse: 0.0072\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0625 - mse: 0.0069 - val_loss: 0.0095 - val_mae: 0.0781 - val_mse: 0.0095\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0070 - mae: 0.0625 - mse: 0.0070 - val_loss: 0.0065 - val_mae: 0.0592 - val_mse: 0.0065\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0626 - mse: 0.0069 - val_loss: 0.0068 - val_mae: 0.0625 - val_mse: 0.0068\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0620 - mse: 0.0068 - val_loss: 0.0083 - val_mae: 0.0726 - val_mse: 0.0083\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0614 - mse: 0.0067 - val_loss: 0.0062 - val_mae: 0.0577 - val_mse: 0.0062\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0596 - mse: 0.0064 - val_loss: 0.0062 - val_mae: 0.0581 - val_mse: 0.0062\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0601 - mse: 0.0064 - val_loss: 0.0067 - val_mae: 0.0611 - val_mse: 0.0067\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0605 - mse: 0.0065 - val_loss: 0.0069 - val_mae: 0.0641 - val_mse: 0.0069\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0607 - mse: 0.0065 - val_loss: 0.0084 - val_mae: 0.0739 - val_mse: 0.0084\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0597 - mse: 0.0063 - val_loss: 0.0063 - val_mae: 0.0575 - val_mse: 0.0063\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0599 - mse: 0.0064 - val_loss: 0.0070 - val_mae: 0.0653 - val_mse: 0.0070\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0610 - mse: 0.0065 - val_loss: 0.0063 - val_mae: 0.0583 - val_mse: 0.0063\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0582 - mse: 0.0061 - val_loss: 0.0060 - val_mae: 0.0575 - val_mse: 0.0060\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0588 - mse: 0.0062 - val_loss: 0.0059 - val_mae: 0.0563 - val_mse: 0.0059\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0601 - mse: 0.0064 - val_loss: 0.0071 - val_mae: 0.0659 - val_mse: 0.0071\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0575 - mse: 0.0060 - val_loss: 0.0058 - val_mae: 0.0548 - val_mse: 0.0058\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0602 - mse: 0.0064 - val_loss: 0.0059 - val_mae: 0.0560 - val_mse: 0.0059\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0575 - mse: 0.0059 - val_loss: 0.0059 - val_mae: 0.0553 - val_mse: 0.0059\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0581 - mse: 0.0060 - val_loss: 0.0058 - val_mae: 0.0555 - val_mse: 0.0058\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0592 - mse: 0.0062 - val_loss: 0.0061 - val_mae: 0.0571 - val_mse: 0.0061\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0590 - mse: 0.0061 - val_loss: 0.0057 - val_mae: 0.0549 - val_mse: 0.0057\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0581 - mse: 0.0060 - val_loss: 0.0069 - val_mae: 0.0618 - val_mse: 0.0069\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0574 - mse: 0.0059 - val_loss: 0.0059 - val_mae: 0.0566 - val_mse: 0.0059\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0581 - mse: 0.0060 - val_loss: 0.0077 - val_mae: 0.0701 - val_mse: 0.0077\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0612 - mse: 0.0065 - val_loss: 0.0059 - val_mae: 0.0552 - val_mse: 0.0059\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0572 - mse: 0.0059 - val_loss: 0.0059 - val_mae: 0.0556 - val_mse: 0.0059\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0614 - mse: 0.0065 - val_loss: 0.0059 - val_mae: 0.0561 - val_mse: 0.0059\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0584 - mse: 0.0061 - val_loss: 0.0061 - val_mae: 0.0568 - val_mse: 0.0061\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0581 - mse: 0.0060 - val_loss: 0.0057 - val_mae: 0.0548 - val_mse: 0.0057\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0577 - mse: 0.0059 - val_loss: 0.0092 - val_mae: 0.0768 - val_mse: 0.0092\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0599 - mse: 0.0063 - val_loss: 0.0060 - val_mae: 0.0576 - val_mse: 0.0060\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0566 - mse: 0.0058 - val_loss: 0.0057 - val_mae: 0.0556 - val_mse: 0.0057\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0596 - mse: 0.0062 - val_loss: 0.0059 - val_mae: 0.0572 - val_mse: 0.0059\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0606 - mse: 0.0064 - val_loss: 0.0058 - val_mae: 0.0559 - val_mse: 0.0058\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0574 - mse: 0.0059 - val_loss: 0.0058 - val_mae: 0.0548 - val_mse: 0.0058\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0575 - mse: 0.0059 - val_loss: 0.0066 - val_mae: 0.0603 - val_mse: 0.0066\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0584 - mse: 0.0061 - val_loss: 0.0058 - val_mae: 0.0566 - val_mse: 0.0058\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0612 - mse: 0.0065 - val_loss: 0.0058 - val_mae: 0.0552 - val_mse: 0.0058\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0586 - mse: 0.0061 - val_loss: 0.0067 - val_mae: 0.0634 - val_mse: 0.0067\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0576 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0572 - val_mse: 0.0059\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0597 - mse: 0.0062 - val_loss: 0.0058 - val_mae: 0.0544 - val_mse: 0.0058\n",
      "Epoch 67/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0571 - mse: 0.0059 - val_loss: 0.0057 - val_mae: 0.0554 - val_mse: 0.0057\n",
      "Epoch 68/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0575 - mse: 0.0059 - val_loss: 0.0062 - val_mae: 0.0597 - val_mse: 0.0062\n",
      "Epoch 69/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0591 - mse: 0.0062 - val_loss: 0.0070 - val_mae: 0.0635 - val_mse: 0.0070\n",
      "Epoch 70/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0594 - mse: 0.0062 - val_loss: 0.0057 - val_mae: 0.0553 - val_mse: 0.0057\n",
      "Epoch 71/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0583 - mse: 0.0060 - val_loss: 0.0058 - val_mae: 0.0548 - val_mse: 0.0058\n",
      "Epoch 72/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0579 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0554 - val_mse: 0.0059\n",
      "Epoch 72: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 1ms/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  0.00908252644483933    std:  0.07383017021008562    MAE:  0.053631406693531024     R2:  0.9499106101726061\n",
      "Test. mean:  0.009725770034186032    std:  0.07624464155537537    MAE:  0.05542845834091011     R2:  0.9485501448593079\n",
      "\u001b[1m\u001b[91mFold 4\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0910 - mae: 0.2358 - mse: 0.0910 - val_loss: 0.0466 - val_mae: 0.1789 - val_mse: 0.0466\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0379 - mae: 0.1611 - mse: 0.0379 - val_loss: 0.0292 - val_mae: 0.1392 - val_mse: 0.0292\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0215 - mae: 0.1169 - mse: 0.0215 - val_loss: 0.0151 - val_mae: 0.0964 - val_mse: 0.0151\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0133 - mae: 0.0879 - mse: 0.0133 - val_loss: 0.0125 - val_mae: 0.0834 - val_mse: 0.0125\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0118 - mae: 0.0811 - mse: 0.0118 - val_loss: 0.0112 - val_mae: 0.0769 - val_mse: 0.0112\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0107 - mae: 0.0758 - mse: 0.0107 - val_loss: 0.0106 - val_mae: 0.0745 - val_mse: 0.0106\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0098 - mae: 0.0728 - mse: 0.0098 - val_loss: 0.0101 - val_mae: 0.0754 - val_mse: 0.0101\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0712 - mse: 0.0092 - val_loss: 0.0092 - val_mae: 0.0706 - val_mse: 0.0092\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0690 - mse: 0.0086 - val_loss: 0.0081 - val_mae: 0.0665 - val_mse: 0.0081\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0686 - mse: 0.0083 - val_loss: 0.0091 - val_mae: 0.0728 - val_mse: 0.0091\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0080 - mae: 0.0671 - mse: 0.0080 - val_loss: 0.0091 - val_mae: 0.0722 - val_mse: 0.0091\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0080 - mae: 0.0676 - mse: 0.0080 - val_loss: 0.0078 - val_mae: 0.0655 - val_mse: 0.0078\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0074 - mae: 0.0641 - mse: 0.0074 - val_loss: 0.0100 - val_mae: 0.0768 - val_mse: 0.0100\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0076 - mae: 0.0656 - mse: 0.0076 - val_loss: 0.0098 - val_mae: 0.0751 - val_mse: 0.0098\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0075 - mae: 0.0654 - mse: 0.0075 - val_loss: 0.0084 - val_mae: 0.0672 - val_mse: 0.0084\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0070 - mae: 0.0627 - mse: 0.0070 - val_loss: 0.0088 - val_mae: 0.0708 - val_mse: 0.0088\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0076 - mae: 0.0663 - mse: 0.0076 - val_loss: 0.0068 - val_mae: 0.0614 - val_mse: 0.0068\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0072 - mae: 0.0638 - mse: 0.0072 - val_loss: 0.0076 - val_mae: 0.0672 - val_mse: 0.0076\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0641 - mse: 0.0071 - val_loss: 0.0066 - val_mae: 0.0607 - val_mse: 0.0066\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0618 - mse: 0.0067 - val_loss: 0.0066 - val_mae: 0.0594 - val_mse: 0.0066\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0616 - mse: 0.0067 - val_loss: 0.0073 - val_mae: 0.0649 - val_mse: 0.0073\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0621 - mse: 0.0067 - val_loss: 0.0064 - val_mae: 0.0595 - val_mse: 0.0064\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0611 - mse: 0.0066 - val_loss: 0.0068 - val_mae: 0.0626 - val_mse: 0.0068\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0065 - mae: 0.0607 - mse: 0.0065 - val_loss: 0.0069 - val_mae: 0.0619 - val_mse: 0.0069\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0624 - mse: 0.0067 - val_loss: 0.0089 - val_mae: 0.0769 - val_mse: 0.0089\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0615 - mse: 0.0067 - val_loss: 0.0073 - val_mae: 0.0644 - val_mse: 0.0073\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0596 - mse: 0.0063 - val_loss: 0.0069 - val_mae: 0.0648 - val_mse: 0.0069\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0605 - mse: 0.0064 - val_loss: 0.0062 - val_mae: 0.0575 - val_mse: 0.0062\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0592 - mse: 0.0062 - val_loss: 0.0070 - val_mae: 0.0653 - val_mse: 0.0070\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0625 - mse: 0.0067 - val_loss: 0.0060 - val_mae: 0.0566 - val_mse: 0.0060\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0592 - mse: 0.0062 - val_loss: 0.0061 - val_mae: 0.0585 - val_mse: 0.0061\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0612 - mse: 0.0065 - val_loss: 0.0066 - val_mae: 0.0633 - val_mse: 0.0066\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0599 - mse: 0.0063 - val_loss: 0.0060 - val_mae: 0.0589 - val_mse: 0.0060\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0616 - mse: 0.0066 - val_loss: 0.0087 - val_mae: 0.0757 - val_mse: 0.0087\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0587 - mse: 0.0061 - val_loss: 0.0067 - val_mae: 0.0614 - val_mse: 0.0067\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0601 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0594 - val_mse: 0.0061\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0593 - mse: 0.0062 - val_loss: 0.0063 - val_mae: 0.0593 - val_mse: 0.0063\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0589 - mse: 0.0062 - val_loss: 0.0065 - val_mae: 0.0598 - val_mse: 0.0065\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0603 - mse: 0.0063 - val_loss: 0.0060 - val_mae: 0.0573 - val_mse: 0.0060\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0599 - mse: 0.0063 - val_loss: 0.0059 - val_mae: 0.0568 - val_mse: 0.0059\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0584 - mse: 0.0060 - val_loss: 0.0072 - val_mae: 0.0680 - val_mse: 0.0072\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0597 - mse: 0.0062 - val_loss: 0.0058 - val_mae: 0.0564 - val_mse: 0.0058\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0591 - mse: 0.0062 - val_loss: 0.0085 - val_mae: 0.0746 - val_mse: 0.0085\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0605 - mse: 0.0064 - val_loss: 0.0059 - val_mae: 0.0563 - val_mse: 0.0059\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0594 - mse: 0.0062 - val_loss: 0.0059 - val_mae: 0.0567 - val_mse: 0.0059\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0589 - mse: 0.0061 - val_loss: 0.0057 - val_mae: 0.0570 - val_mse: 0.0057\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0585 - mse: 0.0061 - val_loss: 0.0058 - val_mae: 0.0560 - val_mse: 0.0058\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0587 - mse: 0.0060 - val_loss: 0.0070 - val_mae: 0.0642 - val_mse: 0.0070\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0586 - mse: 0.0059 - val_loss: 0.0063 - val_mae: 0.0632 - val_mse: 0.0063\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0593 - mse: 0.0060 - val_loss: 0.0070 - val_mae: 0.0647 - val_mse: 0.0070\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0588 - mse: 0.0059 - val_loss: 0.0053 - val_mae: 0.0551 - val_mse: 0.0053\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0589 - mse: 0.0059 - val_loss: 0.0054 - val_mae: 0.0565 - val_mse: 0.0054\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0055 - mae: 0.0562 - mse: 0.0055 - val_loss: 0.0052 - val_mae: 0.0541 - val_mse: 0.0052\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0055 - mae: 0.0566 - mse: 0.0055 - val_loss: 0.0076 - val_mae: 0.0703 - val_mse: 0.0076\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0571 - mse: 0.0056 - val_loss: 0.0050 - val_mae: 0.0518 - val_mse: 0.0050\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0054 - mae: 0.0557 - mse: 0.0054 - val_loss: 0.0070 - val_mae: 0.0660 - val_mse: 0.0070\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0052 - mae: 0.0547 - mse: 0.0052 - val_loss: 0.0046 - val_mae: 0.0507 - val_mse: 0.0046\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0534 - mse: 0.0050 - val_loss: 0.0047 - val_mae: 0.0515 - val_mse: 0.0047\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0048 - mae: 0.0523 - mse: 0.0048 - val_loss: 0.0048 - val_mae: 0.0526 - val_mse: 0.0048\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0538 - mse: 0.0050 - val_loss: 0.0052 - val_mae: 0.0562 - val_mse: 0.0052\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0050 - mae: 0.0537 - mse: 0.0050 - val_loss: 0.0047 - val_mae: 0.0498 - val_mse: 0.0047\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0051 - mae: 0.0536 - mse: 0.0051 - val_loss: 0.0046 - val_mae: 0.0506 - val_mse: 0.0046\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0049 - mae: 0.0528 - mse: 0.0049 - val_loss: 0.0051 - val_mae: 0.0553 - val_mse: 0.0051\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0048 - mae: 0.0518 - mse: 0.0048 - val_loss: 0.0058 - val_mae: 0.0608 - val_mse: 0.0058\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0515 - mse: 0.0047 - val_loss: 0.0047 - val_mae: 0.0503 - val_mse: 0.0047\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0508 - mse: 0.0046 - val_loss: 0.0054 - val_mae: 0.0560 - val_mse: 0.0054\n",
      "Epoch 67/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0521 - mse: 0.0047 - val_loss: 0.0051 - val_mae: 0.0537 - val_mse: 0.0051\n",
      "Epoch 68/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0048 - mae: 0.0525 - mse: 0.0048 - val_loss: 0.0045 - val_mae: 0.0508 - val_mse: 0.0045\n",
      "Epoch 69/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0517 - mse: 0.0047 - val_loss: 0.0048 - val_mae: 0.0536 - val_mse: 0.0048\n",
      "Epoch 70/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0512 - mse: 0.0047 - val_loss: 0.0046 - val_mae: 0.0514 - val_mse: 0.0046\n",
      "Epoch 71/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0518 - mse: 0.0047 - val_loss: 0.0071 - val_mae: 0.0680 - val_mse: 0.0071\n",
      "Epoch 72/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0521 - mse: 0.0047 - val_loss: 0.0054 - val_mae: 0.0555 - val_mse: 0.0054\n",
      "Epoch 73/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0507 - mse: 0.0046 - val_loss: 0.0050 - val_mae: 0.0534 - val_mse: 0.0050\n",
      "Epoch 74/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0514 - mse: 0.0046 - val_loss: 0.0048 - val_mae: 0.0537 - val_mse: 0.0048\n",
      "Epoch 75/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0048 - mae: 0.0530 - mse: 0.0048 - val_loss: 0.0048 - val_mae: 0.0515 - val_mse: 0.0048\n",
      "Epoch 76/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0515 - mse: 0.0046 - val_loss: 0.0045 - val_mae: 0.0506 - val_mse: 0.0045\n",
      "Epoch 77/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0512 - mse: 0.0046 - val_loss: 0.0044 - val_mae: 0.0498 - val_mse: 0.0044\n",
      "Epoch 78/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0514 - mse: 0.0047 - val_loss: 0.0044 - val_mae: 0.0492 - val_mse: 0.0044\n",
      "Epoch 79/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0510 - mse: 0.0046 - val_loss: 0.0042 - val_mae: 0.0475 - val_mse: 0.0042\n",
      "Epoch 80/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0508 - mse: 0.0046 - val_loss: 0.0049 - val_mae: 0.0539 - val_mse: 0.0049\n",
      "Epoch 81/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0522 - mse: 0.0047 - val_loss: 0.0075 - val_mae: 0.0709 - val_mse: 0.0075\n",
      "Epoch 82/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0508 - mse: 0.0046 - val_loss: 0.0043 - val_mae: 0.0495 - val_mse: 0.0043\n",
      "Epoch 83/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0513 - mse: 0.0046 - val_loss: 0.0044 - val_mae: 0.0486 - val_mse: 0.0044\n",
      "Epoch 84/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0519 - mse: 0.0047 - val_loss: 0.0060 - val_mae: 0.0615 - val_mse: 0.0060\n",
      "Epoch 85/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0508 - mse: 0.0045 - val_loss: 0.0042 - val_mae: 0.0487 - val_mse: 0.0042\n",
      "Epoch 86/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0044 - mae: 0.0497 - mse: 0.0044 - val_loss: 0.0043 - val_mae: 0.0489 - val_mse: 0.0043\n",
      "Epoch 87/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0508 - mse: 0.0045 - val_loss: 0.0048 - val_mae: 0.0538 - val_mse: 0.0048\n",
      "Epoch 88/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0506 - mse: 0.0045 - val_loss: 0.0041 - val_mae: 0.0476 - val_mse: 0.0041\n",
      "Epoch 89/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0509 - mse: 0.0045 - val_loss: 0.0045 - val_mae: 0.0515 - val_mse: 0.0045\n",
      "Epoch 90/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0509 - mse: 0.0045 - val_loss: 0.0041 - val_mae: 0.0480 - val_mse: 0.0041\n",
      "Epoch 91/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0503 - mse: 0.0045 - val_loss: 0.0044 - val_mae: 0.0500 - val_mse: 0.0044\n",
      "Epoch 92/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0044 - mae: 0.0498 - mse: 0.0044 - val_loss: 0.0053 - val_mae: 0.0571 - val_mse: 0.0053\n",
      "Epoch 93/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0503 - mse: 0.0045 - val_loss: 0.0045 - val_mae: 0.0519 - val_mse: 0.0045\n",
      "Epoch 94/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0044 - mae: 0.0502 - mse: 0.0044 - val_loss: 0.0041 - val_mae: 0.0475 - val_mse: 0.0041\n",
      "Epoch 95/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0044 - mae: 0.0502 - mse: 0.0044 - val_loss: 0.0041 - val_mae: 0.0477 - val_mse: 0.0041\n",
      "Epoch 96/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0507 - mse: 0.0045 - val_loss: 0.0043 - val_mae: 0.0483 - val_mse: 0.0043\n",
      "Epoch 97/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0504 - mse: 0.0045 - val_loss: 0.0059 - val_mae: 0.0617 - val_mse: 0.0059\n",
      "Epoch 98/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0518 - mse: 0.0046 - val_loss: 0.0049 - val_mae: 0.0527 - val_mse: 0.0049\n",
      "Epoch 99/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0508 - mse: 0.0045 - val_loss: 0.0046 - val_mae: 0.0521 - val_mse: 0.0046\n",
      "Epoch 100/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0512 - mse: 0.0045 - val_loss: 0.0042 - val_mae: 0.0477 - val_mse: 0.0042\n",
      "Epoch 101/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0043 - mae: 0.0496 - mse: 0.0043 - val_loss: 0.0041 - val_mae: 0.0471 - val_mse: 0.0041\n",
      "Epoch 102/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0505 - mse: 0.0045 - val_loss: 0.0046 - val_mae: 0.0513 - val_mse: 0.0046\n",
      "Epoch 103/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0043 - mae: 0.0495 - mse: 0.0043 - val_loss: 0.0043 - val_mae: 0.0481 - val_mse: 0.0043\n",
      "Epoch 104/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0521 - mse: 0.0047 - val_loss: 0.0044 - val_mae: 0.0502 - val_mse: 0.0044\n",
      "Epoch 105/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0505 - mse: 0.0045 - val_loss: 0.0048 - val_mae: 0.0540 - val_mse: 0.0048\n",
      "Epoch 105: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 1ms/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.02591034135035708    std:  0.06291192885014324    MAE:  0.053176110992563946     R2:  0.9640604217505759\n",
      "Test. mean:  -0.026011488423581402    std:  0.06425804911894974    MAE:  0.05402426141826272     R2:  0.9614647222726365\n",
      "\n",
      "Duration :  00:05:01 214ms\n",
      "\u001b[1maverage MAE of the training set:\u001b[0m   0.09 +/- 0.06\n",
      "\u001b[1maverage MAE of the validation set:\u001b[0m 0.09 +/- 0.06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#######################################################################################\n",
    "# optimization of the ANN\n",
    "# library used: keras and scikit learn for the KFold cross-validator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "VERBOSE = 1\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 25\n",
    "N_SPLIT = 5\n",
    "\n",
    "vID.chrono_start()\n",
    "\n",
    "# variables created to save at each iteration of the KFold process: the man error, the standard deviation, MAE, R2\n",
    "meantT=list()\n",
    "stdtT=list()\n",
    "MAEtT=list()\n",
    "R2tT=list()\n",
    "meanvT=list()\n",
    "stdvT=list()\n",
    "MAEvT=list()\n",
    "R2vT=list()\n",
    "\n",
    "kfold = KFold(n_splits=N_SPLIT,shuffle=True,random_state=42) # k-fold is here!\n",
    "#print(list(kfold.split(x_train,y_train)))\n",
    "\n",
    "j = 0 # Variable for keeping count of split we are executing\n",
    "# The KFold cv provides train/test indices to split data in train/test sets\n",
    "for train_idx, val_idx in list(kfold.split(xdata,ydata)):\n",
    "\n",
    "    x_train_cv = xdata.iloc[train_idx]\n",
    "    x_valid_cv = xdata.iloc[val_idx]\n",
    "    y_train_cv = ydata.iloc[train_idx]\n",
    "    y_valid_cv = ydata.iloc[val_idx]\n",
    "#    display(x_train_cv,x_valid_cv)\n",
    "# This part has been commented with respect to the original script\n",
    "    # scaler = preprocessing.StandardScaler()\n",
    "    # scaler.fit(x_train_cv.values)\n",
    "    # xt_scaled = scaler.transform(x_train_cv.values) #returns a numpy array\n",
    "    # xv_scaled = scaler.transform(x_valid_cv.values) #returns a numpy array\n",
    "    # x_train_cv = pd.DataFrame(xt_scaled, index=x_train_cv.index, columns=x_train_cv.columns)\n",
    "    # x_valid_cv = pd.DataFrame(xv_scaled, index=x_valid_cv.index, columns=x_valid_cv.columns)\n",
    "    # del xt_scaled, xv_scaled\n",
    "##############\n",
    "#    display(x_train_cv.describe().style.format(\"{0:.2f}\").set_caption(\"Training set after normalization (with scikit-learn):\"))\n",
    "#    display(x_valid_cv.describe().style.format(\"{0:.2f}\").set_caption(\"Validation set after normalization (with scikit-learn):\"))\n",
    "    print(f\"{color.BOLD}{color.RED}Fold {j}{color.OFF}\")\n",
    "    j+=1\n",
    "    ANNmodel=defANN( (53,), acthL )\n",
    "    ANNhistory = ANNmodel.fit(x_train_cv,\n",
    "                        y_train_cv,\n",
    "                        epochs          = EPOCHS,\n",
    "                        batch_size      = BATCH_SIZE,\n",
    "                        verbose         = VERBOSE,\n",
    "                        validation_data = (x_valid_cv, y_valid_cv),\n",
    "                        callbacks=[es])\n",
    "    ytrain_hat=ANNmodel.predict(x_train_cv)\n",
    "    yvalid_hat=ANNmodel.predict(x_valid_cv)\n",
    "    diffyt = ytrain_hat.ravel() - y_train_cv.ravel()\n",
    "    diffyv = yvalid_hat.ravel() - y_valid_cv.ravel()\n",
    "\n",
    "    print()\n",
    "    print(\"xCO2(predicted) - xCO2(actual)\")\n",
    "    print(\n",
    "          \"Train.\",\"mean: \", np.mean(diffyt),\n",
    "          \"   std: \", np.std(diffyt),\n",
    "          \"   MAE: \", np.average(abs(diffyt)),\n",
    "          \"    R2: \", np.corrcoef(y_train_cv.ravel(),ytrain_hat.ravel())[0,1]\n",
    "         )\n",
    "    print(\n",
    "          \"Test.\",\"mean: \", np.mean(diffyv),\n",
    "          \"   std: \", np.std(diffyv),\n",
    "          \"   MAE: \", np.average(abs(diffyv)),\n",
    "          \"    R2: \", np.corrcoef(y_valid_cv.ravel(),yvalid_hat.ravel())[0,1]\n",
    "         )\n",
    "    meantT.append(np.mean(diffyt))\n",
    "    meanvT.append(np.mean(diffyv))\n",
    "    stdtT.append(np.std(diffyt))\n",
    "    stdvT.append(np.std(diffyv))\n",
    "    MAEtT.append(np.average(abs(diffyt)))\n",
    "    MAEvT.append(np.average(abs(diffyv)))\n",
    "    R2tT.append(np.corrcoef(y_train_cv.ravel(),ytrain_hat.ravel())[0,1])\n",
    "    R2vT.append(np.corrcoef(y_valid_cv.ravel(),yvalid_hat.ravel())[0,1])\n",
    "    \n",
    "vID.chrono_show()\n",
    "\n",
    "#######################################################################################\n",
    "# accuracy of the ANN?\n",
    "# library used: numpy\n",
    "print(f\"{color.BOLD}average MAE of the training set:{color.OFF}   {np.mean(MAEtT):.2f} +/- {np.std(MAEtT):.2f}\")\n",
    "print(f\"{color.BOLD}average MAE of the validation set:{color.OFF} {np.mean(MAEvT):.2f} +/- {np.std(MAEvT):.2f}\")\n",
    "\n",
    "figCV, axCV = plt.subplots(1, 1)\n",
    "figCV.set_size_inches(12,5)\n",
    "axCV.errorbar(x=np.arange(len(meantT)), y=meantT, yerr=MAEtT, label='training sets', fmt='o-', capsize=10)\n",
    "axCV.errorbar(x=np.arange(len(meanvT))+0.1, y=meanvT, yerr=MAEvT, label='validation sets', fmt='o-', capsize=10)\n",
    "axCV.legend(loc='lower left', shadow=True, fontsize='14')\n",
    "axCV.set_xlabel('cross-validation k-values ',fontdict={'fontsize':16})\n",
    "axCV.set_ylabel('$\\hat{y}-y_{\\mathrm{actual}}$',fontdict={'fontsize':16})\n",
    "axCV.tick_params(labelsize = 14)\n",
    "plt.savefig('../DS4B-CO2-images/KFold-cv-AppliedToSong_etal.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d639724-2e98-43dd-b6e5-7f0767cfb27d",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "You have probably found results similar to those reported in the following error plot:\n",
    "<p style=\"text-align: center\"><img width=\"650px\" src=\"../DS4B-CO2-images/KFold-cv-AppliedToSong_etalK-saved.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_ResultsSong\"></p>\n",
    "    <b>This error plot shows a bad performance of the original ML algorithm of Song <i>et al</i>. (<i>i.e.</i> without standardization of the data), with a strong variation of error bars.</b>\n",
    "    \n",
    "Either the authors did actually apply a standardization preprocessing and they forgot to mention it in the article, or they ran several optimization algorithms of the ANN until they found a seemingly performant one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1998a632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**End at:** Monday 31 October 2022, 21:05:33  \n",
       "**Duration:** 00:05:06 796ms"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"../config/svg/logoEnd.svg\" style=\"margin-left:auto; margin-right:auto\"/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vID.end(cwd0)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
